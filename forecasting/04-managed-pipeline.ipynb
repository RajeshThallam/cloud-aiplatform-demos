{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWECL_E0S-cl"
   },
   "source": [
    "# Managed Pipelines Experimental: Custom containers and resource specs\n",
    "\n",
    "This notebook shows how to build and use custom containers for Pipeline components.  It also shows how to pass typed artifact data between component, and how to specify required resources when defining a pipeline.\n",
    "\n",
    "This example uses one of the TensorFlow Datasets, in particular the [Large Movie Review Dataset](https://www.tensorflow.org/datasets/catalog/imdb_reviews#imdb_reviewssubwords8k), for a binary sentiment classification task: predicting whether a movie review is negative or positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNu_BtiA5h9N"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Before you run this notebook, ensure that your Google Cloud user account and project are granted access to the Managed Pipelines Experimental. To be granted access to the Managed Pipelines Experimental, fill out this [form](http://go/cloud-mlpipelines-signup) and let your account representative know you have requested access. \n",
    "\n",
    "This notebook is intended to be run on either one of:\n",
    "* [AI Platform Notebooks](https://cloud.google.com/ai-platform-notebooks). See the \"AI Platform Notebooks\" section in the Experimental [User Guide](https://docs.google.com/document/d/1JXtowHwppgyghnj1N1CT73hwD1caKtWkLcm2_0qGBoI/edit?usp=sharing) for more detail on creating a notebook server instance.\n",
    "* [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb)\n",
    "\n",
    "**To run this notebook on AI Platform Notebooks**, click on the **File** menu, then select \"Download .ipynb\".  Then, upload that notebook from your local machine to AI Platform Notebooks. (In the AI Platform Notebooks left panel, look for an icon of an arrow pointing up, to upload).\n",
    "\n",
    "We'll first install some libraries and set up some variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ-GWdI7SmrN"
   },
   "source": [
    "Set `gcloud` to use your project.  **Edit the following cell before running it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pD5jOcSURdcU"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'rthallam-demo-project'  # <---CHANGE THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAaCPLjgiJrO"
   },
   "source": [
    "Set `gcloud` to use your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VkWdxe4TXRHk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gckGHdW9iPrq"
   },
   "source": [
    "If you're running this notebook on colab, authenticate with your user account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kZQA0KrfXCvU"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaqJjbmk6o0o"
   },
   "source": [
    "-----------------\n",
    "\n",
    "**If you're on AI Platform Notebooks**, authenticate with Google Cloud before running the next section, by running\n",
    "```sh\n",
    "gcloud auth login\n",
    "```\n",
    "**in the Terminal window** (which you can open via **File** > **New** in the menu).  You only need to do this once per notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOpZ41iBW7bl"
   },
   "source": [
    "### Install the KFP SDK and AI Platform Pipelines client library\n",
    "\n",
    "For Managed Pipelines Experimental, you'll need to download a special version of the AI Platform client library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "injJzlmllbEL"
   },
   "source": [
    "Then, install the libraries and restart the kernel. If you see a permissions error for the Metadata libraries, make sure you've run the `gcloud auth login` command as indicated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bnRCttVlajjw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-aiplatform-pipelines/releases/latest/kfp-1.5.0rc5.tar.gz...\n",
      "/ [1 files][188.3 KiB/188.3 KiB]                                                \n",
      "Operation completed over 1 objects/188.3 KiB.                                    \n",
      "Copying gs://cloud-aiplatform-pipelines/releases/latest/aiplatform_pipelines_client-0.1.0.caip20210415-py3-none-any.whl...\n",
      "/ [1 files][ 22.7 KiB/ 22.7 KiB]                                                \n",
      "Operation completed over 1 objects/22.7 KiB.                                     \n",
      "AccessDeniedException: 403 560224572293-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://cloud-aiplatform-pipelines/releases/latest/kfp-1.5.0rc5.tar.gz .\n",
    "!gsutil cp gs://cloud-aiplatform-pipelines/releases/latest/aiplatform_pipelines_client-0.1.0.caip20210415-py3-none-any.whl .\n",
    "# Get the Metadata SDK to query the produced metadata.\n",
    "!gsutil cp gs://cloud-aiplatform-metadata/sdk/google-cloud-aiplatform-metadata-0.0.1.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TmUZzSv6YA9-"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "  USER_FLAG = ''\n",
    "else:\n",
    "  USER_FLAG = '--user'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFSsfPr-Uad1"
   },
   "source": [
    "Install the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbZl0NsXSsmh"
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install {USER_FLAG} kfp-1.5.0rc5.tar.gz google-cloud-aiplatform-metadata-0.0.1.tar.gz aiplatform_pipelines_client-0.1.0.caip20210415-py3-none-any.whl --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "o5kaReN2lbEN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N33S1ikHIOPS"
   },
   "source": [
    "The KFP version should be >= 1.5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "a4uvTyimMYOr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP version: 1.5.0-rc.5\n"
     ]
    }
   ],
   "source": [
    "# Check the KFP version\n",
    "!python3 -c \"import kfp; print('KFP version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1GX5KDOUJuI"
   },
   "source": [
    "If you're on colab, re-authorize after the kernel restart. **Edit the following cell for your project ID before running it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PpkxFp93xBk5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  PROJECT_ID = 'rthallam-demo-project'  # <---CHANGE THIS\n",
    "  !gcloud config set project {PROJECT_ID}\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "  USER_FLAG = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tskC13YxW7b3"
   },
   "source": [
    "### Set some variables\n",
    "\n",
    "**Before you run the next cell**, **edit it** to set variables for your project.  See the \"Before you begin\" section of the User Guide for information on creating your API key.  For `BUCKET_NAME`, enter the name of a Cloud Storage (GCS) bucket in your project.  Don't include the `gs://` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zHsVifdTW7b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n",
      "PIPELINE_ROOT: gs://cloud-ai-platform-2f444b6a-a742-444b-b91a-c7519f51bd77/pipeline_root/rthallam\n"
     ]
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "# Required Parameters\n",
    "USER = 'rthallam' # <---CHANGE THIS\n",
    "BUCKET_NAME = 'cloud-ai-platform-2f444b6a-a742-444b-b91a-c7519f51bd77'  # <---CHANGE THIS\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(BUCKET_NAME, USER)\n",
    "\n",
    "PROJECT_ID = 'rthallam-demo-project'  # <---CHANGE THIS\n",
    "REGION = 'us-central1'\n",
    "API_KEY = 'AIzaSyBtS73ieHd4K-7LwuJX6ghWuQmv3WVzFcs'  # <---CHANGE THIS\n",
    "\n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCi94vXkS-db"
   },
   "source": [
    "## Build custom container components\n",
    "\n",
    "\n",
    "We'll first build the two components that we'll use in our pipeline. The first component generates train and test data, and the second component consumes that data to train a model (to predict movie review sentiment).\n",
    "\n",
    "These components are based on custom Docker container images that we'll build and upload to the Google Container Registry, using Cloud Build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBs5UldB4_jI"
   },
   "source": [
    "### Container 1: Generate examples\n",
    "\n",
    "First, we'll define and write out the `generate_examples.py` code.  It generates train and test set files from the [IMDB review data](https://www.tensorflow.org/datasets/catalog/imdb_reviews#imdb_reviewssubwords8k), in `TFRecord` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "F2J01k4hZaOm"
   },
   "outputs": [],
   "source": [
    "!mkdir -p generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "48YXDQIiS-dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing generate/generate_examples.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generate/generate_examples.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "def _serialize_example(example, label):\n",
    "  example_value = tf.io.serialize_tensor(example).numpy()\n",
    "  label_value = tf.io.serialize_tensor(label).numpy()\n",
    "  feature = {\n",
    "      'examples':\n",
    "          tf.train.Feature(\n",
    "              bytes_list=tf.train.BytesList(value=[example_value])),\n",
    "      'labels':\n",
    "          tf.train.Feature(bytes_list=tf.train.BytesList(value=[label_value])),\n",
    "  }\n",
    "  return tf.train.Example(features=tf.train.Features(\n",
    "      feature=feature)).SerializeToString()\n",
    "\n",
    "\n",
    "def _tf_serialize_example(example, label):\n",
    "  serialized_tensor = tf.py_function(_serialize_example, (example, label),\n",
    "                                     tf.string)\n",
    "  return tf.reshape(serialized_tensor, ())\n",
    "\n",
    "\n",
    "def generate_examples(training_data_uri, test_data_uri, config_file_uri):\n",
    "  (train_data, test_data), info = tfds.load(\n",
    "      # Use the version pre-encoded with an ~8k vocabulary.\n",
    "      'imdb_reviews/subwords8k',\n",
    "      # Return the train/test datasets as a tuple.\n",
    "      split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "      # Return (example, label) pairs from the dataset (instead of a dictionary).\n",
    "      as_supervised=True,\n",
    "      with_info=True)\n",
    "\n",
    "  serialized_train_examples = train_data.map(_tf_serialize_example)\n",
    "  serialized_test_examples = test_data.map(_tf_serialize_example)\n",
    "\n",
    "  filename = os.path.join(training_data_uri, \"train.tfrecord\")\n",
    "  writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "  writer.write(serialized_train_examples)\n",
    "\n",
    "  filename = os.path.join(test_data_uri, \"test.tfrecord\")\n",
    "  writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "  writer.write(serialized_test_examples)\n",
    "\n",
    "  encoder = info.features['text'].encoder\n",
    "  config = {\n",
    "      'vocab_size': encoder.vocab_size,\n",
    "  }\n",
    "  config_file = os.path.join(config_file_uri, \"config\")\n",
    "  with tf.io.gfile.GFile(config_file, 'w') as f:\n",
    "    f.write(json.dumps(config))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--training_data_uri', type=str)\n",
    "  parser.add_argument('--test_data_uri', type=str)\n",
    "  parser.add_argument('--config_file_uri', type=str)\n",
    "\n",
    "  args = parser.parse_args()\n",
    "  generate_examples(args.training_data_uri, args.test_data_uri,\n",
    "                    args.config_file_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRFMWnhPS-df"
   },
   "source": [
    "Next, we'll create a Dockerfile that builds a container to run `generate_examples.py`. We are using a Google [Deep Learning Container](https://cloud.google.com/ai-platform/deep-learning-containers) image as our base, since the image already includes most of what we need. \n",
    "You may use your own image as the base image instead. Note that we're also installing the `tensorflow_datasets` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mzRx9XikS-df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing generate/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile generate/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\n",
    "WORKDIR /pipeline\n",
    "COPY generate_examples.py generate_examples.py\n",
    "RUN pip install tensorflow_datasets\n",
    "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMrMmGB5Hm8y"
   },
   "source": [
    "We'll use [Cloud Build](https://cloud.google.com/cloud-build/docs) to build the container image and write it to [GCR](https://cloud.google.com/container-registry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nQ3Y9CErXs9M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 2.5 KiB before compression.\n",
      "Uploading tarball of [generate] to [gs://rthallam-demo-project_cloudbuild/source/1619757303.39331-915c7bcb1cc94b699760bafee31b0eff.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/rthallam-demo-project/locations/global/builds/6f30c34c-b4ee-45c0-9c09-019758207233].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/6f30c34c-b4ee-45c0-9c09-019758207233?project=560224572293].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"6f30c34c-b4ee-45c0-9c09-019758207233\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://rthallam-demo-project_cloudbuild/source/1619757303.39331-915c7bcb1cc94b699760bafee31b0eff.tgz#1619757303635476\n",
      "Copying gs://rthallam-demo-project_cloudbuild/source/1619757303.39331-915c7bcb1cc94b699760bafee31b0eff.tgz#1619757303635476...\n",
      "/ [1 files][  1.1 KiB/  1.1 KiB]                                                \n",
      "Operation completed over 1 objects/1.1 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   5.12kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\n",
      "latest: Pulling from deeplearning-platform-release/tf2-cpu.2-3\n",
      "6e0aa5e7af40: Pulling fs layer\n",
      "d47239a868b3: Pulling fs layer\n",
      "49cbb10cca85: Pulling fs layer\n",
      "111e3a13fbd4: Pulling fs layer\n",
      "b35c22f6a9bc: Pulling fs layer\n",
      "8206ae2b86e3: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "a9b2fba996c8: Pulling fs layer\n",
      "57c8f95170d4: Pulling fs layer\n",
      "217c82b1f15a: Pulling fs layer\n",
      "973cc805e91c: Pulling fs layer\n",
      "c425ab4c2c48: Pulling fs layer\n",
      "aed13e0eac71: Pulling fs layer\n",
      "8af440fcd8dd: Pulling fs layer\n",
      "dd95c6ad22f2: Pulling fs layer\n",
      "4e6c4135d19b: Pulling fs layer\n",
      "3d9b09ca1e5d: Pulling fs layer\n",
      "8d3d61835fa5: Pulling fs layer\n",
      "1056ded2ebce: Pulling fs layer\n",
      "8d251a009545: Pulling fs layer\n",
      "35ef6f473609: Pulling fs layer\n",
      "111e3a13fbd4: Waiting\n",
      "b35c22f6a9bc: Waiting\n",
      "8206ae2b86e3: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "a9b2fba996c8: Waiting\n",
      "57c8f95170d4: Waiting\n",
      "217c82b1f15a: Waiting\n",
      "973cc805e91c: Waiting\n",
      "c425ab4c2c48: Waiting\n",
      "aed13e0eac71: Waiting\n",
      "8af440fcd8dd: Waiting\n",
      "dd95c6ad22f2: Waiting\n",
      "4e6c4135d19b: Waiting\n",
      "3d9b09ca1e5d: Waiting\n",
      "8d3d61835fa5: Waiting\n",
      "1056ded2ebce: Waiting\n",
      "8d251a009545: Waiting\n",
      "35ef6f473609: Waiting\n",
      "49cbb10cca85: Verifying Checksum\n",
      "49cbb10cca85: Download complete\n",
      "d47239a868b3: Download complete\n",
      "111e3a13fbd4: Download complete\n",
      "6e0aa5e7af40: Verifying Checksum\n",
      "6e0aa5e7af40: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "a9b2fba996c8: Verifying Checksum\n",
      "a9b2fba996c8: Download complete\n",
      "8206ae2b86e3: Verifying Checksum\n",
      "8206ae2b86e3: Download complete\n",
      "217c82b1f15a: Verifying Checksum\n",
      "217c82b1f15a: Download complete\n",
      "973cc805e91c: Verifying Checksum\n",
      "973cc805e91c: Download complete\n",
      "c425ab4c2c48: Verifying Checksum\n",
      "c425ab4c2c48: Download complete\n",
      "aed13e0eac71: Verifying Checksum\n",
      "aed13e0eac71: Download complete\n",
      "8af440fcd8dd: Verifying Checksum\n",
      "8af440fcd8dd: Download complete\n",
      "dd95c6ad22f2: Verifying Checksum\n",
      "dd95c6ad22f2: Download complete\n",
      "57c8f95170d4: Verifying Checksum\n",
      "57c8f95170d4: Download complete\n",
      "4e6c4135d19b: Verifying Checksum\n",
      "4e6c4135d19b: Download complete\n",
      "3d9b09ca1e5d: Verifying Checksum\n",
      "3d9b09ca1e5d: Download complete\n",
      "b35c22f6a9bc: Verifying Checksum\n",
      "b35c22f6a9bc: Download complete\n",
      "8d251a009545: Verifying Checksum\n",
      "8d251a009545: Download complete\n",
      "1056ded2ebce: Verifying Checksum\n",
      "1056ded2ebce: Download complete\n",
      "35ef6f473609: Verifying Checksum\n",
      "35ef6f473609: Download complete\n",
      "6e0aa5e7af40: Pull complete\n",
      "d47239a868b3: Pull complete\n",
      "49cbb10cca85: Pull complete\n",
      "111e3a13fbd4: Pull complete\n",
      "b35c22f6a9bc: Pull complete\n",
      "8d3d61835fa5: Verifying Checksum\n",
      "8d3d61835fa5: Download complete\n",
      "8206ae2b86e3: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "a9b2fba996c8: Pull complete\n",
      "57c8f95170d4: Pull complete\n",
      "217c82b1f15a: Pull complete\n",
      "973cc805e91c: Pull complete\n",
      "c425ab4c2c48: Pull complete\n",
      "aed13e0eac71: Pull complete\n",
      "8af440fcd8dd: Pull complete\n",
      "dd95c6ad22f2: Pull complete\n",
      "4e6c4135d19b: Pull complete\n",
      "3d9b09ca1e5d: Pull complete\n",
      "8d3d61835fa5: Pull complete\n",
      "1056ded2ebce: Pull complete\n",
      "8d251a009545: Pull complete\n",
      "35ef6f473609: Pull complete\n",
      "Digest: sha256:f1a829d1256ccf879a401190ab1bc461212b0c3bb538f56cd1761e0e462e413e\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\n",
      " ---> 61f75c259476\n",
      "Step 2/5 : WORKDIR /pipeline\n",
      " ---> Running in 88a8ef726b7e\n",
      "Removing intermediate container 88a8ef726b7e\n",
      " ---> 62174bf54080\n",
      "Step 3/5 : COPY generate_examples.py generate_examples.py\n",
      " ---> 3f52319bf971\n",
      "Step 4/5 : RUN pip install tensorflow_datasets\n",
      " ---> Running in 44b0d681d302\n",
      "Requirement already satisfied: tensorflow_datasets in /opt/conda/lib/python3.7/site-packages (3.0.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (3.15.8)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (0.3.1.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (4.60.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (2.25.1)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (0.10.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (1.12.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (1.19.5)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (0.26.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_datasets) (20.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (4.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\n",
      "Removing intermediate container 44b0d681d302\n",
      " ---> a8448e577b70\n",
      "Step 5/5 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      " ---> Running in b4e244825679\n",
      "Removing intermediate container b4e244825679\n",
      " ---> adb6ce13664e\n",
      "Successfully built adb6ce13664e\n",
      "Successfully tagged gcr.io/rthallam-demo-project/custom-container-generate:rthallam\n",
      "PUSH\n",
      "Pushing gcr.io/rthallam-demo-project/custom-container-generate:rthallam\n",
      "The push refers to repository [gcr.io/rthallam-demo-project/custom-container-generate]\n",
      "8a3dca9a2ead: Preparing\n",
      "30be20d457ea: Preparing\n",
      "93ecd616ca3c: Preparing\n",
      "4324fed28be4: Preparing\n",
      "cc6e7f002a0b: Preparing\n",
      "8bb57749c9a7: Preparing\n",
      "98daa89d057a: Preparing\n",
      "37a8adb4fafb: Preparing\n",
      "25eacd11cb14: Preparing\n",
      "8bc9ffacd92c: Preparing\n",
      "640b32ec2e81: Preparing\n",
      "a803e99a9e4f: Preparing\n",
      "6fd6ff343521: Preparing\n",
      "889232675b1d: Preparing\n",
      "037b4e8512b6: Preparing\n",
      "15d726558b56: Preparing\n",
      "8bb57749c9a7: Waiting\n",
      "98daa89d057a: Waiting\n",
      "37a8adb4fafb: Waiting\n",
      "1f2e24bbaee1: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "31152d177e09: Preparing\n",
      "3cec1b907843: Preparing\n",
      "dcc1b8729dd0: Preparing\n",
      "6f15325cc380: Preparing\n",
      "1e77dd81f9fa: Preparing\n",
      "030309cad0ba: Preparing\n",
      "25eacd11cb14: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "8bc9ffacd92c: Waiting\n",
      "640b32ec2e81: Waiting\n",
      "31152d177e09: Waiting\n",
      "3cec1b907843: Waiting\n",
      "a803e99a9e4f: Waiting\n",
      "dcc1b8729dd0: Waiting\n",
      "6f15325cc380: Waiting\n",
      "6fd6ff343521: Waiting\n",
      "1e77dd81f9fa: Waiting\n",
      "889232675b1d: Waiting\n",
      "030309cad0ba: Waiting\n",
      "037b4e8512b6: Waiting\n",
      "15d726558b56: Waiting\n",
      "1f2e24bbaee1: Waiting\n",
      "cc6e7f002a0b: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "4324fed28be4: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "8bb57749c9a7: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "98daa89d057a: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "30be20d457ea: Pushed\n",
      "93ecd616ca3c: Pushed\n",
      "25eacd11cb14: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "8a3dca9a2ead: Pushed\n",
      "37a8adb4fafb: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "8bc9ffacd92c: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "640b32ec2e81: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "6fd6ff343521: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "a803e99a9e4f: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "889232675b1d: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "037b4e8512b6: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "5f70bf18a086: Layer already exists\n",
      "15d726558b56: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "1f2e24bbaee1: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "31152d177e09: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "6f15325cc380: Layer already exists\n",
      "dcc1b8729dd0: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "1e77dd81f9fa: Layer already exists\n",
      "3cec1b907843: Mounted from deeplearning-platform-release/tf2-cpu.2-3\n",
      "030309cad0ba: Layer already exists\n",
      "rthallam: digest: sha256:684c80b4fdd05317175dcbbad6709c3c2b4bcfc6dab54e24fc148cc825e82ba1 size: 5334\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                              IMAGES                                                           STATUS\n",
      "6f30c34c-b4ee-45c0-9c09-019758207233  2021-04-30T04:35:03+00:00  2M44S     gs://rthallam-demo-project_cloudbuild/source/1619757303.39331-915c7bcb1cc94b699760bafee31b0eff.tgz  gcr.io/rthallam-demo-project/custom-container-generate:rthallam  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag gcr.io/{PROJECT_ID}/custom-container-generate:{USER} generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEJdna3RS-ds"
   },
   "source": [
    "### Container 2: Train Examples\n",
    "\n",
    "Next, we'll do the same for the 'Train Examples' custom container. We'll first write out a `train_examples.py` file, then build a container that runs it.  This script takes as input training and test data in `TFRecords` format and trains a Keras binary classification model to predict review sentiment. When training has finished, it writes out model and metrics information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "he0VlF2Hbghh"
   },
   "outputs": [],
   "source": [
    "!mkdir -p train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "E2t0eLglS-ds"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train/train_examples.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train/train_examples.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def _parse_example(record):\n",
    "  f = {\n",
    "      'examples': tf.io.FixedLenFeature((), tf.string, default_value=''),\n",
    "      'labels': tf.io.FixedLenFeature((), tf.string, default_value='')\n",
    "  }\n",
    "  return tf.io.parse_single_example(record, f)\n",
    "\n",
    "\n",
    "def _to_tensor(record):\n",
    "  examples = tf.io.parse_tensor(record['examples'], tf.int64)\n",
    "  labels = tf.io.parse_tensor(record['labels'], tf.int64)\n",
    "  return (examples, labels)\n",
    "\n",
    "\n",
    "def train_examples(training_data_uri, test_data_uri, config_file_uri,\n",
    "                   output_model_uri, output_metrics_uri):\n",
    "  train_examples = tf.data.TFRecordDataset(\n",
    "      [os.path.join(training_data_uri, 'train.tfrecord')])\n",
    "  test_examples = tf.data.TFRecordDataset(\n",
    "      [os.path.join(test_data_uri, 'test.tfrecord')])\n",
    "\n",
    "  train_batches = train_examples.map(_parse_example).map(_to_tensor)\n",
    "  test_batches = test_examples.map(_parse_example).map(_to_tensor)\n",
    "\n",
    "  with tf.io.gfile.GFile(os.path.join(config_file_uri, 'config')) as f:\n",
    "    config = json.loads(f.read())\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Embedding(config['vocab_size'], 16),\n",
    "      tf.keras.layers.GlobalAveragePooling1D(),\n",
    "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "  model.compile(\n",
    "      optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "  train_batches = train_batches.shuffle(1000).padded_batch(\n",
    "      32, (tf.TensorShape([None]), tf.TensorShape([])))\n",
    "\n",
    "  test_batches = test_batches.padded_batch(\n",
    "      32, (tf.TensorShape([None]), tf.TensorShape([])))\n",
    "\n",
    "  history = model.fit(\n",
    "      train_batches,\n",
    "      epochs=10,\n",
    "      validation_data=test_batches,\n",
    "      validation_steps=30)\n",
    "\n",
    "  loss, accuracy = model.evaluate(test_batches)\n",
    "\n",
    "  metrics = {\n",
    "      'loss': str(loss),\n",
    "      'accuracy': str(accuracy),\n",
    "  }\n",
    "\n",
    "  model_json = model.to_json()\n",
    "  with tf.io.gfile.GFile(os.path.join(output_model_uri, 'model.json'),\n",
    "                         'w') as f:\n",
    "    f.write(model_json)\n",
    "\n",
    "  with tf.io.gfile.GFile(os.path.join(output_metrics_uri, 'metrics.json'),\n",
    "                         'w') as f:\n",
    "    f.write(json.dumps(metrics))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--training_data_uri', type=str)\n",
    "  parser.add_argument('--test_data_uri', type=str)\n",
    "  parser.add_argument('--config_file_uri', type=str)\n",
    "  parser.add_argument('--output_model_uri', type=str)\n",
    "  parser.add_argument('--output_metrics_uri', type=str)\n",
    "\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  train_examples(args.training_data_uri, args.test_data_uri,\n",
    "                 args.config_file_uri, args.output_model_uri,\n",
    "                 args.output_metrics_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVJyxdeCS-du"
   },
   "source": [
    "Next, we'll create a Dockerfile that builds a container to run `train_examples.py`.  Again we're using a Google [Deep Learning Container](https://cloud.google.com/ai-platform/deep-learning-containers) image as our base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HoDYRpzlS-dv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile train/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\n",
    "WORKDIR /pipeline\n",
    "COPY train_examples.py train_examples.py\n",
    "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgd_zce_AXYv"
   },
   "source": [
    "We'll use [Cloud Build](https://cloud.google.com/cloud-build/docs) to build the container image and write it to [GCR](https://cloud.google.com/container-registry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YPmhXcXFbqVK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 2.8 KiB before compression.\n",
      "Uploading tarball of [train] to [gs://rthallam-demo-project_cloudbuild/source/1619757504.015502-342545a9f35d4326bb8ade97ba2a2cf3.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/rthallam-demo-project/locations/global/builds/3a647dbb-6daa-48ee-aed8-79f35da97431].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/3a647dbb-6daa-48ee-aed8-79f35da97431?project=560224572293].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"3a647dbb-6daa-48ee-aed8-79f35da97431\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://rthallam-demo-project_cloudbuild/source/1619757504.015502-342545a9f35d4326bb8ade97ba2a2cf3.tgz#1619757504215515\n",
      "Copying gs://rthallam-demo-project_cloudbuild/source/1619757504.015502-342545a9f35d4326bb8ade97ba2a2cf3.tgz#1619757504215515...\n",
      "/ [1 files][  1.1 KiB/  1.1 KiB]                                                \n",
      "Operation completed over 1 objects/1.1 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  5.632kB\n",
      "Step 1/4 : FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\n",
      "latest: Pulling from deeplearning-platform-release/tf2-cpu.2-3\n",
      "6e0aa5e7af40: Pulling fs layer\n",
      "d47239a868b3: Pulling fs layer\n",
      "49cbb10cca85: Pulling fs layer\n",
      "111e3a13fbd4: Pulling fs layer\n",
      "b35c22f6a9bc: Pulling fs layer\n",
      "8206ae2b86e3: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "a9b2fba996c8: Pulling fs layer\n",
      "57c8f95170d4: Pulling fs layer\n",
      "217c82b1f15a: Pulling fs layer\n",
      "973cc805e91c: Pulling fs layer\n",
      "c425ab4c2c48: Pulling fs layer\n",
      "aed13e0eac71: Pulling fs layer\n",
      "8af440fcd8dd: Pulling fs layer\n",
      "dd95c6ad22f2: Pulling fs layer\n",
      "4e6c4135d19b: Pulling fs layer\n",
      "3d9b09ca1e5d: Pulling fs layer\n",
      "8d3d61835fa5: Pulling fs layer\n",
      "1056ded2ebce: Pulling fs layer\n",
      "8d251a009545: Pulling fs layer\n",
      "35ef6f473609: Pulling fs layer\n",
      "111e3a13fbd4: Waiting\n",
      "b35c22f6a9bc: Waiting\n",
      "8206ae2b86e3: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "a9b2fba996c8: Waiting\n",
      "57c8f95170d4: Waiting\n",
      "217c82b1f15a: Waiting\n",
      "973cc805e91c: Waiting\n",
      "c425ab4c2c48: Waiting\n",
      "aed13e0eac71: Waiting\n",
      "8af440fcd8dd: Waiting\n",
      "dd95c6ad22f2: Waiting\n",
      "4e6c4135d19b: Waiting\n",
      "3d9b09ca1e5d: Waiting\n",
      "8d3d61835fa5: Waiting\n",
      "1056ded2ebce: Waiting\n",
      "8d251a009545: Waiting\n",
      "35ef6f473609: Waiting\n",
      "49cbb10cca85: Verifying Checksum\n",
      "49cbb10cca85: Download complete\n",
      "d47239a868b3: Download complete\n",
      "111e3a13fbd4: Verifying Checksum\n",
      "111e3a13fbd4: Download complete\n",
      "6e0aa5e7af40: Verifying Checksum\n",
      "6e0aa5e7af40: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "a9b2fba996c8: Verifying Checksum\n",
      "a9b2fba996c8: Download complete\n",
      "8206ae2b86e3: Verifying Checksum\n",
      "8206ae2b86e3: Download complete\n",
      "217c82b1f15a: Verifying Checksum\n",
      "217c82b1f15a: Download complete\n",
      "973cc805e91c: Verifying Checksum\n",
      "973cc805e91c: Download complete\n",
      "c425ab4c2c48: Verifying Checksum\n",
      "c425ab4c2c48: Download complete\n",
      "aed13e0eac71: Verifying Checksum\n",
      "aed13e0eac71: Download complete\n",
      "8af440fcd8dd: Verifying Checksum\n",
      "8af440fcd8dd: Download complete\n",
      "57c8f95170d4: Verifying Checksum\n",
      "57c8f95170d4: Download complete\n",
      "4e6c4135d19b: Verifying Checksum\n",
      "4e6c4135d19b: Download complete\n",
      "dd95c6ad22f2: Verifying Checksum\n",
      "dd95c6ad22f2: Download complete\n",
      "3d9b09ca1e5d: Verifying Checksum\n",
      "3d9b09ca1e5d: Download complete\n",
      "b35c22f6a9bc: Verifying Checksum\n",
      "b35c22f6a9bc: Download complete\n",
      "8d251a009545: Verifying Checksum\n",
      "8d251a009545: Download complete\n",
      "35ef6f473609: Verifying Checksum\n",
      "35ef6f473609: Download complete\n",
      "1056ded2ebce: Verifying Checksum\n",
      "1056ded2ebce: Download complete\n",
      "6e0aa5e7af40: Pull complete\n",
      "d47239a868b3: Pull complete\n",
      "49cbb10cca85: Pull complete\n",
      "111e3a13fbd4: Pull complete\n",
      "8d3d61835fa5: Verifying Checksum\n",
      "8d3d61835fa5: Download complete\n",
      "b35c22f6a9bc: Pull complete\n",
      "8206ae2b86e3: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "a9b2fba996c8: Pull complete\n",
      "57c8f95170d4: Pull complete\n",
      "217c82b1f15a: Pull complete\n",
      "973cc805e91c: Pull complete\n",
      "c425ab4c2c48: Pull complete\n",
      "aed13e0eac71: Pull complete\n",
      "8af440fcd8dd: Pull complete\n",
      "dd95c6ad22f2: Pull complete\n",
      "4e6c4135d19b: Pull complete\n",
      "3d9b09ca1e5d: Pull complete\n",
      "8d3d61835fa5: Pull complete\n",
      "1056ded2ebce: Pull complete\n",
      "8d251a009545: Pull complete\n",
      "35ef6f473609: Pull complete\n",
      "Digest: sha256:f1a829d1256ccf879a401190ab1bc461212b0c3bb538f56cd1761e0e462e413e\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\n",
      " ---> 61f75c259476\n",
      "Step 2/4 : WORKDIR /pipeline\n",
      " ---> Running in 4d7b45ec7cfc\n",
      "Removing intermediate container 4d7b45ec7cfc\n",
      " ---> 2dcfa085aadb\n",
      "Step 3/4 : COPY train_examples.py train_examples.py\n",
      " ---> d4843c2640bb\n",
      "Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      " ---> Running in 74d826300ea4\n",
      "Removing intermediate container 74d826300ea4\n",
      " ---> 6d859e110f07\n",
      "Successfully built 6d859e110f07\n",
      "Successfully tagged gcr.io/rthallam-demo-project/custom-container-train:rthallam\n",
      "PUSH\n",
      "Pushing gcr.io/rthallam-demo-project/custom-container-train:rthallam\n",
      "The push refers to repository [gcr.io/rthallam-demo-project/custom-container-train]\n",
      "5f9d3bd2c6bc: Preparing\n",
      "028e75f9481b: Preparing\n",
      "4324fed28be4: Preparing\n",
      "cc6e7f002a0b: Preparing\n",
      "8bb57749c9a7: Preparing\n",
      "98daa89d057a: Preparing\n",
      "37a8adb4fafb: Preparing\n",
      "25eacd11cb14: Preparing\n",
      "8bc9ffacd92c: Preparing\n",
      "640b32ec2e81: Preparing\n",
      "a803e99a9e4f: Preparing\n",
      "6fd6ff343521: Preparing\n",
      "889232675b1d: Preparing\n",
      "037b4e8512b6: Preparing\n",
      "15d726558b56: Preparing\n",
      "1f2e24bbaee1: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "31152d177e09: Preparing\n",
      "3cec1b907843: Preparing\n",
      "dcc1b8729dd0: Preparing\n",
      "6f15325cc380: Preparing\n",
      "1e77dd81f9fa: Preparing\n",
      "030309cad0ba: Preparing\n",
      "98daa89d057a: Waiting\n",
      "37a8adb4fafb: Waiting\n",
      "25eacd11cb14: Waiting\n",
      "8bc9ffacd92c: Waiting\n",
      "640b32ec2e81: Waiting\n",
      "a803e99a9e4f: Waiting\n",
      "6fd6ff343521: Waiting\n",
      "889232675b1d: Waiting\n",
      "037b4e8512b6: Waiting\n",
      "15d726558b56: Waiting\n",
      "1f2e24bbaee1: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "31152d177e09: Waiting\n",
      "3cec1b907843: Waiting\n",
      "dcc1b8729dd0: Waiting\n",
      "6f15325cc380: Waiting\n",
      "1e77dd81f9fa: Waiting\n",
      "030309cad0ba: Waiting\n",
      "8bb57749c9a7: Layer already exists\n",
      "4324fed28be4: Layer already exists\n",
      "cc6e7f002a0b: Layer already exists\n",
      "37a8adb4fafb: Layer already exists\n",
      "98daa89d057a: Layer already exists\n",
      "25eacd11cb14: Layer already exists\n",
      "640b32ec2e81: Layer already exists\n",
      "8bc9ffacd92c: Layer already exists\n",
      "a803e99a9e4f: Layer already exists\n",
      "6fd6ff343521: Layer already exists\n",
      "889232675b1d: Layer already exists\n",
      "037b4e8512b6: Layer already exists\n",
      "15d726558b56: Layer already exists\n",
      "1f2e24bbaee1: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "31152d177e09: Layer already exists\n",
      "3cec1b907843: Layer already exists\n",
      "dcc1b8729dd0: Layer already exists\n",
      "6f15325cc380: Layer already exists\n",
      "1e77dd81f9fa: Layer already exists\n",
      "030309cad0ba: Layer already exists\n",
      "5f9d3bd2c6bc: Pushed\n",
      "028e75f9481b: Pushed\n",
      "rthallam: digest: sha256:3c95d4ab05a49a60cf5e79492159803749e2a4ce4504b869d6172d202297ab45 size: 5126\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                               IMAGES                                                        STATUS\n",
      "3a647dbb-6daa-48ee-aed8-79f35da97431  2021-04-30T04:38:24+00:00  2M36S     gs://rthallam-demo-project_cloudbuild/source/1619757504.015502-342545a9f35d4326bb8ade97ba2a2cf3.tgz  gcr.io/rthallam-demo-project/custom-container-train:rthallam  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag gcr.io/{PROJECT_ID}/custom-container-train:{USER} train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3BqUxQ_9c31"
   },
   "source": [
    "### Create pipeline components using the custom container images\n",
    "\n",
    "Next, we'll define components for the 'generate' and 'train' steps, using the container images we just built.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "c2llvI0w9r3f"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from kfp import components\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQAff2NRAHKZ"
   },
   "source": [
    "The 'generate' component specifies three outputs: training and test data, of type `Dataset`, and a config file, of type `File`. \n",
    "\n",
    "The component definition uses  `outputUri`  in specifying the `generate_example.py` script args.  These args are set to automatically-generated GCS URIs, and when `generate_examples` writes to those URIs, the outputs are available to downstream components.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XKqGpulg9b_J"
   },
   "outputs": [],
   "source": [
    "generate_op = components.load_component_from_text(\"\"\"\n",
    "name: GenerateExamples\n",
    "outputs:\n",
    "- {name: training_data, type: Dataset}\n",
    "- {name: test_data, type: Dataset}\n",
    "- {name: config_file, type: File}\n",
    "implementation:\n",
    "  container:\n",
    "    image: gcr.io/%s/custom-container-generate:%s\n",
    "    command:\n",
    "    - python\n",
    "    - /pipeline/generate_examples.py\n",
    "    args:\n",
    "    - --training_data_uri\n",
    "    - {outputUri: training_data}\n",
    "    - --test_data_uri\n",
    "    - {outputUri: test_data}\n",
    "    - --config_file_uri\n",
    "    - {outputUri: config_file}\n",
    "\"\"\" % (PROJECT_ID, USER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8qtwaqU9zbC"
   },
   "source": [
    "The train component takes as input training and test data of type `Dataset`, and a config `File`: it can consume the outputs of the \"generate\" component.   It specifies two outputs, one of type `Model` and one of type `Metrics`.\n",
    "\n",
    "The component definition uses  `inputUri` and `outputUri` when passing args to the `train_examples` script. So, the script's arg values will be GCS URIs, from which it will read its inputs and write its outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IM_tmbv09prn"
   },
   "outputs": [],
   "source": [
    "train_op = components.load_component_from_text(\"\"\"\n",
    "name: Train\n",
    "inputs:\n",
    "- {name: training_data, type: Dataset}\n",
    "- {name: test_data, type: Dataset}\n",
    "- {name: config_file, type: File}\n",
    "outputs:\n",
    "- {name: model, type: Model}\n",
    "- {name: metrics, type: Metrics}\n",
    "implementation:\n",
    "  container:\n",
    "    image: gcr.io/%s/custom-container-train:%s\n",
    "    command:\n",
    "    - python\n",
    "    - /pipeline/train_examples.py\n",
    "    args:\n",
    "    - --training_data_uri\n",
    "    - {inputUri: training_data}\n",
    "    - --test_data_uri\n",
    "    - {inputUri: test_data}\n",
    "    - --config_file_uri\n",
    "    - {inputUri: config_file}\n",
    "    - --output_model_uri\n",
    "    - {outputUri: model}\n",
    "    - --output_metrics_uri\n",
    "    - {outputUri: metrics}\n",
    "\"\"\" % (PROJECT_ID, USER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1r1CWF0S-d7"
   },
   "source": [
    "## Define a KFP pipeline that uses the components\n",
    "\n",
    "Now we're ready to define a pipeline that uses these components. The `train` step takes its inputs from the `generate` step's outputs. \n",
    "\n",
    "Note also that we are able to define pipeline *resource* specs, which we do here for the training step, including memory constraints, the number of GPUs to allocate, and the type of accelerator to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tYhopKUBS-d7"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name='custom-container-pipeline-{}-{}'.format(USER, str(int(time.time()))))\n",
    "def pipeline():\n",
    "  generate = generate_op()\n",
    "  train = (train_op(\n",
    "      training_data=generate.outputs['training_data'],\n",
    "      test_data=generate.outputs['test_data'],\n",
    "      config_file=generate.outputs['config_file']).\n",
    "    set_cpu_limit('4').\n",
    "    set_memory_limit('14Gi').\n",
    "    add_node_selector_constraint(\n",
    "      'cloud.google.com/gke-accelerator',\n",
    "      'nvidia-tesla-k80').\n",
    "    set_gpu_limit(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huP7loXFG6s8"
   },
   "source": [
    "Compile the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6qf9KkkoA1y7"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, \n",
    "                            package_path='custom_container_pipeline_spec.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mH5QFfSuW7cJ"
   },
   "source": [
    "### Submit the pipeline job\n",
    "\n",
    "Here, we'll create an API client using the API key you generated.\n",
    "\n",
    "Then, we'll submit the pipeline job by passing the compiled spec to the `create_run_from_job_spec()` method. Note that we're passing a `parameter_values` dict that specifies the pipeline input parameters we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NSnrYUDAW7cK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/ai/platform/locations/us-central1/pipelines/runs/custom-container-pipeline-rthallam-1619799291-20210430161604?e=CaipPipelinesAlphaLaunch::CaipPipelinesAlphaEnabled,BackendzOverridingLaunch::BackendzOverridingEnabled,CloudAiLaunch::CloudAiEnabled&project=rthallam-demo-project\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from aiplatform.pipelines import client\n",
    "\n",
    "api_client = client.Client(project_id=PROJECT_ID, region=REGION, api_key=API_KEY)\n",
    "\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path='custom_container_pipeline_spec.json',\n",
    "    pipeline_root=PIPELINE_ROOT, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhfJYO3T613t"
   },
   "source": [
    "## Query the metadata produced by the pipeline.\n",
    "\n",
    "The set of artifacts and executions produced by the pipeline can also be queried using the AIPlatform Metadata SDK. The following shows a snippet for querying the metadata for a given pipeline run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "csZSsQHO1ZdQ"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "from google import auth\n",
    "from google.cloud.aiplatform_v1alpha1 import MetadataServiceClient\n",
    "from google.auth.transport import grpc, requests\n",
    "from google.cloud.aiplatform_v1alpha1.services.metadata_service.transports import grpc as transports_grpc\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _initialize_metadata_service_client() -> MetadataServiceClient:\n",
    "  scope = 'https://www.googleapis.com/auth/cloud-platform'\n",
    "  api_uri = 'us-central1-aiplatform.googleapis.com'\n",
    "  credentials, _ = auth.default(scopes=(scope,))\n",
    "  request = requests.Request()\n",
    "  channel = grpc.secure_authorized_channel(credentials, request, api_uri)\n",
    "\n",
    "  return MetadataServiceClient(\n",
    "      transport=transports_grpc.MetadataServiceGrpcTransport(channel=channel))\n",
    "\n",
    "client = _initialize_metadata_service_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "9hBBZz5g41Ey"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artifacts {\n",
       "  name: \"projects/560224572293/locations/us-central1/metadataStores/default/artifacts/18231325568793775957\"\n",
       "  display_name: \"training_data\"\n",
       "  instance_schema_title: \"system.Dataset\"\n",
       "  uri: \"gs://cloud-ai-platform-2f444b6a-a742-444b-b91a-c7519f51bd77/pipeline_root/rthallam/560224572293/custom-container-pipeline-rthallam-1619799291-20210430161604/task-generateexamples_827184587808440320/training_data\"\n",
       "  etag: \"1619800138293\"\n",
       "  create_time {\n",
       "    seconds: 1619799376\n",
       "    nanos: 446000000\n",
       "  }\n",
       "  update_time {\n",
       "    seconds: 1619800138\n",
       "    nanos: 293000000\n",
       "  }\n",
       "  state: LIVE\n",
       "}\n",
       "artifacts {\n",
       "  name: \"projects/560224572293/locations/us-central1/metadataStores/default/artifacts/4398058870343032914\"\n",
       "  display_name: \"config_file\"\n",
       "  instance_schema_title: \"system.Artifact\"\n",
       "  uri: \"gs://cloud-ai-platform-2f444b6a-a742-444b-b91a-c7519f51bd77/pipeline_root/rthallam/560224572293/custom-container-pipeline-rthallam-1619799291-20210430161604/task-generateexamples_827184587808440320/config_file\"\n",
       "  etag: \"1619800137696\"\n",
       "  create_time {\n",
       "    seconds: 1619799377\n",
       "    nanos: 650000000\n",
       "  }\n",
       "  update_time {\n",
       "    seconds: 1619800137\n",
       "    nanos: 696000000\n",
       "  }\n",
       "  state: LIVE\n",
       "}\n",
       "artifacts {\n",
       "  name: \"projects/560224572293/locations/us-central1/metadataStores/default/artifacts/13099934123442219092\"\n",
       "  display_name: \"test_data\"\n",
       "  instance_schema_title: \"system.Dataset\"\n",
       "  uri: \"gs://cloud-ai-platform-2f444b6a-a742-444b-b91a-c7519f51bd77/pipeline_root/rthallam/560224572293/custom-container-pipeline-rthallam-1619799291-20210430161604/task-generateexamples_827184587808440320/test_data\"\n",
       "  etag: \"1619800137963\"\n",
       "  create_time {\n",
       "    seconds: 1619799378\n",
       "    nanos: 292000000\n",
       "  }\n",
       "  update_time {\n",
       "    seconds: 1619800137\n",
       "    nanos: 963000000\n",
       "  }\n",
       "  state: LIVE\n",
       "}\n",
       "artifacts {\n",
       "  name: \"projects/560224572293/locations/us-central1/metadataStores/default/artifacts/14398509906326841459\"\n",
       "  display_name: \"metrics\"\n",
       "  instance_schema_title: \"system.Metrics\"\n",
       "  uri: \"gs://cloud-ai-platform-2f444b6a-a742-444b-b91a-c7519f51bd77/pipeline_root/rthallam/560224572293/custom-container-pipeline-rthallam-1619799291-20210430161604/task-train_-8396187449046335488/metrics\"\n",
       "  etag: \"1619800813129\"\n",
       "  create_time {\n",
       "    seconds: 1619800141\n",
       "    nanos: 864000000\n",
       "  }\n",
       "  update_time {\n",
       "    seconds: 1619800813\n",
       "    nanos: 129000000\n",
       "  }\n",
       "  state: LIVE\n",
       "}\n",
       "artifacts {\n",
       "  name: \"projects/560224572293/locations/us-central1/metadataStores/default/artifacts/4263002386027186685\"\n",
       "  display_name: \"model\"\n",
       "  instance_schema_title: \"system.Model\"\n",
       "  uri: \"gs://cloud-ai-platform-2f444b6a-a742-444b-b91a-c7519f51bd77/pipeline_root/rthallam/560224572293/custom-container-pipeline-rthallam-1619799291-20210430161604/task-train_-8396187449046335488/model\"\n",
       "  etag: \"1619800813437\"\n",
       "  create_time {\n",
       "    seconds: 1619800142\n",
       "    nanos: 172000000\n",
       "  }\n",
       "  update_time {\n",
       "    seconds: 1619800813\n",
       "    nanos: 437000000\n",
       "  }\n",
       "  state: LIVE\n",
       "}\n",
       "executions {\n",
       "  name: \"projects/560224572293/locations/us-central1/metadataStores/default/executions/452090490998092461\"\n",
       "  display_name: \"custom-container-pipeline-rthallam-1619799291-20210430161604\"\n",
       "  instance_schema_title: \"system.Run\"\n",
       "  state: COMPLETE\n",
       "  etag: \"1619800814566\"\n",
       "  create_time {\n",
       "    seconds: 1619799367\n",
       "    nanos: 41000000\n",
       "  }\n",
       "  update_time {\n",
       "    seconds: 1619800814\n",
       "    nanos: 566000000\n",
       "  }\n",
       "}\n",
       "executions {\n",
       "  name: \"projects/560224572293/locations/us-central1/metadataStores/default/executions/11443974765889496760\"\n",
       "  display_name: \"task-generateexamples\"\n",
       "  instance_schema_title: \"system.ContainerExecution\"\n",
       "  state: COMPLETE\n",
       "  etag: \"1619800137468\"\n",
       "  create_time {\n",
       "    seconds: 1619799371\n",
       "    nanos: 630000000\n",
       "  }\n",
       "  update_time {\n",
       "    seconds: 1619800137\n",
       "    nanos: 468000000\n",
       "  }\n",
       "}\n",
       "executions {\n",
       "  name: \"projects/560224572293/locations/us-central1/metadataStores/default/executions/17550896927101739842\"\n",
       "  display_name: \"task-train\"\n",
       "  instance_schema_title: \"system.ContainerExecution\"\n",
       "  state: COMPLETE\n",
       "  etag: \"1619800812742\"\n",
       "  create_time {\n",
       "    seconds: 1619800139\n",
       "    nanos: 613000000\n",
       "  }\n",
       "  update_time {\n",
       "    seconds: 1619800812\n",
       "    nanos: 742000000\n",
       "  }\n",
       "}\n",
       "events {\n",
       "  artifact: \"projects/560224572293/locations/us-central1/metadataStores/default/artifacts/14398509906326841459\"\n",
       "  execution: \"projects/560224572293/locations/us-central1/metadataStores/default/executions/452090490998092461\"\n",
       "  event_time {\n",
       "  }\n",
       "  type_: OUTPUT\n",
       "  labels {\n",
       "    key: \"name\"\n",
       "    value: \"task-train-metrics\"\n",
       "  }\n",
       "}\n",
       "events {\n",
       "  artifact: \"projects/560224572293/locations/us-central1/metadataStores/default/artifacts/18231325568793775957\"\n",
       "  execution: \"projects/560224572293/locations/us-central1/metadataStores/default/executions/11443974765889496760\"\n",
       "  event_time {\n",
       "  }\n",
       "  type_: OUTPUT\n",
       "  labels {\n",
       "    key: \"name\"\n",
       "    value: \"training_data\"\n",
       "  }\n",
       "}\n",
       "events {\n",
       "  artifact: \"projects/560224572293/locations/us-central1/metadataStores/default/artifacts/4398058870343032914\"\n",
       "  execution: \"projects/560224572293/locations/us-central1/metadataStores/default/executions/11443974765889496760\"\n",
       "  event_time {\n",
       "  }\n",
       "  type_: OUTPUT\n",
       "  labels {\n",
       "    key: \"name\"\n",
       "    value: \"config_file\"\n",
       "  }\n",
       "}\n",
       "events {\n",
       "  artifact: \"projects/560224572293/locations/us-central1/metadataStores/default/artifacts/13099934123442219092\"\n",
       "  execution: \"projects/560224572293/locations/us-central1/metadataStores/default/executions/11443974765889496760\"\n",
       "  event_time {\n",
       "  }\n",
       "  type_: OUTPUT\n",
       "  labels {\n",
       "    key: \"name\"\n",
       "    value: \"test_data\"\n",
       "  }\n",
       "}\n",
       "events {\n",
       "  artifact: \"projects/560224572293/locations/us-central1/metadataStores/default/artifacts/18231325568793775957\"\n",
       "  execution: \"projects/560224572293/locations/us-central1/metadataStores/default/executions/17550896927101739842\"\n",
       "  event_time {\n",
       "  }\n",
       "  type_: INPUT\n",
       "  labels {\n",
       "    key: \"name\"\n",
       "    value: \"training_data\"\n",
       "  }\n",
       "}\n",
       "events {\n",
       "  artifact: \"projects/560224572293/locations/us-central1/metadataStores/default/artifacts/4398058870343032914\"\n",
       "  execution: \"projects/560224572293/locations/us-central1/metadataStores/default/executions/17550896927101739842\"\n",
       "  event_time {\n",
       "  }\n",
       "  type_: INPUT\n",
       "  labels {\n",
       "    key: \"name\"\n",
       "    value: \"config_file\"\n",
       "  }\n",
       "}\n",
       "events {\n",
       "  artifact: \"projects/560224572293/locations/us-central1/metadataStores/default/artifacts/13099934123442219092\"\n",
       "  execution: \"projects/560224572293/locations/us-central1/metadataStores/default/executions/17550896927101739842\"\n",
       "  event_time {\n",
       "  }\n",
       "  type_: INPUT\n",
       "  labels {\n",
       "    key: \"name\"\n",
       "    value: \"test_data\"\n",
       "  }\n",
       "}\n",
       "events {\n",
       "  artifact: \"projects/560224572293/locations/us-central1/metadataStores/default/artifacts/14398509906326841459\"\n",
       "  execution: \"projects/560224572293/locations/us-central1/metadataStores/default/executions/17550896927101739842\"\n",
       "  event_time {\n",
       "  }\n",
       "  type_: OUTPUT\n",
       "  labels {\n",
       "    key: \"name\"\n",
       "    value: \"metrics\"\n",
       "  }\n",
       "}\n",
       "events {\n",
       "  artifact: \"projects/560224572293/locations/us-central1/metadataStores/default/artifacts/4263002386027186685\"\n",
       "  execution: \"projects/560224572293/locations/us-central1/metadataStores/default/executions/17550896927101739842\"\n",
       "  event_time {\n",
       "  }\n",
       "  type_: OUTPUT\n",
       "  labels {\n",
       "    key: \"name\"\n",
       "    value: \"model\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_run_context_name(pipeline_run):\n",
    "  contexts = client.list_contexts(parent='projects/{}/locations/{}/metadataStores/default'.format(PROJECT_ID, REGION))\n",
    "  for context in contexts:\n",
    "    if context.display_name == pipeline_run:\n",
    "      return context.name\n",
    "  \n",
    "run_context_name = get_run_context_name('custom-container-pipeline-rthallam-1619799291-20210430161604')  # <- Name of the pipeline run\n",
    "\n",
    "client.query_context_lineage_subgraph(context=run_context_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Kgtx8-bW7cM"
   },
   "source": [
    "### Monitor the pipeline run in the Cloud Console\n",
    "\n",
    "Once you've deployed the pipeline run, you can monitor it in the [Cloud Console](https://console.cloud.google.com/ai/platform/pipelines) under **AI Platform (Unified)** > **Pipelines**. \n",
    "\n",
    "Click in to the pipeline run to see the run graph (for our pipeline, this consists of two steps), and click on a step to view the job detail and the logs for that step.\n",
    "\n",
    "As you look at the pipeline graph, you'll see that you can inspect the artifacts passed between the pipeline steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFwR7J6O3TCq"
   },
   "source": [
    "<a href=\"https://storage.googleapis.com/amy-jo/images/kf-pls/generate_train.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/kf-pls/generate_train.png\" width=\"70%\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feV62LXyW7cN"
   },
   "source": [
    "## What next?\n",
    "\n",
    "Next, try out some of the other notebooks.\n",
    "\n",
    "- a [KFP intro notebook](https://colab.research.google.com/drive/1mrud9HjsVp5fToHwwNL0RotFtJCKtfZ1#scrollTo=feV62LXyW7cN).\n",
    "- a simple KFP example that [shows how data can be passed between pipeline steps](https://colab.research.google.com/drive/1NztsGV-FAp71MU7zfMHU0SlfQ8dpw-9u).\n",
    "\n",
    "- A TFX notebook that [shows the canonical 'Chicago taxi' example](https://colab.research.google.com/drive/1dNLlm21F6f5_4aeIg-Zs_F1iGGRPEvhW), and how to use custom Python functions and custom containers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89fYarRLW7cN"
   },
   "source": [
    "-----------------------------\n",
    "Copyright 2020 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "mp_kfp_custom_containers_resources.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
