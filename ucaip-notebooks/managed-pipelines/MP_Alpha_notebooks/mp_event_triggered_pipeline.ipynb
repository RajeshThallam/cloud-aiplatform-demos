{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mp_event_triggered_pipeline.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-ZEcVTiB-RNl"},"source":["# Supporting event-triggered Managed Pipeline runs via Cloud Functions\n","\n","## Introduction\n","\n","This notebook shows an example of how to launch a Managed Pipelines job from a [**Cloud Functions**](https://cloud.google.com/functions/docs/) (GCF) function, where the Function run is triggered by addition of a file to a given [Cloud Storage](https://cloud.google.com/storage) (GCS) bucket.\n","\n","In many cases, you wouldn't want to launch a new pipeline run for every new file added to a GCS bucket (e.g., suppose you were uploading a set of new data files).  Rather, it is often preferable to trigger a pipeline run after the upload of a *batch* of new data has completed.\n","\n","So, this example uses an approach where the the 'trigger' bucket is different from the bucket used to store new files. A 'trigger file' contains the path to the new data (or other new info that needs processing); then the trigger file is uploaded once the new data upload has completed, and that upload triggers a run of the GCF function, which in turn launches the pipeline job.\n","\n","The example includes use of the [**Secret Manager**](https://cloud.google.com/secret-manager/docs#docs) to store the API key needed to deploy the pipeline from the GCF function (it is bad practice to hardwire the key in the GCF definition).  **The use of the API key will not be necessary once Managed Pipelines is in Preview**, thus in future this part of the process won't be required.\n","\n","The example uses a very simple (non-ML) pipeline in order to focus on the GCF setup; this pipeline does not actually do any data processing.\n","\n","(A follow-on notebook will show an ML-oriented scenario in which the addition of new data triggers re-validation of the dataset)."]},{"cell_type":"markdown","metadata":{"id":"RWACue6PW7bk"},"source":["## Setup\n","\n","Before you run this notebook, ensure that your Google Cloud user account and project are granted access to Managed Pipelines Experimental. To be granted access to Managed Pipelines Experimental, fill out this [form](http://go/cloud-mlpipelines-signup) and let your account representative know you have requested access. \n","\n","This notebook is intended to be run on either one of:\n","* [AI Platform Notebooks](https://cloud.google.com/ai-platform-notebooks). See the \"AI Platform Notebooks\" section in the Experimental [User Guide](https://docs.google.com/document/d/1JXtowHwppgyghnj1N1CT73hwD1caKtWkLcm2_0qGBoI/edit?usp=sharing) for more detail on creating a notebook server instance.\n","* [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb)\n","\n","\n","**To run this notebook on AI Platform Notebooks**, click on the **File** menu, then select \"Download .ipynb\".  Then, upload that notebook from your local machine to AI Platform Notebooks. (In the AI Platform Notebooks left panel, look for an icon of an arrow pointing up, to upload).\n","\n","We'll first install some libraries and set up some variables.\n"]},{"cell_type":"markdown","metadata":{"id":"GAaCPLjgiJrO"},"source":["Set `gcloud` to use your project.  **Edit the following cell before running it**."]},{"cell_type":"code","metadata":{"id":"0GlP_C9mY3Gq"},"source":["PROJECT_ID = 'your-project-id'  # <---CHANGE THIS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VkWdxe4TXRHk"},"source":["!gcloud config set project {PROJECT_ID}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gckGHdW9iPrq"},"source":["If you're running this notebook on colab, authenticate with your user account:"]},{"cell_type":"code","metadata":{"id":"kZQA0KrfXCvU"},"source":["import sys\n","if 'google.colab' in sys.modules:\n","  from google.colab import auth\n","  auth.authenticate_user()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aaqJjbmk6o0o"},"source":["-----------------\n","\n","**If you're on AI Platform Notebooks**, authenticate with Google Cloud before running the next section, by running\n","```sh\n","gcloud auth login\n","```\n","**in the Terminal window** (which you can open via **File** > **New** in the menu). You only need to do this once per notebook instance."]},{"cell_type":"markdown","metadata":{"id":"fOpZ41iBW7bl"},"source":["### Install the KFP SDK and AI Platform Pipelines client library\n","\n","For Managed Pipelines Experimental, you'll need to download a special version of the AI Platform client library."]},{"cell_type":"code","metadata":{"id":"lcZkq7U8QZab"},"source":["!mkdir -p gcf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2sFpGZHQZ5V"},"source":["%cd gcf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlPnul5UW7bl"},"source":["!gsutil cp gs://cloud-aiplatform-pipelines/releases/latest/kfp-1.5.0rc5.tar.gz .\n","!gsutil cp gs://cloud-aiplatform-pipelines/releases/latest/aiplatform_pipelines_client-0.1.0.caip20210415-py3-none-any.whl ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lpdfRA4vW7bq"},"source":["Then, install the libraries and restart the kernel."]},{"cell_type":"code","metadata":{"id":"TmUZzSv6YA9-"},"source":["if 'google.colab' in sys.modules:\n","  USER_FLAG = ''\n","else:\n","  USER_FLAG = '--user'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JGdU0lEfVwM-"},"source":["!python3 -m pip install {USER_FLAG} kfp-1.5.0rc5.tar.gz --upgrade\n","!python3 -m pip install {USER_FLAG} aiplatform_pipelines_client-0.1.0.caip20210415-py3-none-any.whl --upgrade\n","!python3 -m pip install {USER_FLAG} google-cloud-secret-manager==2.0.0\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UrQxxf97YLfg"},"source":["# Automatically restart kernel after installs \n","# (for this notebook, necessary for colab too. Ignore the pop-up warning that results.)\n","import IPython\n","app = IPython.Application.instance()\n","app.kernel.do_shutdown(True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0mqs-ZFuW7bx"},"source":["The KFP version should be >= 1.5.\n","\n"]},{"cell_type":"code","metadata":{"id":"a4uvTyimMYOr"},"source":["# Check the KFP version\n","!python3 -c \"import kfp; print('KFP version: {}'.format(kfp.__version__))\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t1GX5KDOUJuI"},"source":["If you're on colab, re-authorize after the kernel restart. **Edit the following cell for your project ID before running it.**"]},{"cell_type":"code","metadata":{"id":"PpkxFp93xBk5"},"source":["import sys\n","if 'google.colab' in sys.modules:\n","  PROJECT_ID = 'your-project-id'  # <---CHANGE THIS\n","  !gcloud config set project {PROJECT_ID}\n","  from google.colab import auth\n","  auth.authenticate_user()\n","  USER_FLAG = ''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_FZKbx-WBZT"},"source":["%cd gcf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tskC13YxW7b3"},"source":["### Set some variables\n","\n","**Before you run the next cell**, **edit it** to set variables for your project.  See the \"Before you begin\" section of the User Guide for information on creating your API key.  For `BUCKET_NAME`, enter the name of a Cloud Storage (GCS) bucket in your project.  Don't include the `gs://` prefix."]},{"cell_type":"code","metadata":{"id":"iXvbCu2CYvoS"},"source":["PATH=%env PATH\n","%env PATH={PATH}:/home/jupyter/.local/bin\n","\n","# Required Parameters\n","USER = 'your-user-name' # <---CHANGE THIS\n","BUCKET_NAME = 'your-bucket-name'  # <---CHANGE THIS\n","PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(BUCKET_NAME, USER)\n","\n","PROJECT_ID = 'your-project-id'  # <---CHANGE THIS\n","REGION = 'us-central1'\n","API_KEY = 'your-api-key'  # <---CHANGE THIS\n","\n","print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KV-ydP0DhxZ3"},"source":["### Give service account access to the Secret Manager\n","\n","As mentioned above, in this example we'll use the Secret Manager to store the API key, necessary to launch a pipeline run from the GCF function.   \n","Note: The use of the API key will not be necessary once Managed Pipelines is in Preview, thus in future this part of the process won't be required.\n","\n","#### GCF service account\n","\n","We'll need to give the service account used by the GCF runtime access to defined secrets.  \n","\n","In the Console's \"IAM & Admin\" panel, look for a service account named `<your-project-name>@appspot.gserviceaccount.com`.  Edit the permissions for that account to add the \"**Secret Manager Secret Accessor**\" role.\n","\n","<a href=\"https://storage.googleapis.com/amy-jo/images/kfp-deploy/secret_role.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/kfp-deploy/secret_role.png\" width=\"50%\"/></a>\n","\n","#### AI Notebook service account\n","\n","If you're running this example on an AI Notebook, you'll also need to give the notebook instance's service account the \"**Secret Manager Admin.**\" role (in order to create secrets from the notebook environment).  Visit the Notebooks dashboard in the Cloud Console and mouse over \"Service account\" for the notebook instance you're using to identify the service account you're using.\n","\n","<a href=\"https://storage.googleapis.com/amy-jo/images/kfp-deploy/secret_role2.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/kfp-deploy/secret_role2.png\" width=\"50%\"/></a>\n","\n","(If you are a project Owner, then Secrets admin should work in a Colab notebook out of the box. However, if you have a different project role, you may need to be given Secret Manager Admin access first).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IbN_49SUW7b7"},"source":["## Define a simple pipeline and create its job spec\n","\n","Now we'll define a very simple pipeline. It takes one input parameter and has one step.\n","\n","### Create a function-based pipeline component\n","\n","We'll first create a component based on a very simple python function. It takes a string input parameter and returns that value as output."]},{"cell_type":"code","metadata":{"id":"Ye7mr8WPW7b8"},"source":["from kfp import components\n","\n","def hello_world(text: str):\n","    print(text)\n","    return text\n","\n","components.func_to_container_op(hello_world,\n","      output_component_file='hw.yaml')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"84N6V7_jW7b_"},"source":["Next, we'll define a one-step pipeline that uses that component.\n","The pipeline takes an input parameter, and passes that parameter as an argument to the pipeline step."]},{"cell_type":"code","metadata":{"id":"QJkQwWzsW7cA"},"source":["from kfp.v2 import dsl\n","from kfp.v2 import compiler\n","from kfp import components\n","\n","\n","# Create a pipeline op from the component we defined above.\n","hw_op = components.load_component_from_file('./hw.yaml') # you can also use load_component_from_url\n","\n","@dsl.pipeline(\n","  name='remote-deploy-v2',\n","  description='A simple intro pipeline'\n",")\n","def pipeline_parameter_to_consumer(text: str='hi there'):\n","    '''Pipeline that passes small pipeline parameter string to consumer op'''\n","    consume_task = hw_op(text) # Passing pipeline parameter as argument to consumer op\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"foOQ6jYYR3Cp"},"source":["Compile the pipeline:"]},{"cell_type":"code","metadata":{"id":"EG9tXf4ZLgtM"},"source":["JOB_SPEC = 'pipeline_job.json'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6qf9KkkoA1y7"},"source":["compiler.Compiler().compile(pipeline_func=pipeline_parameter_to_consumer, \n","                            output_path=JOB_SPEC)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mH5QFfSuW7cJ"},"source":["## Do a test submission of the pipeline job\n","\n","Before defining a GCF function to deploying the pipeline job spec, let's first test it directly.\n","\n","Here, we'll create an API client using the API key you generated.    \n","Then, we'll submit the pipeline job by passing the compiled spec to the `create_run_from_job_spec()` method. Note that we're passing a `parameter_values` dict that specifies the pipeline input parameters we want to use."]},{"cell_type":"code","metadata":{"id":"NSnrYUDAW7cK"},"source":["from aiplatform.pipelines import client\n","\n","api_client = client.Client(project_id=PROJECT_ID, region=REGION, api_key=API_KEY)\n","\n","response = api_client.create_run_from_job_spec(\n","          job_spec_path=JOB_SPEC,\n","          pipeline_root=PIPELINE_ROOT, \n","          parameter_values={'text': 'Hello world!'})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wXtEH2OVF5_k"},"source":["View the running pipeline in the Console by clicking on the generated link above."]},{"cell_type":"markdown","metadata":{"id":"AOOrMur9HC4v"},"source":["## Create a Secret Manager key for your `API_KEY`\n","\n","Next, we'll use the Secret Manager to store your API KEY, so that we can access it in our GCF function.\n","\n","If you're using an AI Platform Notebook, then before you run this section, check that you've updated the notebook's service account roles to include \"Secret Manager Admin.\" as indicated in the \"Setup\" section."]},{"cell_type":"markdown","metadata":{"id":"BVECqQWNck_X"},"source":["Make sure that the `secretmanager` API has been enabled for your project:"]},{"cell_type":"code","metadata":{"id":"MI3-AlgRN8om"},"source":["!gcloud services enable secretmanager.googleapis.com"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kUWCJtm9c40U"},"source":["First, define a key name (you can change this string from `apk1` if you like)."]},{"cell_type":"code","metadata":{"id":"eQ3i0fCNKPVX"},"source":["SECRET_KEY = 'apk1'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-Qaxv3NGkfO"},"source":["Next, we'll define some utility functions to create and access a secret."]},{"cell_type":"code","metadata":{"id":"hZIOqt-8Jf8v"},"source":["from google.cloud import secretmanager\n","\n","def create_secret(client, secret_id):\n","    # Build the resource name of the parent project.\n","    parent = f\"projects/{PROJECT_ID}\"\n","    # Build a dict of settings for the secret\n","    secret = {'replication': {'automatic': {}}}\n","    # Create the secret\n","    response = client.create_secret(secret_id=secret_id, parent=parent, secret=secret)\n","    # Print the new secret name.\n","    print(f'Created secret: {response.name}') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jeDp-4SIJpg4"},"source":["def add_secret_version(client, secret_id, payload):\n","    # Build the resource name of the parent secret.\n","    parent = f\"projects/{PROJECT_ID}/secrets/{secret_id}\"\n","    # Convert the string payload into a bytes. This step can be omitted if you\n","    # pass in bytes instead of a str for the payload argument.\n","    payload = payload.encode('UTF-8')\n","    # Add the secret version.\n","    response = client.add_secret_version(parent=parent, payload={'data': payload})\n","    # Print the new secret version name.\n","    print(f'Added secret version: {response.name}')   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Me_zLX2lKDOW"},"source":["def access_secret_version(client, secret_id, version_id=\"latest\"):\n","    # Build the resource name of the secret version.\n","    name = f\"projects/{PROJECT_ID}/secrets/{secret_id}/versions/{version_id}\"\n","    # Access the secret version.\n","    response = client.access_secret_version(name=name)\n","    # Return the decoded payload.\n","    return response.payload.data.decode('UTF-8')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CV7dqBQmGtdl"},"source":["Create the secret manager client:"]},{"cell_type":"code","metadata":{"id":"jpYivuUZJ1VJ"},"source":["# create the Secret Manager client.\n","sm_client = secretmanager.SecretManagerServiceClient()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ygfBoEHHGxNn"},"source":["Create the key:"]},{"cell_type":"code","metadata":{"id":"pW8oSev6AB2l"},"source":["# if you've already created the key, you don't need to run this again.\n","# If you do, you can ignore the 'already exists' error.\n","create_secret(sm_client, SECRET_KEY)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wf7TAQsrG2Qx"},"source":["... and set the key to a value, in this case your `API_KEY` string:"]},{"cell_type":"code","metadata":{"id":"yYPjG7DlNqC8"},"source":["add_secret_version(sm_client, SECRET_KEY, API_KEY)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VQu7MdgbKk5s"},"source":["Test accessing the secret's value, to make sure it looks correct:"]},{"cell_type":"code","metadata":{"id":"PAx_IPVfKfM5"},"source":["access_secret_version(sm_client, SECRET_KEY)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iskhpQ-_-wwl"},"source":["## Define and deploy the Cloud Function\n","\n","Now we're ready to define and deploy the cloud function.  \n","First ensure that the Cloud Functions and Cloud Build APIs are enabled for your project:"]},{"cell_type":"code","metadata":{"id":"9QORteHbq-WE"},"source":["!gcloud services enable cloudfunctions.googleapis.com\n","!gcloud services enable cloudbuild.googleapis.com\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yd2lO7yCOET-"},"source":["Next, we'll write out a file that specifies the required installations.  Note that one of the lines installs a local file (the `aiplatform_pipelines_client` whl); that file will be uploaded as part of the GCF deployment."]},{"cell_type":"code","metadata":{"id":"88Dy1_-lOa-1"},"source":["%%writefile requirements.txt\n","kfp==1.4\n","kfp-pipeline-spec==0.1.3.1\n","google-cloud-secret-manager==2.0.0\n","aiplatform_pipelines_client-0.1.0.caip20201123-py3-none-any.whl\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TDnfMaESObeO"},"source":["Now we'll define the GCF function code in `main.py`.  \n","\n","Note that the function is grabbing some values from environment variables.  We'll show how those are set below.  One of the env vars holds the Secret key name (from which we will grab the API key).  Another holds the name of the pipeline job spec file (which we created above when we compiled the pipeline). This file will be uploaded to read-only storage as part of the GCF function deployment, and the code will be able to access it.\n","\n","The `gcs_update` function will be called on the addition (or modification) of a file in the specified trigger bucket."]},{"cell_type":"code","metadata":{"id":"5IVXmGcF-KW_"},"source":["%%writefile main.py\n","\n","import logging\n","import os\n","\n","from kfp.v2 import dsl\n","from kfp.v2 import compiler\n","from kfp import components\n","\n","from aiplatform.pipelines import client\n","\n","from google.cloud import secretmanager\n","from google.cloud import storage\n","\n","PIPELINE_PROJECT_ID = os.getenv('PIPELINE_PROJECT_ID')\n","REGION = 'us-central1'\n","SECRET_KEY = os.getenv('SECRET_KEY')\n","PIPELINE_ROOT = os.getenv('PIPELINE_ROOT') # bucket under PIPELINE_PROJECT_ID\n","PIPELINE_JOB_SPEC = os.getenv('JOB_SPEC')\n","\n","\n","def access_secret_version(secret_id, version_id=\"latest\"):\n","    \"\"\"Access a Secret's value, and return as string.\n","    \"\"\"\n","    # Create the Secret Manager client.\n","    client = secretmanager.SecretManagerServiceClient()\n","    # Build the resource name of the secret version.\n","    name = f\"projects/{PIPELINE_PROJECT_ID}/secrets/{secret_id}/versions/{version_id}\"\n","    # Access the secret version.\n","    response = client.access_secret_version(name=name)\n","    # Return the decoded payload.\n","    return response.payload.data.decode('UTF-8')\n","\n","\n","def read_trigger_file(data, context, storage_client):\n","    \"\"\"Read the contents of the trigger file and return as string.\n","    \"\"\"\n","    print('Event ID: {}'.format(context.event_id))\n","    print('Event type: {}'.format(context.event_type))\n","    print('Data: {}'.format(data))\n","    print('Bucket: {}'.format(data['bucket']))\n","    print('File: {}'.format(data['name']))\n","    print('Metageneration: {}'.format(data['metageneration']))\n","    print('Created: {}'.format(data['timeCreated']))\n","    print('Updated: {}'.format(data['updated']))\n","\n","    bucket = storage_client.get_bucket(data['bucket'])\n","    blob = bucket.get_blob(data['name'])\n","    trigger_file_string = blob.download_as_string().strip()\n","    logging.info('trigger file contents: {}'.format(trigger_file_string))\n","    return trigger_file_string.decode('UTF-8')\n","\n","\n","def gcs_update(data, context):\n","    \"\"\"Background Cloud Function to be triggered by Cloud Storage.\n","    \"\"\"\n","    logging.getLogger().setLevel(logging.INFO)\n","\n","    storage_client = storage.Client()\n","    # get the contents of the trigger file\n","    trigger_file_string = read_trigger_file(data, context, storage_client)\n","\n","    # then run the pipeline using the given job spec, passing the trigger file contents\n","    # as a parameter value.\n","    logging.info('running pipeline...')\n","    API_KEY = access_secret_version(SECRET_KEY)  # grab the API_KEY from the Secrets Manager\n","    # create the client object\n","    api_client = client.Client(project_id=PIPELINE_PROJECT_ID, region=REGION, api_key=API_KEY)\n","    # deploy the pipeline run\n","    response = api_client.create_run_from_job_spec(\n","              job_spec_path=PIPELINE_JOB_SPEC,\n","              # pipeline_root=PIPELINE_ROOT,  # optional- use if want to override compile-time value\n","              # example of passing info on the blob that triggered the GCF as a pipeline param...\n","              parameter_values={'text': trigger_file_string})\n","    logging.info('job response: %s', response)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3i0E3Q4TGi1o"},"source":["Next, identify a GCS bucket in your project to use for the 'trigger bucket'.  This bucket must already exist.  You can create a new bucket via the [Cloud Console](https://console.cloud.google.com/storage/browser) (or via the `gsutil mb` command-line utility).\n","\n","**Edit the following cell before running it.**"]},{"cell_type":"code","metadata":{"id":"IaR2RDpjgyVA"},"source":["# Change this to your bucket name.  Do not include the 'gs://' prefix.\n","TRIGGER_BUCKET = 'your-bucket-name'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OGE-xuF6Vvz0"},"source":["# check that you're in the gcf subdirectory\n","!pwd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ystw9qUIrmv"},"source":["Now we're ready to deploy the GCF function code.  We specify to use the `gcs_update` definition (in the file `main.py`, which is implicit), and to use the value of the `TRIGGER_BUCKET` var as the trigger bucket.  The trigger event is specified to be the addition or modification of a GCS file in that bucket.\n","\n","Note that we're setting several environment variables as part of the deployment, to which the function code will have access.  These include the name of the pipeline job spec file, as well as the name of the Secret key that holds the API key string. So, ensure that all these vars are set properly— which they should be if you've run all the notebook cells above.\n","\n","The deployment will include the files in the current directory (`gcf`), including the pipeline job spec file and the .whl file.  The GCF function will have read-only access to the directory contents.\n"]},{"cell_type":"code","metadata":{"id":"fltAbzRyGlU9"},"source":["!gcloud functions deploy gcs_update --set-env-vars \\\n","  SECRET_KEY={SECRET_KEY},PIPELINE_PROJECT_ID={PROJECT_ID},PIPELINE_ROOT={PIPELINE_ROOT},JOB_SPEC={JOB_SPEC} \\\n","  --runtime python37 --trigger-resource {TRIGGER_BUCKET} --trigger-event google.storage.object.finalize\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LMTgl1Ig_Zzb"},"source":["Once deployment has completed, we're ready to test triggering the GCF function.  To do this, we'll upload a new file to the `TRIGGER_BUCKET`.\n","\n","We'll create a file that contains a (dummy) path to 'new data' to be processed, and upload it.  The intent is that for a real pipeline, this file would contain information about the location of some new data to process.\n","\n","Uploading this file will trigger a run of the GCF function we defined.\n","\n","The GCF function will read the contents of the trigger file, and kick off a pipeline run, passing that string— the contents of the 'trigger file'— as a parameter to the pipeline. \n","(Our simple example pipeline won't do anything but print that information, of course).\n","\n"]},{"cell_type":"code","metadata":{"id":"dOYqN9bx_due"},"source":["!echo {PIPELINE_ROOT}/newdata > temp.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n3_YstAFBCBR"},"source":["Upload the newly created file to the `TRIGGER_BUCKET`:"]},{"cell_type":"code","metadata":{"id":"obl5WMy7_nlZ"},"source":["!gsutil cp temp.txt gs://{TRIGGER_BUCKET}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6hBC4nSX_zb6"},"source":["The upload will trigger a run of the GCF function, which in turn will trigger a pipeline run.  \n","\n","First, view the GCF logs via [Cloud Logging](https://console.cloud.google.com/logs/viewer?resource=cloud_function) in the Console.\n","You should see indication of the trigger file being read, and then the Managed Pipeline run initiated.\n","\n","Then, you can view the triggered Managed Pipeline run itself in the Console as well.\n"]},{"cell_type":"markdown","metadata":{"id":"89fYarRLW7cN"},"source":["-----------------------------\n","Copyright 2020 Google LLC\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","\n","     http://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License."]}]}