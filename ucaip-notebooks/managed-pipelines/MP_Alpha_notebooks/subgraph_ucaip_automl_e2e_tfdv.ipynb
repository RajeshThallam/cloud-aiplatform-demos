{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Tags","colab":{"name":"subgraph_ucaip_automl_e2e_tfdv.ipynb","provenance":[{"file_id":"1tN08xCy7TFRV-XyCa3jMlBOvzzmCbYcl","timestamp":1614122821388},{"file_id":"1JUVfMwL8hnONxVsy1NHc1ZZtQY4wJLE6","timestamp":1612817426981}],"collapsed_sections":[],"toc_visible":true},"environment":{"name":"tf2-2-3-gpu.2-3.m56","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}}},"cells":[{"cell_type":"markdown","metadata":{"id":"JwAi27xscRFB"},"source":["# Managed Pipelines & Unified AI Platform: AutoML Tabular +TFDV 'end-to-end' workflow + subgraph components\n"]},{"cell_type":"markdown","metadata":{"id":"am1vrCYu1dkq"},"source":["\n","## Introduction\n","\n","The first section of this example notebook shows how to build an [AutoML Tabular](https://cloud.google.com/ai-platform-unified/docs/training/training) 'end-to-end' workflow for **Managed Pipelines**, using the [Unified AI Platform](https://cloud.google.com/ai-platform-unified/docs)'s SDK along with the [KFP SDK](https://www.kubeflow.org/docs/pipelines/sdk/).\n","\n","The pipeline creates a [*Dataset*](https://cloud.google.com/ai-platform-unified/docs/datasets/datasets) from a [BigQuery](https://cloud.google.com/bigquery/) table, uses it to train an **AutoML** tabular regression model, gets evaluation information about the trained model, and if the model is sufficiently accurate, deploys the model for prediction. (It's also possible to export the model to run off-platform, though that's not included in this example).\n","\n","The pipeline is built using a KFP SDK feature for generating **subgraph components**: we'll build a subgraph component for the full AutoML e2e flow, and use that component to construct the pipeline. Then, in the next section of the notebook, we'll use this subgraph component to construct a larger pipeline.\n","\n","This section also shows how to use [**Cloud Build**](https://cloud.google.com/cloud-build) to create a base container image for pipeline steps; use of component I/O, to pass information between steps of a pipeline; how to set up **scheduled** (recurring) pipeline runs; and how to query the **Metadata Server** via the SDK to examine a pipeline's automatically tracked metadata.\n","\n","<a href=\"https://storage.googleapis.com/amy-jo/images/automl/ucaip_automl_tabular_dag.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/automl/ucaip_automl_tabular_dag.png\" width=\"90%\"/></a>\n","\n","Then, the next section of the example notebook shows how to use the [TFDV](https://www.tensorflow.org/tfx/guide/tfdv) (**TensorFlow Data Validation**) library to analyze dataset stats; then compare two sets of stats to determine whether an update to a dataset suggests enough 'data drift' for a model to be retrained, and initiate retraining if so. The stats generation step can be run on the [Dataflow](https://cloud.google.com/dataflow) service, or locally to the pipeline step using the [Beam](https://beam.apache.org/) DirectRunner.\n","\n","We'll build a pipeline that **uses these new TFDV components in conjunction with the AutoML subgraph component** built previously, to construct the full workflow.\n","\n","This section also shows how to configure resource constraints for pipeline steps.\n","\n","<a href=\"https://storage.googleapis.com/amy-jo/images/automl/ucaip-automl-tables-tfdv.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/automl/ucaip-automl-tables-tfdv.png\" width=\"90%\"/></a>\n","\n","The training can take a few hours, so this example notebook can take a while to run in full.\n","\n","### About the dataset and modeling task\n","\n","The  [Cloud Public Datasets Program](https://cloud.google.com/bigquery/public-data/)  makes available public datasets that are useful for experimenting with machine learning. Just as with this “[Explaining model predictions on structured data](https://cloud.google.com/blog/products/ai-machine-learning/explaining-model-predictions-structured-data)” post, we’ll use data that is essentially a join of two public datasets stored in  [BigQuery](https://cloud.google.com/bigquery/) :  [London Bike rentals](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=london_bicycles&page=dataset)  and  [NOAA weather data](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=noaa_gsod&page=dataset) , with some additional processing to clean up outliers and derive additional GIS and day-of-week fields.\n","\n","We’ll use this dataset to build a *regression* model to predict the *duration* of a bike rental based on information about the start and end stations, the day of the week, the weather on that day, and other data. If we were running a bike rental company, for example, these predictions— and their explanations— could help us anticipate demand and even plan how to stock each location."]},{"cell_type":"markdown","metadata":{"id":"RWACue6PW7bk"},"source":["## Setup\n","\n","Before you run this notebook, ensure that your Google Cloud user account and project are granted access to the Managed Pipelines Experimental. To be granted access to the Managed Pipelines Experimental, fill out this [form](http://go/cloud-mlpipelines-signup) and let your account representative know you have requested access. \n","\n","This notebook is intended to be run on either one of:\n","* [AI Platform Notebooks](https://cloud.google.com/ai-platform-notebooks). See the \"AI Platform Notebooks\" section in the Experimental [User Guide](https://docs.google.com/document/d/1JXtowHwppgyghnj1N1CT73hwD1caKtWkLcm2_0qGBoI/edit?usp=sharing) for more detail on creating a notebook server instance.\n","* [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb)\n","\n","If you haven't already enabled the AI Platform API, on the [AI Platform (Unified) Dashboard](https://console.cloud.google.com/ai/platform) page in the Google Cloud Console, click **Enable the AI Platform API**.\n","\n","\n","We'll first install some libraries and set up some variables.\n"]},{"cell_type":"markdown","metadata":{"id":"GAaCPLjgiJrO"},"source":["Set `gcloud` to use your project.  **Edit the following cell before running it**."]},{"cell_type":"code","metadata":{"id":"0GlP_C9mY3Gq"},"source":["PROJECT_ID = 'your-project-id'  # <---CHANGE THIS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VkWdxe4TXRHk"},"source":["!gcloud config set project {PROJECT_ID}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gckGHdW9iPrq"},"source":["If you're running this notebook on colab, authenticate with your user account:"]},{"cell_type":"code","metadata":{"id":"kZQA0KrfXCvU"},"source":["import sys\n","if 'google.colab' in sys.modules:\n","  from google.colab import auth\n","  auth.authenticate_user()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aaqJjbmk6o0o"},"source":["-----------------\n","\n","**If you're on AI Platform Notebooks**, authenticate with Google Cloud before running the next section, by running\n","```sh\n","gcloud auth login\n","```\n","**in the Terminal window** (which you can open via **File** > **New** in the menu). You only need to do this once per notebook instance."]},{"cell_type":"markdown","metadata":{"id":"NPw5TVfWYLfQ"},"source":["### Install the KFP SDK and AI Platform Pipelines client library\n","\n","For Managed Pipelines Experimental, you'll need to download a special version of the AI Platform client library."]},{"cell_type":"code","metadata":{"id":"QlPnul5UW7bl"},"source":["!gsutil cp gs://cloud-aiplatform-pipelines/releases/latest/kfp-1.5.0rc5.tar.gz .\n","!gsutil cp gs://cloud-aiplatform-pipelines/releases/latest/aiplatform_pipelines_client-0.1.0.caip20210415-py3-none-any.whl .\n","# Get the Metadata SDK to query the produced metadata.\n","!gsutil cp gs://cloud-aiplatform-metadata/sdk/google-cloud-aiplatform-metadata-0.0.1.tar.gz ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fX-JOHG_YLfY"},"source":["Then, install the libraries and restart the kernel."]},{"cell_type":"code","metadata":{"id":"4c4l9EWnYLfY"},"source":["if 'google.colab' in sys.modules:\n","  USER_FLAG = ''\n","else:\n","  USER_FLAG = '--user'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-r8250sUlTf8"},"source":["!python3 -m pip install {USER_FLAG} kfp-1.5.0rc5.tar.gz google-cloud-aiplatform-metadata-0.0.1.tar.gz aiplatform_pipelines_client-0.1.0.caip20210415-py3-none-any.whl google-cloud-aiplatform --upgrade"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UrQxxf97YLfg"},"source":["# Automatically restart kernel after installs \n","import IPython\n","app = IPython.Application.instance()\n","app.kernel.do_shutdown(True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N33S1ikHIOPS"},"source":["The KFP version should be >= 1.5.\n","\n"]},{"cell_type":"code","metadata":{"id":"a4uvTyimMYOr"},"source":["# Check the KFP version\n","!python3 -c \"import kfp; print('KFP version: {}'.format(kfp.__version__))\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t1GX5KDOUJuI"},"source":["If you're on colab, re-authorize after the kernel restart. **Edit the following cell for your project ID before running it.**"]},{"cell_type":"code","metadata":{"id":"PpkxFp93xBk5"},"source":["import sys\n","if 'google.colab' in sys.modules:\n","  PROJECT_ID = 'your-project-id'  # <---CHANGE THIS\n","  !gcloud config set project {PROJECT_ID}\n","  from google.colab import auth\n","  auth.authenticate_user()\n","  USER_FLAG = ''\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tskC13YxW7b3"},"source":["### Set some variables\n","\n","**Before you run the next cell**, **edit it** to set variables for your project.  See the \"Before you begin\" section of the User Guide for information on creating your API key.  For `BUCKET_NAME`, enter the name of a Cloud Storage (GCS) bucket in your project.  Don't include the `gs://` prefix."]},{"cell_type":"code","metadata":{"id":"iXvbCu2CYvoS"},"source":["PATH=%env PATH\n","%env PATH={PATH}:/home/jupyter/.local/bin\n","\n","# Required Parameters\n","USER = 'your-user-name' # <---CHANGE THIS\n","BUCKET_NAME = 'your-bucket-name'  # <---CHANGE THIS\n","PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(BUCKET_NAME, USER)\n","\n","PROJECT_ID = 'your-project-id'  # <---CHANGE THIS\n","REGION = 'us-central1'\n","API_KEY = 'your-api-key'  # <---CHANGE THIS\n","\n","print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hsBFsM5gWlY8"},"source":["## (Optional) Create the underlying container image used for the pipeline steps\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0s0kBIPtR-_d"},"source":["We'll use Cloud Build to generate the container image used by the AutoML pipeline components in this example, so that we don't need to load these libraries at runtime.\n","\n","If you'd like to skip this section, you can instead use a prebuilt version of this same container image: `gcr.io/google-samples/automl-ucaip:v1`."]},{"cell_type":"code","metadata":{"id":"gy5J9juWW1Vd"},"source":["%%writefile Dockerfile\n","\n","FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\n","\n","RUN pip install -U google-cloud-aiplatform\n","RUN pip install -U google-cloud-storage"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQ3Y9CErXs9M"},"source":["!gcloud builds submit --tag gcr.io/{PROJECT_ID}/custom-container-ucaip:{USER} ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rmsFSuX0VlEw"},"source":["**If you built your own container image and want to use that instead of the prebuilt one, uncomment that line from the cell below** before running it."]},{"cell_type":"code","metadata":{"id":"RTyPqQaRWGs8"},"source":["# Uncomment the following to use your own container image\n","# CONTAINER_IMAGE = 'gcr.io/{}/custom-container-ucaip:{}'.format(PROJECT_ID, USER)\n","CONTAINER_IMAGE = 'gcr.io/google-samples/automl-ucaip:v1'\n","print(CONTAINER_IMAGE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sszx28cdfMHr"},"source":["## Create the AutoML pipeline components\n","\n","In this section we'll define the pipeline components for the AutoML workflow. \n","We'll build components for creating a Dataset, training a model, obtaining evaluation information for a given model, creating a model *endpoint*, and deploying the trained model to the endpoint for serving.\n","\n","We'll build ['lightweight' Python-function-based components](https://www.kubeflow.org/docs/pipelines/sdk/python-function-components/) using the KFP SDK. \n","First we'll define the Python functions for each step, then generate component `.yaml` files based on those function definitions."]},{"cell_type":"markdown","metadata":{"id":"owsU_9R3ms7w"},"source":["### Create the dataset component function\n","\n","Create a *Dataset* and ingest data into it from a BigQuery table. This function assumes a BQ table as the source, and would be a bit different if the source was a set of GCS files."]},{"cell_type":"code","metadata":{"id":"b1-GXmdx06Ex"},"source":["from kfp.components import OutputPath\n","\n","def create_dataset_tabular_bigquery_sample(\n","    project: str,\n","    display_name: str,\n","    bigquery_uri: str, # eg 'bq://aju-dev-demos.london_bikes_weather.bikes_weather',\n","    location: str, # \"us-central1\",\n","    api_endpoint: str, # \"us-central1-aiplatform.googleapis.com\",\n","    timeout: int, # 500,\n","    drift_res: str,\n","    dataset_id: OutputPath('String'),\n","):\n","\n","  import logging\n","  import subprocess\n","  import time\n","\n","  logging.getLogger().setLevel(logging.INFO)\n","  if drift_res == 'false':\n","    logging.warning('dataset drift detected; not proceeding')\n","    exit(1)\n","\n","  from google.cloud import aiplatform\n","  from google.protobuf import json_format\n","  from google.protobuf.struct_pb2 import Value\n","\n","\n","  client_options = {\"api_endpoint\": api_endpoint}\n","  # Initialize client that will be used to create and send requests.\n","  client = aiplatform.gapic.DatasetServiceClient(client_options=client_options)\n","  metadata_dict = {\"input_config\": {\"bigquery_source\": {\"uri\": bigquery_uri}}}\n","  metadata = json_format.ParseDict(metadata_dict, Value())\n","\n","  dataset = {\n","      \"display_name\": display_name,\n","      \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/tabular_1.0.0.yaml\",\n","      \"metadata\": metadata,\n","  }\n","  parent = f\"projects/{project}/locations/{location}\"\n","  response = client.create_dataset(parent=parent, dataset=dataset)\n","  print(\"Long running operation:\", response.operation.name)\n","  create_dataset_response = response.result(timeout=timeout)\n","  logging.info(\"create_dataset_response: %s\", create_dataset_response)\n","  path_components = create_dataset_response.name.split('/')\n","  logging.info('got dataset id: %s', path_components[-1])\n","  # write the dataset id as output\n","  with open('temp.txt', \"w\") as outfile:\n","    outfile.write(path_components[-1])\n","  subprocess.run(['gsutil', 'cp', 'temp.txt', dataset_id])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pWWpp0J4vnDO"},"source":["### Create a component function to train an AutoML tabular regression model\n","\n","We'll train an AutoML tabular model on the dataset.  To set up the training job, we need to provide the target (label) column and a dict of the input transformations: which fields to use as model inputs, and the input types.  These will be set later on in the notebook, when we run the pipeline.\n","\n","For the model we're training, the target column will be set to `duration`, a numeric field. Thus, this function definition assumes a *regression* model, and would be a bit different if we were training a classification model instead."]},{"cell_type":"code","metadata":{"id":"A0XvsYNM-Dey"},"source":["from kfp.components import OutputPath\n","\n","def training_tabular_regression(\n","    project: str,\n","    display_name: str,\n","    dataset_id: str,\n","    model_prefix: str,\n","    target_column: str,\n","    transformations_str: str,\n","    location: str, # \"us-central1\",\n","    api_endpoint: str, # \"us-central1-aiplatform.googleapis.com\"\n","    model_id: OutputPath('String'),\n","    model_dispname: OutputPath('String')\n","):\n","  import json\n","  import logging\n","  import subprocess\n","  import time\n","  from google.cloud import aiplatform\n","  from google.protobuf import json_format\n","  from google.protobuf.struct_pb2 import Value\n","  from google.cloud.aiplatform_v1beta1.types import pipeline_state\n","\n","  SLEEP_INTERVAL = 100\n","\n","  logging.getLogger().setLevel(logging.INFO)\n","  logging.info('using dataset id: %s', dataset_id)\n","  client_options = {\"api_endpoint\": api_endpoint}\n","  # Initialize client that will be used to create and send requests.\n","  client = aiplatform.gapic.PipelineServiceClient(client_options=client_options)\n","  # set the columns used for training and their data types\n","  transformations = json.loads(transformations_str)\n","  logging.info('using transformations: %s', transformations)\n","\n","  training_task_inputs_dict = {\n","        # required inputs\n","        \"targetColumn\": target_column,\n","        \"predictionType\": \"regression\",\n","        \"transformations\": transformations,\n","        \"trainBudgetMilliNodeHours\": 2000,\n","        \"disableEarlyStopping\": False,\n","        \"optimizationObjective\": \"minimize-rmse\",\n","  }\n","  training_task_inputs = json_format.ParseDict(training_task_inputs_dict, Value())\n","  model_display_name = '{}_{}'.format(model_prefix, str(int(time.time())))\n","\n","  training_pipeline = {\n","        \"display_name\": display_name,\n","        \"training_task_definition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_tabular_1.0.0.yaml\",\n","        \"training_task_inputs\": training_task_inputs,\n","        \"input_data_config\": {\n","            \"dataset_id\": dataset_id,\n","            \"fraction_split\": {\n","                \"training_fraction\": 0.8,\n","                \"validation_fraction\": 0.1,\n","                \"test_fraction\": 0.1,\n","            },\n","        },\n","        \"model_to_upload\": {\"display_name\": model_display_name},\n","  }\n","  parent = f\"projects/{project}/locations/{location}\"\n","  response = client.create_training_pipeline(\n","        parent=parent, training_pipeline=training_pipeline\n","  )\n","  training_pipeline_name = response.name\n","  logging.info(\"pipeline name: %s\", training_pipeline_name)\n","  # Poll periodically until training completes\n","  while True:\n","    mresponse = client.get_training_pipeline(name=training_pipeline_name)\n","    logging.info('mresponse: %s', mresponse)\n","    logging.info('job state: %s', mresponse.state)\n","    if mresponse.state == pipeline_state.PipelineState.PIPELINE_STATE_SUCCEEDED:\n","      logging.info('training finished')\n","      # write some outputs once finished\n","      model_name = mresponse.model_to_upload.name \n","      logging.info('got model name: %s', model_name)\n","      with open('temp.txt', \"w\") as outfile:\n","        outfile.write(model_name)\n","      subprocess.run(['gsutil', 'cp', 'temp.txt', model_id])\n","      with open('temp2.txt', \"w\") as outfile:\n","        outfile.write(model_display_name)\n","      subprocess.run(['gsutil', 'cp', 'temp2.txt', model_dispname])      \n","      break\n","    else:\n","      time.sleep(SLEEP_INTERVAL)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"84bKcN7Z-En-"},"source":["### Create a component function to get evaluation information for the trained model\n","\n","Once the model training has finished, we can get model evaluation metrics.\n"]},{"cell_type":"code","metadata":{"id":"0NQrhIsrv3le"},"source":["\n","def get_model_evaluation_tabular(\n","    project: str,\n","    model_id: str,\n","    location: str, #\"us-central1\",\n","    api_endpoint: str, # \"us-central1-aiplatform.googleapis.com\",\n","    eval_info: OutputPath('String'),\n","):\n","  import json\n","  import logging\n","  import subprocess  \n","  from google.cloud import aiplatform\n","\n","  def get_eval_id(client, model_name):\n","    from google.protobuf.json_format import MessageToDict\n","    response = client.list_model_evaluations(parent=model_name)\n","    for evaluation in response:\n","        print(\"model_evaluation\")\n","        print(\" name:\", evaluation.name)\n","        print(\" metrics_schema_uri:\", evaluation.metrics_schema_uri)\n","        metrics = MessageToDict(evaluation._pb.metrics)\n","        for metric in metrics.keys():\n","            logging.info('metric: %s, value: %s', metric, metrics[metric])\n","        metrics_str = json.dumps(metrics)\n","\n","    return (evaluation.name, metrics_str)  # for regression, only one slice\n","\n","  logging.getLogger().setLevel(logging.INFO)\n","\n","  client_options = {\"api_endpoint\": api_endpoint}\n","  # Initialize client that will be used to create and send requests.\n","  client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n","  eval_name, metrics_str = get_eval_id(client, model_id)\n","  logging.info('got evaluation name: %s', eval_name)\n","  logging.info('got metrics dict string: %s', metrics_str)\n","  with open('temp.txt', \"w\") as outfile:\n","    outfile.write(metrics_str)\n","  subprocess.run(['gsutil', 'cp', 'temp.txt', eval_info])  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T6BXo0Uj9uYX"},"source":["### Create a component function to create a serving endpoint\n","\n","Here we're creating a serving endpoint to which we'll deploy the trained model.  If an existing endpoint path is given as an arg, we'll use that instead of creating a new endpoint. "]},{"cell_type":"code","metadata":{"id":"33c4zW-v9wi8"},"source":["from kfp.components import OutputPath\n","\n","def create_endpoint(\n","    project: str,\n","    display_name: str,\n","    endpoint_path: str,    \n","    location: str, # \"us-central1\",\n","    api_endpoint: str, # \"us-central1-aiplatform.googleapis.com\",\n","    timeout: int,\n","    endpoint_id: OutputPath('String'),\n","\n","):\n","  import logging\n","  import subprocess  \n","  from google.cloud import aiplatform\n","\n","  logging.getLogger().setLevel(logging.INFO)\n","  if endpoint_path == 'new':  # then create new endpoint, using given display name\n","    logging.info('creating new endpoint with display name: %s', display_name)\n","    client_options = {\"api_endpoint\": api_endpoint}\n","    # Initialize client that will be used to create and send requests.\n","    client = aiplatform.gapic.EndpointServiceClient(client_options=client_options)\n","    endpoint = {\"display_name\": display_name}\n","    parent = f\"projects/{project}/locations/{location}\"\n","    response = client.create_endpoint(parent=parent, endpoint=endpoint)\n","    logging.info(\"Long running operation: %s\", response.operation.name)\n","    create_endpoint_response = response.result(timeout=timeout)\n","    logging.info(\"create_endpoint_response: %s\", create_endpoint_response)\n","    endpoint_name = create_endpoint_response.name \n","    logging.info('endpoint name: %s', endpoint_name)\n","  else:  # otherwise, use given endpoint path expression (TODO: add error checking)\n","    logging.info('using existing endpoint: %s', endpoint_path)\n","    endpoint_name = endpoint_path\n","  # write the endpoint name (path expression) as output\n","  with open('temp.txt', \"w\") as outfile:\n","    outfile.write(endpoint_name)\n","  subprocess.run(['gsutil', 'cp', 'temp.txt', endpoint_id])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kDgGLoLQv_s3"},"source":["### Create a component function to deploy the trained model for serving\n","\n","This function deploys the trained model for serving.  It supports simple 'gating' logic on model quality.\n","\n","The model eval metrics dict (extracted in a previous step of the pipeline) is passed as a component input, and is compared  with a dict of metrics threshold values, passed as a pipeline arg.  If the model doesn't meet the given threshold value(s), it won't be deployed. "]},{"cell_type":"code","metadata":{"id":"loeYWroDwC3B"},"source":["def deploy_automl_tabular_model(\n","    project: str,\n","    endpoint_name: str,\n","    model_name: str,\n","    deployed_model_display_name: str,\n","    eval_info: str,\n","    location: str = \"us-central1\",\n","    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n","    timeout: int = 7200,\n","    thresholds_dict_str: str = '{\"meanAbsoluteError\": 470}'\n","):\n","  import json\n","  import logging\n","  from google.cloud import aiplatform\n","\n","  # check the model metrics against the given thresholds dict\n","  def regression_thresholds_check(metrics_dict, thresholds_dict):\n","    for k, v in thresholds_dict.items():\n","      logging.info('k {}, v {}'.format(k, v))\n","      if k in ['rootMeanSquaredError', 'meanAbsoluteError']:  # lower is better\n","        if metrics_dict[k] > v:  # if over threshold\n","          logging.info('{} > {}; returning False'.format(\n","              metrics_dict[k], v))\n","          return False\n","      elif k in ['rSquared']:  # higher is better\n","        if metrics_dict[k] < v:  # if under threshold\n","          logging.info('{} < {}; returning False'.format(\n","              metrics_dict[k], v))\n","          return False\n","      else:  # unhandled key in thresholds dict\n","        # TODO: should the default instead be to deploy?\n","        logging.info('unhandled threshold key %s; not deploying', k)\n","        return False\n","    logging.info('threshold checks passed.')\n","    return True  \n","\n","  logging.getLogger().setLevel(logging.INFO)\n","  metrics_dict = json.loads(eval_info)\n","  thresholds_dict = json.loads(thresholds_dict_str)\n","  logging.info('got metrics dict: %s', metrics_dict)\n","  logging.info('got thresholds dict: %s', thresholds_dict)\n","  deploy = regression_thresholds_check(metrics_dict, thresholds_dict)\n","  if not deploy:\n","    # then don't deploy the model\n","    logging.warning('model is not accurate enough to deploy')\n","    return \n","\n","  client_options = {\"api_endpoint\": api_endpoint}\n","  # Initialize client that will be used to create and send requests.\n","  client = aiplatform.gapic.EndpointServiceClient(client_options=client_options)\n","  deployed_model = {\n","      # format: 'projects/{project}/locations/{location}/models/{model}'\n","      \"model\": model_name,\n","      \"display_name\": deployed_model_display_name,\n","      \"dedicated_resources\": {\n","          \"min_replica_count\": 1,\n","          \"machine_spec\": {\n","              \"machine_type\": \"n1-standard-8\",\n","              # Accelerators can be used only if the model specifies a GPU image.\n","              # 'accelerator_type': aiplatform.AcceleratorType.NVIDIA_TESLA_K80,\n","              # 'accelerator_count': 1,\n","          },\n","      }        \n","  }\n","  # key '0' assigns traffic for the newly deployed model\n","  # Traffic percentage values must add up to 100\n","  # Leave dictionary empty if endpoint should not accept any traffic\n","  traffic_split = {\"0\": 100}\n","  response = client.deploy_model(\n","      endpoint=endpoint_name, deployed_model=deployed_model, traffic_split=traffic_split\n","  )\n","  print(\"Long running operation:\", response.operation.name)\n","  deploy_model_response = response.result(timeout=timeout)\n","  print(\"deploy_model_response:\", deploy_model_response)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"naBzue0Tv5Sr"},"source":["### Create components from the python functions\n","\n","Now we'll use the `kfp.components.func_to_container_op` method to create `yaml` component files for the functions above.  In all cases we'll use as the component `base_image` the container we built above.  \n","\n","The `yaml` component files make it easy to version-track and share component definitions. "]},{"cell_type":"code","metadata":{"id":"j31DEZ1q4sXS"},"source":["from kfp import components\n","from kfp.v2 import dsl\n","from kfp.v2 import compiler\n","\n","components.func_to_container_op(create_dataset_tabular_bigquery_sample,\n","      output_component_file='tables_create_dataset_component.yaml', \n","      base_image=CONTAINER_IMAGE)\n","\n","components.func_to_container_op(training_tabular_regression,\n","      output_component_file='tables_train_component.yaml', \n","      base_image=CONTAINER_IMAGE)\n","\n","components.func_to_container_op(get_model_evaluation_tabular,\n","      output_component_file='tables_eval_component.yaml', \n","      base_image=CONTAINER_IMAGE)\n","\n","components.func_to_container_op(create_endpoint,\n","      output_component_file='tables_endpoint_component.yaml', \n","      base_image=CONTAINER_IMAGE)\n","\n","components.func_to_container_op(deploy_automl_tabular_model,\n","      output_component_file='tables_deploy_component.yaml', \n","      base_image=CONTAINER_IMAGE)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"thmD2xhlfjZu"},"source":["## Define and run an e2e AutoML pipeline using the components you defined\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wHT3ZW1ZfjZy"},"source":["Now we're ready to define and run a pipeline, using the components defined above.\n","\n","First, we'll create pipeline 'ops' from the components, using the `load_component_from_file` method. While we're not using it here, there is also a `load_component_from_url` method, which is handy if your component files are checked into a repo or otherwise stored online. (For GitHub files, use the 'raw' URL).\n","\n","Next, we'll build a **subgraph component** from the ops.  The subgraph component lets us package a (sub-)workflow in a reusable manner.\n","\n","Then, we'll define a pipeline using that subgraph component.  \n","(In the next section, we'll use that subgraph component in a larger pipeline).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"81P-E8Yw7Xyq"},"source":["We'll first do some imports.  Note that currently, for Managed Pipelines, we're using the `kfp.v2` namespace for the `dsl` and `compiler` packages."]},{"cell_type":"code","metadata":{"id":"jQ0FKbsP7RkT"},"source":["import json\n","from kfp.v2 import dsl\n","from kfp.v2 import compiler\n","from kfp import components"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s2pliDXP7n6_"},"source":["Now we'll instantiate the pipeline op constructors from the component files we generated above."]},{"cell_type":"code","metadata":{"id":"CqEzQ2qKfjZz"},"source":["create_dataset_op = components.load_component_from_file(\n","  './tables_create_dataset_component.yaml'\n","  )\n","\n","train_op = components.load_component_from_file(\n","  './tables_train_component.yaml'\n","  )\n","\n","eval_op = components.load_component_from_file(\n","  './tables_eval_component.yaml'\n","  )\n","\n","create_endpoint_op = components.load_component_from_file(\n","  './tables_endpoint_component.yaml'\n","  )\n","\n","deploy_op = components.load_component_from_file(\n","  './tables_deploy_component.yaml'\n","  )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UNUKVgMNdAoO"},"source":["Next, we'll define a _subgraph component_ using those ops.\n","\n","> Because of [this bug](https://github.com/kubeflow/pipelines/issues/5173), we're temporarily re-instantiating the ops from URLs rather than using the op definitions above.\n","\n","In the following, **note that some of the ops take as inputs the outputs from other ops.**\n","\n","Note also that the `create_endpoint` step has no input dependencies, and thus no implicit ordering constraints. Thus, it can run right away, so it can run concurrently with the steps that create a dataset and train a model.  "]},{"cell_type":"code","metadata":{"id":"rBgQaqrACxiU"},"source":["from kfp import components\n","\n","create_dataset_op = components.load_component_from_url(\n","  'https://gist.githubusercontent.com/amygdala/33b424c8cb2a287728fe0f07a81b19e0/raw/c79fc70908f7f36267f37edd923088e4c28da725/tables_create_dataset_component.yaml'\n","  )\n","\n","train_op = components.load_component_from_url(\n","  'https://gist.githubusercontent.com/amygdala/7892b872b3b6ccfa518ac7f447010f22/raw/e3424e8a5ed9010668478cd5a8e3b7396fe738d2/tables_train_component.yaml'\n","  )\n","\n","eval_op = components.load_component_from_url(\n","  'https://gist.githubusercontent.com/amygdala/7c3a74dcabb4a42bcf46470d68715c09/raw/a5f140acf5e0a952f379b07f1d77251fdebe1998/tables_eval_component.yaml'\n","  )\n","\n","create_endpoint_op = components.load_component_from_url(\n","  'https://gist.githubusercontent.com/amygdala/8331619bd400e695f4f41c62747d9bd1/raw/63b0d69857ad6dc82cc9fe9e6f783a55fa0cc465/tables_endpoint_component.yaml'\n","  )\n","\n","deploy_op = components.load_component_from_url(\n","  'https://gist.githubusercontent.com/amygdala/3a0db8c44cbeb573f6437978b65ff3ec/raw/6923d101d0c0319088dfa19a2b0fa6919b484c55/tables_deploy_component.yaml'\n","  )\n","\n","\n","def automl_tables( \n","  gcp_project_id: str = 'YOUR_PROJECT_HERE',\n","  gcp_region: str = 'us-central1',\n","  dataset_display_name: str = 'YOUR_DATASET_NAME',\n","  api_endpoint: str = 'us-central1-aiplatform.googleapis.com',\n","  timeout: int = 2000,\n","  bigquery_uri: str = 'bq://aju-dev-demos.london_bikes_weather.bikes_weather',\n","  # bigquery_uri: str = 'bq://aju-vtests2.demos.bw_ts_sorted2',\n","  target_col_name: str = 'duration',\n","  time_col_name: str = 'none',    \n","  transformations: str = '{}',\n","  train_budget_milli_node_hours: int = 1000,\n","  model_prefix: str = 'bw',    \n","  # optimization_objective: str = 'minimize-rmse', \n","  training_display_name: str = 'CHANGE THIS',\n","  endpoint_display_name: str = 'CHANGE THIS',\n","  # if set to other than 'new', use the given endpoint path rather than create new endpoint.  \n","  endpoint_path: str = 'new',\n","  thresholds_dict_str: str = '{\"meanAbsoluteError\": 470}',\n","  drift_res: str = 'true'\n","  ):  \n","\n","  create_dataset = create_dataset_op(\n","    gcp_project_id,\n","    dataset_display_name,\n","    bigquery_uri,\n","    gcp_region,\n","    api_endpoint,\n","    timeout,\n","    drift_res\n","    )\n","  \n","  train = train_op(\n","    gcp_project_id,\n","    training_display_name,\n","    create_dataset.outputs['dataset_id'],\n","    model_prefix,\n","    target_col_name,\n","    transformations,\n","    gcp_region,\n","    api_endpoint\n","    )\n","  \n","  eval = eval_op(\n","    gcp_project_id,\n","    train.outputs['model_id'],\n","    gcp_region,\n","    api_endpoint\n","    )\n","  \n","  create_endpoint = create_endpoint_op(\n","    gcp_project_id,\n","    dataset_display_name,\n","    endpoint_path,\n","    gcp_region,\n","    api_endpoint,\n","    timeout\n","  )\n","\n","  deploy = deploy_op(\n","    gcp_project_id,\n","    create_endpoint.outputs['endpoint_id'],\n","    train.outputs['model_id'],\n","    train.outputs['model_dispname'],\n","    eval.outputs['eval_info'],\n","    gcp_region,\n","    api_endpoint,\n","    timeout,\n","    thresholds_dict_str\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IyOehoKBdxZi"},"source":["Now, we'll call `components.create_graph_component_from_pipeline_func()` to generate a subgraph component `.yaml` file from the definition above."]},{"cell_type":"code","metadata":{"id":"rLDb_S6JdFpJ"},"source":["automl_e2e_op = components.create_graph_component_from_pipeline_func(\n","    automl_tables,\n","    output_component_file='automl_tables_component.yaml',\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WqJ6C3l1KRyV"},"source":["Instantiate an op from the new component:"]},{"cell_type":"code","metadata":{"id":"dg4ESM34KVQB"},"source":["automl_e2e_op = components.load_component_from_file(\n","  './automl_tables_component.yaml'\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FKZioDFGfjZz"},"source":["Next, we'll define the pipeline, using the subgraph component op defined above.\n","\n","We'll first define some dataset type information to be used by AutoML tabular during training."]},{"cell_type":"code","metadata":{"id":"55M7p9yofjZz"},"source":["import json\n","# We'll use this transformation specification as an arg for the training step.\n","TRANSFORMATIONS = [\n","    {\"auto\": {\"column_name\": \"bike_id\"}},\n","    {\"auto\": {\"column_name\": \"day_of_week\"}},\n","    {\"auto\": {\"column_name\": \"dewp\"}},\n","    {\"auto\": {\"column_name\": \"duration\"}},\n","    {\"auto\": {\"column_name\": \"end_latitude\"}},\n","    {\"auto\": {\"column_name\": \"end_longitude\"}},\n","    {\"categorical\": {\"column_name\": \"end_station_id\"}},\n","    {\"auto\": {\"column_name\": \"euclidean\"}},\n","    {\"categorical\": {\"column_name\": \"loc_cross\"}},\n","    {\"auto\": {\"column_name\": \"max\"}},\n","    {\"auto\": {\"column_name\": \"min\"}},\n","    {\"auto\": {\"column_name\": \"prcp\"}},\n","    {\"auto\": {\"column_name\": \"start_latitude\"}},\n","    {\"auto\": {\"column_name\": \"start_longitude\"}},\n","    {\"categorical\": {\"column_name\": \"start_station_id\"}},\n","    {\"auto\": {\"column_name\": \"temp\"}},\n","    {\"timestamp\": {\"column_name\": \"ts\"}}\n","]\n","TRANSFORMATIONS_STR = json.dumps(TRANSFORMATIONS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gszEyrBDfoRQ"},"source":["Now the pipeline definition itself.  This pipeline uses only the subgraph component, which defines the AutoML 'e2e' workflow.  \n","(In the next section, we'll build a larger pipeline that adds some additional components)."]},{"cell_type":"code","metadata":{"id":"NPScir0vKOos"},"source":["@dsl.pipeline(\n","  name='ucaip-automl-tables-subgraph',\n","  description='Demonstrate a uCAIP AutoML Tables workflow'\n",")\n","def automl_tables_subgraph( \n","  gcp_project_id: str = 'YOUR_PROJECT_HERE',\n","  gcp_region: str = 'us-central1',\n","  dataset_display_name: str = 'YOUR_DATASET_NAME',\n","  api_endpoint: str = 'us-central1-aiplatform.googleapis.com',\n","  timeout: int = 2000,\n","  bigquery_uri: str = 'bq://aju-dev-demos.london_bikes_weather.bikes_weather',\n","  # bigquery_uri: str = 'bq://aju-vtests2.demos.bw_ts_sorted2',\n","  target_col_name: str = 'duration',\n","  time_col_name: str = 'none',    \n","  transformations: str = TRANSFORMATIONS_STR,\n","  train_budget_milli_node_hours: int = 1000,\n","  model_prefix: str = 'bw',    \n","  # optimization_objective: str = 'minimize-rmse', \n","  training_display_name: str = 'CHANGE THIS',\n","  endpoint_display_name: str = 'CHANGE THIS',\n","  # if set to other than 'new', use the given endpoint path rather than create new endpoint.  \n","  endpoint_path: str = 'new',\n","  thresholds_dict_str: str = '{\"meanAbsoluteError\": 470}',\n","  drift_res: str = 'true'\n","  ):\n","\n","  automl_e2e = automl_e2e_op(\n","    gcp_project_id,\n","    gcp_region,\n","    dataset_display_name,\n","    api_endpoint, timeout, bigquery_uri,\n","    target_col_name, time_col_name, transformations, \n","    train_budget_milli_node_hours, model_prefix,\n","    training_display_name, endpoint_display_name,\n","    endpoint_path, thresholds_dict_str,\n","    drift_res\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vtrw4SXzfjZ1"},"source":["Compile the pipeline..."]},{"cell_type":"code","metadata":{"id":"UZNQ8GyffjZ3"},"source":["compiler.Compiler().compile(pipeline_func=automl_tables_subgraph,\n","                            package_path='automl_pipeline_spec.json')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gd0LXU9FfjZ3"},"source":["... then create an API client object and run it."]},{"cell_type":"code","metadata":{"id":"rE259yLafjZ3"},"source":["import time\n","from aiplatform.pipelines import client\n","\n","api_client = client.Client(project_id=PROJECT_ID, region=REGION, api_key=API_KEY)\n","DISPLAY_NAME = 'automl{}'.format(str(int(time.time())))\n","print(DISPLAY_NAME)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"16jwYyrFfjZ4"},"source":["Note that we can define pipeline input values via the `parameter_values` arg."]},{"cell_type":"code","metadata":{"id":"1Uh4rWb3fjZ5"},"source":["result = api_client.create_run_from_job_spec(\n","          job_spec_path='automl_pipeline_spec.json',\n","          name = DISPLAY_NAME,\n","          pipeline_root=PIPELINE_ROOT,  \n","          parameter_values={'gcp_project_id': '{}'.format(PROJECT_ID),\n","                           'dataset_display_name': DISPLAY_NAME,\n","                            'endpoint_display_name': DISPLAY_NAME,\n","                            'training_display_name': DISPLAY_NAME,\n","                            'thresholds_dict_str': '{\"meanAbsoluteError\": 470}',\n","                           })"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LpSGPdo9fjZ5"},"source":["Visit the running pipeline job in the Cloud Console by clicking the link above. As it runs, you should see a graph like the following.  \n","\n","<a href=\"https://storage.googleapis.com/amy-jo/images/automl/ucaip_automl_tabular_pipeline_in_progress.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/automl/ucaip_automl_tabular_pipeline_in_progress.png\" width=\"90%\"/></a>\n","\n","You can view and manage information about your dataset, model, and endpoint in the [Cloud Console](https://console.cloud.google.com/ai/platform/models) as well.\n"]},{"cell_type":"markdown","metadata":{"id":"vPPbu5jeghbj"},"source":["## View Pipelines metadata using the Metadata Server\n","\n","The set of artifacts and executions produced by the pipeline can also be queried using the AIPlatform Metadata SDK. The following shows a snippet for querying the metadata for a given pipeline run:"]},{"cell_type":"code","metadata":{"id":"csZSsQHO1ZdQ"},"source":["from google.cloud import aiplatform\n","\n","from google import auth\n","from google.cloud.aiplatform_v1alpha1 import MetadataServiceClient\n","from google.auth.transport import grpc, requests\n","from google.cloud.aiplatform_v1alpha1.services.metadata_service.transports import grpc as transports_grpc\n","\n","import pandas as pd\n","\n","def _initialize_metadata_service_client() -> MetadataServiceClient:\n","  scope = 'https://www.googleapis.com/auth/cloud-platform'\n","  api_uri = 'us-central1-aiplatform.googleapis.com'\n","  credentials, _ = auth.default(scopes=(scope,))\n","  request = requests.Request()\n","  channel = grpc.secure_authorized_channel(credentials, request, api_uri)\n","\n","  return MetadataServiceClient(\n","      transport=transports_grpc.MetadataServiceGrpcTransport(channel=channel))\n","\n","md_client = _initialize_metadata_service_client()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9hBBZz5g41Ey"},"source":["def get_run_context_name(pipeline_run):\n","  contexts = md_client.list_contexts(parent='projects/{}/locations/{}/metadataStores/default'.format(PROJECT_ID, REGION))\n","  for context in contexts:\n","    if context.display_name == pipeline_run:\n","      return context.name\n","  \n","run_context_name = get_run_context_name(DISPLAY_NAME)  # <- Name of the pipeline run\n","\n","md_client.query_context_lineage_subgraph(context=run_context_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3QdnVXcwpZAi"},"source":["## Create a scheduled recurrent pipeline job\n","\n","This section shows how to create a scheduled pipeline job.  \n","\n","> At time of writing, it is not yet possible to pass pipeline args to the scheduling method.\n","(This feature will be **supported in the next release** of the client libs). So, for purposes of this example, we'll generate a version of the pipeline with its inputs hardwired.  Otherwise, this pipeline is the same as the one defined above.\n","\n","Under the hood, the scheduled jobs are supported by the Cloud Scheduler and a Cloud Functions function.  **Check first that the APIs for both of these services are enabled**. \n","\n","See the [Cloud Scheduler](https://cloud.google.com/scheduler/docs/configuring/cron-job-schedules) documentation for more on the cron syntax used."]},{"cell_type":"code","metadata":{"id":"jVZBMmg96CaM"},"source":["from aiplatform.pipelines import schedule"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d8Ks3Vps08Cb"},"source":["import time\n","DISPLAY_NAME = 'automl{}'.format(str(int(time.time())))\n","print(DISPLAY_NAME)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjksVdC_0xLt"},"source":["Create a scheduled pipeline job. (You may see an ignorable auth warning)."]},{"cell_type":"code","metadata":{"id":"RvUu12Rj6G8Z"},"source":["# adjust time zone and cron schedule as necessary\n","schedule.create_from_pipeline_file(\n","    pipeline_path='automl_pipeline_spec.json',\n","    schedule='0 7 * * *',  # run at 7am every day\n","    project_id=PROJECT_ID,\n","    region=REGION,\n","    time_zone='America/Los_Angeles', # change this as necessary\n","    parameter_values={'gcp_project_id': '{}'.format(PROJECT_ID),\n","                      'dataset_display_name': DISPLAY_NAME,\n","                      'endpoint_display_name': DISPLAY_NAME,\n","                      'training_display_name': DISPLAY_NAME,\n","                      'thresholds_dict_str': '{\"meanAbsoluteError\": 470}',\n","                      }    \n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Dvbvc_ksuxE"},"source":["Once the scheduled job is created, you can see it listed in the [Cloud Scheduler](https://console.cloud.google.com/cloudscheduler/) panel in the Console.\n","\n","<a href=\"https://storage.googleapis.com/amy-jo/images/kf-pls/pipelines_scheduler.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/kf-pls/pipelines_scheduler.png\" width=\"95%\"/></a>\n","\n","You can test the setup from the Cloud Scheduler panel by clicking 'RUN NOW'.\n","\n","The implementation is using a GCF function, which you can see listed in the [Cloud Functions](https://console.cloud.google.com/functions/list) panel in the console as `templated_http_request-v1`.  \n","Don't delete this GCF function, as it will prevent the Cloud Scheduler jobs from actually kicking off the pipeline run.  If you do delete it, you will need to create a new scheduled job in order to recreate it. \n","\n","> After experimenting with the scheduled job, you will probably want to **DELETE** or **PAUSE** it so that it does not continue to run every day."]},{"cell_type":"markdown","metadata":{"id":"qBC-TUrpTtlA"},"source":["## Define and add TFDV pipeline components\n","\n","Now that we have the basics of the AutoML e2e workflow running, we can consider an additional scenario: as new data becomes available, how might we determine whether our deployed model needs retraining?  One aspect of that decision is whether or not the original and newer datasets are significantly different.\n","\n","This part of the example demonstrates use of the [TFDV](https://www.tensorflow.org/tfx/guide/tfdv) library to build pipeline *components* that derive dataset **statistics** and detect **drift** between older and newer datasets, and shows how to use drift information to decide whether to retrain a model on newer data.\n","\n","> Note: TFDV also supports some nice notebook widgets for visualization of the analysis results, though this notebook doesn't include a demo of that.\n","\n","We'll define both TFDV components as ['lightweight' Python-function-based components](https://www.kubeflow.org/docs/pipelines/sdk/python-function-components/). For each component, we define a function, then call `kfp.components.func_to_container_op()` on that function to build a reusable component in `.yaml` format. \n","\n","For these components, we need to specify a base container image that will run the function.  We'll use one that has the TFDV libraries installed (its Dockerfile is [here](https://github.com/amygdala/code-snippets/blob/keras_tuner3/ml/kubeflow-pipelines/keras_tuner/components/tfdv/Dockerfile))."]},{"cell_type":"markdown","metadata":{"id":"wJDhVEZJT8bW"},"source":["### Component to generate stats on the dataset\n","\n","This component uses TFDV to generate statistics on a given dataset.\n","\n","It uses a Beam pipeline— not to be confused with KFP Pipelines— to do this. Depending upon configuration, the component can use either the Direct (local) runner or the [Dataflow](https://cloud.google.com/dataflow#section-5) runner. This is determined by the `use_dataflow` input param.\n","\n","Running the Beam pipeline on Dataflow rather than locally can make sense with large datasets. \n","\n","If you do run this component locally (that is, you leave the computation on the pipeline step node rather than starting a Dataflow job), then with the training datatset used for this example, you will need to give that node more memory than the default.  This is done at the pipeline definition stage— see that section below for more info.\n","\n","If you use Dataflow, first ensure that the Datflow API is enabled for your GCP project.  You will also need to give service account used by Managed Pipelines permission to run the Dataflow jobs: \n","\n","```\n","gcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n","--member=serviceAccount:service-${PROJECT_NUMBER}@gcp-sa-aiplatform-cc.iam.gserviceaccount.com \\\n","--role=roles/dataflow.admin\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ebAs_nDi_rbX"},"source":["We'll first define the python function:"]},{"cell_type":"code","metadata":{"id":"tYrOzo4X0QLl"},"source":["\n","from kfp.components import OutputPath\n","\n","def generate_tfdv_stats(input_data: str, output_path: str, job_name: str, use_dataflow: str,\n","                        project_id: str, region:str, gcs_temp_location: str, gcs_staging_location: str,\n","                        whl_location: str, requirements_file: str,\n","                        stats: OutputPath('String')\n","):\n","\n","  import logging\n","  import subprocess\n","  import time\n","\n","  import tensorflow_data_validation as tfdv\n","  import tensorflow_data_validation.statistics.stats_impl\n","  from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions\n","\n","  logging.getLogger().setLevel(logging.INFO)\n","  logging.info(\"output path: %s\", output_path)\n","  logging.info(\"stats: %s\", stats)\n","  logging.info(\"Building pipeline options\")\n","  # Create and set your PipelineOptions.\n","  options = PipelineOptions()\n","\n","  if use_dataflow == 'true':\n","    logging.info(\"using Dataflow\")\n","    if not whl_location:\n","      logging.warning('tfdv whl file required with dataflow runner.')\n","      exit(1)\n","    # For Cloud execution, set the Cloud Platform project, job_name,\n","    # staging location, temp_location and specify DataflowRunner.\n","    google_cloud_options = options.view_as(GoogleCloudOptions)\n","    google_cloud_options.project = project_id\n","    google_cloud_options.job_name = '{}-{}'.format(job_name, str(int(time.time())))\n","    google_cloud_options.staging_location = gcs_staging_location\n","    google_cloud_options.temp_location = gcs_temp_location\n","    google_cloud_options.region = region\n","    options.view_as(StandardOptions).runner = 'DataflowRunner'\n","\n","    setup_options = options.view_as(SetupOptions)\n","    # PATH_TO_WHL_FILE should point to the downloaded tfdv wheel file.\n","    setup_options.extra_packages = [whl_location]\n","    setup_options.requirements_file = 'requirements.txt'\n","\n","  tfdv.generate_statistics_from_csv(\n","    data_location=input_data, output_path=output_path,\n","    pipeline_options=options)\n","  # write the output path as output\n","  with open('temp.txt', \"w\") as outfile:\n","    outfile.write(output_path)\n","  subprocess.run(['gsutil', 'cp', 'temp.txt', stats])  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ecyngEySP0sF"},"source":["Next, we'll create the component from the function definition:"]},{"cell_type":"code","metadata":{"id":"3_GOGbeZ0ofX"},"source":["from kfp import components\n","\n","components.func_to_container_op(generate_tfdv_stats,\n","    output_component_file='tfdv_component.yaml', base_image='gcr.io/google-samples/tfdv-tests:v1')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u8aU-vBkUEZo"},"source":["### Component to detect dataset 'drift'\n","\n","Next we'll define a component to detect drift on the `duration` field between two sets of derived stats files from two datasets.  See the [TFDV docs](https://www.tensorflow.org/tfx/data_validation/get_started#checking_data_skew_and_drift) for more detail.\n","\n","(The component itself makes no assumptions about the datasets used to generate of its input stats files.  However, when we add these to the pipeline, we'll use as the 'new stats' file the output of the component above, and compare that new file to previously-generated stats for the previous version of the dataset).\n"]},{"cell_type":"code","metadata":{"id":"AMO_RycJ0RjA"},"source":["\n","from kfp.components import OutputPath\n","\n","def tfdv_detect_drift(\n","    stats_older_path: str, stats_new_path: str, # expects outputs files from tfdv.generate_statistics_from_csv()\n","    drift_res: OutputPath('String')\n","):\n","\n","  import logging\n","  import subprocess\n","  import time\n","\n","  import tensorflow_data_validation as tfdv\n","  import tensorflow_data_validation.statistics.stats_impl\n","\n","  logging.getLogger().setLevel(logging.INFO)\n","  logging.info('stats_older_path: %s', stats_older_path)\n","  logging.info('stats_new_path: %s', stats_new_path)\n","\n","  if stats_older_path == 'none':\n","    return ('true', )\n","\n","  stats1 = tfdv.load_statistics(stats_older_path)\n","  stats2 = tfdv.load_statistics(stats_new_path)\n","\n","  schema1 = tfdv.infer_schema(statistics=stats1)\n","  tfdv.get_feature(schema1, 'duration').drift_comparator.jensen_shannon_divergence.threshold = 0.01\n","  drift_anomalies = tfdv.validate_statistics(\n","      statistics=stats2, schema=schema1, previous_statistics=stats1)\n","  logging.info('drift analysis results: %s', drift_anomalies.drift_skew_info)\n","\n","  from google.protobuf.json_format import MessageToDict\n","  d = MessageToDict(drift_anomalies)\n","  val = d['driftSkewInfo'][0]['driftMeasurements'][0]['value']\n","  thresh = d['driftSkewInfo'][0]['driftMeasurements'][0]['threshold']\n","  logging.info('value %s and threshold %s', val, thresh)\n","  res = 'true'\n","  if val < thresh:\n","    res = 'false'\n","  logging.info('train decision: %s', res)\n","  with open('temp.txt', \"w\") as outfile:\n","    outfile.write(res)\n","  subprocess.run(['gsutil', 'cp', 'temp.txt', drift_res])    \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WvtJxKnSP5R_"},"source":["Create the component from the function definition..."]},{"cell_type":"code","metadata":{"id":"xZpxNn3q0X0j"},"source":["from kfp import components\n","\n","components.func_to_container_op(tfdv_detect_drift,\n","    output_component_file='tfdv_drift_component.yaml', base_image='gcr.io/google-samples/tfdv-tests:v1')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_8XbvBUVqjV4"},"source":["## Define and run a TFDV + AutoML pipeline using your components\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-6WjcWl8qo8r"},"source":["First, we'll create pipeline ops from the components, using the `load_component_from_file` method.\n","\n","While we're not using it here, there is also a `load_component_from_url` method, which is handy if your component files are checked into a repo or otherwise stored online. (For GitHub files, use the 'raw' URL)."]},{"cell_type":"code","metadata":{"id":"8M-Fup4gr0dW"},"source":["import json\n","from kfp.v2 import dsl\n","from kfp.v2 import compiler\n","from kfp import components\n","\n","automl_e2e_op = components.load_component_from_file(\n","  './automl_tables_component.yaml'\n","  )\n","\n","tfdv_op = components.load_component_from_file(\n","  './tfdv_component.yaml'\n","  )\n","tfdv_drift_op = components.load_component_from_file(\n","  './tfdv_drift_component.yaml'\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g1Hz_nyiqvqW"},"source":["Next, we'll define the pipeline, using the ops defined above."]},{"cell_type":"code","metadata":{"id":"yQHAp_S1EVeS"},"source":["import json\n","# We'll use this transformation specification as an arg for the training step.\n","TRANSFORMATIONS = [\n","    {\"auto\": {\"column_name\": \"bike_id\"}},\n","    {\"auto\": {\"column_name\": \"day_of_week\"}},\n","    {\"auto\": {\"column_name\": \"dewp\"}},\n","    {\"auto\": {\"column_name\": \"duration\"}},\n","    {\"auto\": {\"column_name\": \"end_latitude\"}},\n","    {\"auto\": {\"column_name\": \"end_longitude\"}},\n","    {\"categorical\": {\"column_name\": \"end_station_id\"}},\n","    {\"auto\": {\"column_name\": \"euclidean\"}},\n","    {\"categorical\": {\"column_name\": \"loc_cross\"}},\n","    {\"auto\": {\"column_name\": \"max\"}},\n","    {\"auto\": {\"column_name\": \"min\"}},\n","    {\"auto\": {\"column_name\": \"prcp\"}},\n","    {\"auto\": {\"column_name\": \"start_latitude\"}},\n","    {\"auto\": {\"column_name\": \"start_longitude\"}},\n","    {\"categorical\": {\"column_name\": \"start_station_id\"}},\n","    {\"auto\": {\"column_name\": \"temp\"}},\n","    {\"timestamp\": {\"column_name\": \"ts\"}}\n","]\n","TRANSFORMATIONS_STR = json.dumps(TRANSFORMATIONS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YJiEXWC38l_z"},"source":["We'll first define some pipeline inputs.\n","\n","Note: In the following, we're finessing one aspect of our scenario: the  Dataset pipeline component reads from a BigQuery table.\n","The TFDV stats-generation component reads from a set of CSV files exported from that same BigQuery table. \n","A more robust design would have the Dataset component read from the same set of CSV files (instead of BigQuery), _or_ automate the export from BigQuery to CSV as part of the pipeline."]},{"cell_type":"markdown","metadata":{"id":"on5jhMeFlA9J"},"source":["**Edit this next cell before running.**"]},{"cell_type":"code","metadata":{"id":"nndDsm20NEpE"},"source":["OUTPUT_PATH = 'gs://your/gcs/path'  # CHANGE THIS"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"POklp2wwMoSg"},"source":["(This next cell is doing some work that will soon not be necessary once a bug is fixed)."]},{"cell_type":"code","metadata":{"id":"ytyKaLXYaLx2"},"source":["# Temporarily, we need to generate a unique run id manually.  We'll use that to generate\n","# a unique output path for this run, under which the TFDV eval .pb files will be generated.\n","# In future, this will not be necessary.\n","import time\n","RUN_ID_PLACEHOLDER = 'run{}'.format(str(int(time.time())))\n","\n","# Also temporarily, we need to explicitly construct some of the pipeline steps params from the pipeline input params.\n","# In future, it will be possible to do this construction inline, as part of the pipeline definition.\n","GCS_TEMP_LOCATION = '{}/tmp'.format(OUTPUT_PATH)\n","# DATA_DIR = 'gs://aju-dev-demos-codelabs/bikes_weather_chronological/ds1/'\n","# BIGQUERY_URI = 'bq://aju-dev-demos.london_bikes_weather.bw_ts_sorted1'\n","DATA_DIR = 'gs://aju-dev-demos-codelabs/bikes_weather_chronological/ds2/' # the 'newer' dataset\n","BIGQUERY_URI = 'bq://aju-dev-demos.london_bikes_weather.bw_ts_sorted2'  # the 'newer' dataset\n","TEST_OUTPUT_PATH = '{}/{}/eval/evaltest.pb'.format(OUTPUT_PATH, RUN_ID_PLACEHOLDER)\n","TRAIN_OUTPUT_PATH = '{}/{}/eval/evaltrain.pb'.format(OUTPUT_PATH, RUN_ID_PLACEHOLDER)\n","INPUT_DATA_TEST = '{}test-*.csv'.format(DATA_DIR)\n","INPUT_DATA_TRAIN = '{}train-*.csv'.format(DATA_DIR)\n","JOB_NAME_TEST = RUN_ID_PLACEHOLDER + '-1'\n","JOB_NAME_TRAIN = RUN_ID_PLACEHOLDER + '-2'\n","print(TRAIN_OUTPUT_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BNyHWRkFmt1p"},"source":["print(INPUT_DATA_TRAIN)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DONn1spwAFnd"},"source":["Now we'll define the new pipeline, which augments the AutoML pipeline built in the previous section with the two new TFDV steps.\n","\n","If you run the `tfdv2` step with `use_dataflow` set to `false`, then you will need to uncomment the first line below:\n","```\n","  # tfdv2.set_memory_limit('32G')\n","  # tfdv2.set_gpu_limit(1)\n","  # tfdv2.add_node_selector_constraint('cloud.google.com/gke-accelerator', 'nvidia-tesla-k80')\n","```\n","to give the step more memory. The 2nd and 3rd line show how you'd also run the step on a GPU-enabled node.  "]},{"cell_type":"code","metadata":{"id":"KtTm6WvmsSjC"},"source":["@dsl.pipeline(\n","  name='ucaip-automl-tables-tfdv-subgraph',\n","  description='Demonstrate a ucaip AutoML Tables workflow'\n",")\n","def automl_tables_tfdv_subgraph( \n","  gcp_project_id: str = 'YOUR_PROJECT_HERE',\n","  gcp_region: str = 'us-central1',\n","  dataset_display_name: str = 'YOUR_DATASET_NAME',\n","  api_endpoint: str = 'us-central1-aiplatform.googleapis.com',\n","  timeout: int = 2000,\n","  bigquery_uri: str = 'bq://aju-dev-demos.london_bikes_weather.bw_ts_sorted2',\n","  target_col_name: str = 'duration',\n","  time_col_name: str = 'none',    \n","  transformations: str = TRANSFORMATIONS_STR,\n","  train_budget_milli_node_hours: int = 1000,\n","  model_prefix: str = 'bw',    \n","  # optimization_objective: str = 'minimize-rmse', \n","  training_display_name: str = 'CHANGE THIS',\n","  endpoint_display_name = 'CHANGE THIS',\n","  # if set to other than 'new', use the given endpoint path rather than create new endpoint.  \n","  endpoint_path:str = 'new',\n","  thresholds_dict_str = '{\"meanAbsoluteError\": 470}',\n","\n","  # tfdv-related\n","  region: str = 'us-central1',\n","  requirements_file: str = 'requirements.txt',\n","  job_name: str = 'testx',\n","  gcs_staging_location: str = OUTPUT_PATH,\n","  gcs_temp_location:str = GCS_TEMP_LOCATION,\n","  data_dir: str = 'gs://aju-dev-demos-codelabs/bikes_weather_chronological/ds2/', # requires trailing slash\n","  output_path: str = OUTPUT_PATH,\n","  whl_location: str = 'tensorflow_data_validation-0.26.0-cp37-cp37m-manylinux2010_x86_64.whl',\n","  use_dataflow: str = '',\n","  thresholds: str = '{\"root_mean_squared_error\": 2000}',\n","  stats_older_path: str = 'gs://aju-dev-demos-codelabs/bikes_weather_chronological/evaltrain1.pb'  # tfdv stats for the 'older' dataset\n","  ):\n","\n","  tfdv1 = tfdv_op(\n","    input_data = INPUT_DATA_TEST,\n","    output_path = TEST_OUTPUT_PATH,\n","    job_name=JOB_NAME_TEST,\n","    use_dataflow='false',\n","    # use_dataflow=use_dataflow,\n","    project_id=gcp_project_id, region=region,\n","    gcs_temp_location=gcs_temp_location, gcs_staging_location=gcs_staging_location,\n","    whl_location=whl_location, requirements_file=requirements_file\n","    )\n","  tfdv2 = tfdv_op(\n","    input_data = INPUT_DATA_TRAIN,\n","    output_path = TRAIN_OUTPUT_PATH,\n","    job_name=JOB_NAME_TRAIN,\n","    use_dataflow=use_dataflow,\n","    # use_dataflow='false',\n","    project_id=gcp_project_id, region=region,\n","    gcs_temp_location=gcs_temp_location, gcs_staging_location=gcs_staging_location,\n","    whl_location=whl_location, requirements_file=requirements_file\n","    )\n","  # tfdv2.set_memory_limit('32G')\n","  # tfdv2.set_gpu_limit(1)\n","  # tfdv2.add_node_selector_constraint('cloud.google.com/gke-accelerator', 'nvidia-tesla-k80')\n","\n","  tfdv_drift = tfdv_drift_op(stats_older_path, tfdv2.outputs['stats'])\n","\n","  automl_e2e = automl_e2e_op(\n","    gcp_project_id,\n","    gcp_region,\n","    dataset_display_name,\n","    api_endpoint, timeout, bigquery_uri,\n","    target_col_name, time_col_name, transformations, \n","    train_budget_milli_node_hours, model_prefix,\n","    training_display_name, endpoint_display_name,\n","    endpoint_path, thresholds_dict_str,\n","    tfdv_drift.outputs['drift_res']\n","  )  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BeS7LA_1ZIMa"},"source":["Compile the pipeline..."]},{"cell_type":"code","metadata":{"id":"ot0UW7dT7WAZ"},"source":["compiler.Compiler().compile(pipeline_func=automl_tables_tfdv_subgraph,\n","                            pipeline_root=PIPELINE_ROOT,\n","                            output_path='automl_tfdv_pipeline_spec.json')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5RDgxW1pBW1H"},"source":["... then run it."]},{"cell_type":"code","metadata":{"id":"SDUsSmiz7WAZ"},"source":["import time\n","from aiplatform.pipelines import client\n","\n","api_client = client.Client(project_id=PROJECT_ID, region=REGION, api_key=API_KEY)\n","DISPLAY_NAME = 'automltfdv{}'.format(str(int(time.time())))\n","print(DISPLAY_NAME)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKfoh_3jB0I3"},"source":["Note that we can define pipeline input values via the `parameter_values` arg."]},{"cell_type":"code","metadata":{"id":"J8SO7gEi7WAZ"},"source":["result = api_client.create_run_from_job_spec(\n","          job_spec_path='automl_tfdv_pipeline_spec.json',\n","          name = DISPLAY_NAME,\n","          parameter_values={'gcp_project_id': '{}'.format(PROJECT_ID),\n","                           'dataset_display_name': DISPLAY_NAME,\n","                            'endpoint_display_name': DISPLAY_NAME,\n","                            'training_display_name': DISPLAY_NAME,\n","                            'thresholds_dict_str': '{\"meanAbsoluteError\": 470}',\n","                            'use_dataflow': 'true',\n","                            'data_dir': DATA_DIR, 'bigquery_uri': BIGQUERY_URI\n","                           })"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jLV5VGAk3ZHi"},"source":["Visit the running pipeline job in the Cloud Console by clicking the link above. As it runs, you should see a graph like the following.  \n","\n","<a href=\"https://storage.googleapis.com/amy-jo/images/automl/ucaip-automl-tables-tfdv.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/automl/ucaip-automl-tables-tfdv.png\" width=\"90%\"/></a>\n","\n","You can view and manage information about your dataset, model, and endpoint in the [Cloud Console](https://console.cloud.google.com/ai/platform/models) as well.\n"]},{"cell_type":"markdown","metadata":{"id":"MHPgJ_cT3e1L"},"source":["### How to automatically rerun this pipeline in the presence of new data?\n","\n","[This MP example notebook](https://colab.research.google.com/drive/1njIzO3XIEgpaMe2CV2xUl_rAj3zId2JF) shows how to use GCF to support event-triggered Managed Pipelines. \n","\n","(**TODO**): add a version of that example which shows use of TFDV on Managed Pipelines. "]},{"cell_type":"markdown","metadata":{"id":"5kUmXtBAjZnk"},"source":["## (TODO) Using your deployed model for prediction\n","\n","..."]},{"cell_type":"code","metadata":{"id":"vtY9fu5snqpr"},"source":["from google.cloud import aiplatform\n","\n","def predict_custom_model_sample(endpoint: str, instance: dict, parameters_dict: dict):\n","    client_options = dict(api_endpoint=\"us-central1-prediction-aiplatform.googleapis.com\")\n","    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n","\n","    from google.protobuf import json_format\n","    from google.protobuf.struct_pb2 import Value\n","\n","    # The format of the parameters must be consistent with what the model expects.\n","    parameters = json_format.ParseDict(parameters_dict, Value())\n","\n","    # The format of the instances must be consistent with what the model expects.\n","    instances_list = [instance]\n","    instances = [json_format.ParseDict(s, Value()) for s in instances_list]\n","    response = client.predict(\n","        endpoint=endpoint, instances=instances, parameters=parameters\n","    )\n","\n","    print(\"response\")\n","    print(\" deployed_model_id:\", response.deployed_model_id)\n","    predictions = response.predictions\n","    print(\"predictions\")\n","    for prediction in predictions:\n","        print(\" prediction:\", dict(prediction))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxh_IkolnvPX"},"source":["endpoint_path = \"projects/467744782358/locations/us-central1/endpoints/6770352799193497600\"  # aju temp testing\n","instance1 =  {\n","      \"bike_id\": \"5373\",\n","      \"day_of_week\": \"3\",\n","      \"end_latitude\": 51.52059681,\n","      \"end_longitude\": -0.116688468,\n","      \"end_station_id\": \"68\",\n","      \"euclidean\": 3589.5146210024977,\n","      \"loc_cross\": \"POINT(-0.07 51.52)POINT(-0.12 51.52)\",\n","      \"max\": 44.6,\n","      \"min\": 34.0,\n","      \"prcp\": 0,\n","      \"ts\": \"1480407420\",\n","      \"start_latitude\": 51.52388,\n","      \"start_longitude\": -0.065076,\n","      \"start_station_id\": \"445\",\n","      \"temp\": 38.2,\n","      \"dewp\": 28.6\n","    }\n","\n","predict_custom_model_sample(\n","    endpoint_path,\n","    instance1, {}\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"89fYarRLW7cN"},"source":["-----------------------------\n","Copyright 2021 Google LLC\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","\n","     http://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing, software\n","distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License."]}]}