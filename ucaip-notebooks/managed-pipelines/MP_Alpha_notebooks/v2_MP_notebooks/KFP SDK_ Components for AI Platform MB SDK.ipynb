{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpYscdzmf4Gu"
   },
   "source": [
    "# Examples of AutoML workflows using google-cloud-pipeline-components\n",
    "\n",
    "This notebook shows preliminary examples of how to build pipelines using new components for AI Platform (Unified) services. These components are based on the new [high-level AI Platform (Unified) SDK](https://cloud.google.com/ai-platform-unified/docs/start/client-libraries#client_libraries), available now in Preview.\n",
    "\n",
    "More documentation on these components will be available soon. \n",
    "\n",
    "For this demo ensure the following APIs are enabled:\n",
    "- [Cloudbuild](https://pantheon.corp.google.com/apis/library/cloudbuild.googleapis.com?q=Cloudbuild)\n",
    "- [Container Registry](https://pantheon.corp.google.com/apis/library/containerregistry.googleapis.com?q=container%20registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWACue6PW7bk"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Before you run this notebook, ensure that your Google Cloud user account and project are granted access to the Managed Pipelines Experimental. To be granted access to the Managed Pipelines Experimental, fill out this [form](http://go/cloud-mlpipelines-signup) and let your account representative know you have requested access. \n",
    "\n",
    "This notebook is intended to be run on either one of:\n",
    "* [AI Platform Notebooks](https://cloud.google.com/ai-platform-notebooks). See the \"AI Platform Notebooks\" section in the Experimental [User Guide](https://docs.google.com/document/d/1JXtowHwppgyghnj1N1CT73hwD1caKtWkLcm2_0qGBoI/edit?usp=sharing) for more detail on creating a notebook server instance.\n",
    "* [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb)\n",
    "\n",
    "\n",
    "**To run this notebook on AI Platform Notebooks**, click on the **File** menu, then select \"Download .ipynb\".  Then, upload that notebook from your local machine to AI Platform Notebooks. (In the AI Platform Notebooks left panel, look for an icon of an arrow pointing up, to upload).\n",
    "\n",
    "We'll first install some libraries and set up some variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAaCPLjgiJrO"
   },
   "source": [
    "Set `gcloud` to use your project.  **Edit the following cell before running it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD5jOcSURdcU"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'your-project-id'  # <---CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkWdxe4TXRHk"
   },
   "outputs": [],
   "source": [
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gckGHdW9iPrq"
   },
   "source": [
    "If you're running this notebook on colab, authenticate with your user account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZQA0KrfXCvU"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaqJjbmk6o0o"
   },
   "source": [
    "-----------------\n",
    "\n",
    "**If you're on AI Platform Notebooks**, authenticate with Google Cloud before running the next section, by running\n",
    "```sh\n",
    "gcloud auth login\n",
    "```\n",
    "**in the Terminal window** (which you can open via **File** > **New** in the menu). You only need to do this once per notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOpZ41iBW7bl"
   },
   "source": [
    "### Install the KFP SDK and AI Platform Pipelines client library\n",
    "\n",
    "For Managed Pipelines Experimental, you'll need to download special versions of the KFP SDK and the AI Platform client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHzv7rQ_hHnC"
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://cloud-aiplatform-pipelines/releases/latest/kfp-1.5.0rc5.tar.gz .\n",
    "!gsutil cp gs://cloud-aiplatform-pipelines/releases/latest/aiplatform_pipelines_client-0.1.0.caip20210415-py3-none-any.whl .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpdfRA4vW7bq"
   },
   "source": [
    "Then, install the libraries and restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmUZzSv6YA9-"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "  USER_FLAG = ''\n",
    "else:\n",
    "  USER_FLAG = '--user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGdU0lEfVwM-"
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install {USER_FLAG} kfp-1.5.0rc5.tar.gz aiplatform_pipelines_client-0.1.0.caip20210415-py3-none-any.whl google-cloud-aiplatform --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ghwOw1ZiHzY"
   },
   "outputs": [],
   "source": [
    "\n",
    "!python3 -m pip install {USER_FLAG} \"git+https://github.com/kubeflow/pipelines.git#egg=google-cloud-pipeline-components&subdirectory=components/google-cloud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IKZcgZnX3j6"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1GX5KDOUJuI"
   },
   "source": [
    "If you're on colab, re-authorize after the kernel restart. **Edit the following cell for your project ID before running it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpkxFp93xBk5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  PROJECT_ID = 'your-project-id'  # <---CHANGE THIS\n",
    "  !gcloud config set project {PROJECT_ID}\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "  USER_FLAG = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mqs-ZFuW7bx"
   },
   "source": [
    "The KFP version should be >= 1.5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4uvTyimMYOr"
   },
   "outputs": [],
   "source": [
    "# Check the KFP version\n",
    "!python3 -c \"import kfp; print('KFP version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tskC13YxW7b3"
   },
   "source": [
    "### Set some variables and do some imports\n",
    "\n",
    "**Before you run the next cell**, **edit it** to set variables for your project.  See the \"Before you begin\" section of the User Guide for information on creating your API key.  For `BUCKET_NAME`, enter the name of a Cloud Storage (GCS) bucket in your project.  Don't include the `gs://` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHsVifdTW7b4"
   },
   "outputs": [],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "# Required Parameters\n",
    "USER = 'YOUR_USER_NAME' # <---CHANGE THIS\n",
    "BUCKET_NAME = 'YOUR_BUCKET_NAME'  # <---CHANGE THIS\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(BUCKET_NAME, USER)\n",
    "\n",
    "PROJECT_ID = 'YOUR_PROJECT_ID'  # <---CHANGE THIS\n",
    "REGION = 'us-central1'\n",
    "API_KEY = 'YOUR_API_KEY'  # <---CHANGE THIS\n",
    "\n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzA4psp5DmD5"
   },
   "source": [
    "## Create a container for the component\n",
    "Note: Soon, a prebuilt container will be available and this step will not be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVeG-LPOftDy"
   },
   "source": [
    "### Create Cloudbuild YAML\n",
    "Using Kaniko cache to speed up build time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0FOjU6rdAGf"
   },
   "outputs": [],
   "source": [
    "CONTAINER_ARTIFACTS_DIR=\"demo-container-artifacts\"\n",
    "!mkdir -p {CONTAINER_ARTIFACTS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kODuFZCftDy"
   },
   "outputs": [],
   "source": [
    "# You can add a faster build machine using: \n",
    "# options:\n",
    "#   machineType: 'E2_HIGHCPU_8'\n",
    "\n",
    "cloudbuild_yaml=f\"\"\"steps:\n",
    "- name: 'gcr.io/kaniko-project/executor:latest'\n",
    "  args: \n",
    "  - --destination=gcr.io/$PROJECT_ID/test-custom-container\n",
    "  - --cache=false\n",
    "  - --cache-ttl=99h\n",
    "\"\"\"\n",
    "\n",
    "CONTAINER_GCR_URI=f\"gcr.io/{PROJECT_ID}/test-custom-container\" \n",
    "with open(f\"{CONTAINER_ARTIFACTS_DIR}/cloudbuild.yaml\", 'w') as fp:\n",
    "    fp.write(cloudbuild_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ_GUCtZftDz"
   },
   "source": [
    "### Write Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rja_jo3rftDz"
   },
   "outputs": [],
   "source": [
    "%%writefile {CONTAINER_ARTIFACTS_DIR}/Dockerfile\n",
    "\n",
    "# Base image to use for this docker\n",
    "FROM gcr.io/google-appengine/python:latest\n",
    "\n",
    "WORKDIR /root\n",
    "\n",
    "# Upgrade pip to latest\n",
    "RUN pip3 install --upgrade pip\n",
    "\n",
    "# Installs additional packages\n",
    "RUN pip3 install google-cloud-aiplatform --upgrade\n",
    "\n",
    "RUN pip3 install \"git+https://github.com/kubeflow/pipelines.git#egg=google-cloud-pipeline-components&subdirectory=components/google-cloud\"\n",
    "\n",
    "\n",
    "ENTRYPOINT [\"python3\",\"-m\",\"google_cloud_pipeline_components.aiplatform.remote_runner\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LYlV4D2ftD0"
   },
   "source": [
    "### Build Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tGdX7B_ftD1"
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --config {CONTAINER_ARTIFACTS_DIR}/cloudbuild.yaml {CONTAINER_ARTIFACTS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pf0pugbvftD1"
   },
   "source": [
    "## AutoML image classification\n",
    "\n",
    "Create a managed image dataset from CSV and train it using Automl Image Training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZlbf2cBKUVx"
   },
   "outputs": [],
   "source": [
    "CONTAINER_GCR_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fjGiImBezMo"
   },
   "source": [
    "Define the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEEr62NUftD1"
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from aiplatform.pipelines import client\n",
    "\n",
    "gcc_aip.utils.DEFAULT_CONTAINER_IMAGE=CONTAINER_GCR_URI\n",
    "\n",
    "@kfp.dsl.pipeline(name='automl-image-training-v2')\n",
    "def pipeline():\n",
    "  ds_op = gcc_aip.ImageDatasetCreateOp(\n",
    "      project=PROJECT_ID,\n",
    "      display_name='flowers',\n",
    "      gcs_source='gs://cloud-samples-data/vision/automl_classification/flowers/all_data_v2.csv',\n",
    "      import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,)\n",
    "\n",
    "  training_job_run_op = gcc_aip.AutoMLImageTrainingJobRunOp(\n",
    "      project=PROJECT_ID,\n",
    "      display_name='train-iris-automl-mbsdk-1',\n",
    "      prediction_type='classification',\n",
    "      model_type=\"CLOUD\",\n",
    "      base_model=None,\n",
    "      dataset=ds_op.outputs['dataset'],\n",
    "      model_display_name='iris-classification-model-mbsdk',     \n",
    "      training_fraction_split=0.6,\n",
    "      validation_fraction_split=0.2,\n",
    "      test_fraction_split=0.2,\n",
    "      budget_milli_node_hours=8000,\n",
    "  )\n",
    "  endpoint_op = gcc_aip.ModelDeployOp(\n",
    "      project=PROJECT_ID,\n",
    "      model=training_job_run_op.outputs['model'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVU0HBwkewvm"
   },
   "source": [
    "Compile your pipeline, and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mejq1oFWEsha"
   },
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='image_classif_pipeline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ysd9dCxXCH2P"
   },
   "outputs": [],
   "source": [
    "api_client = client.Client(project_id=PROJECT_ID, region='us-central1',\n",
    "                          api_key=API_KEY)\n",
    "\n",
    "response = api_client.create_run_from_job_spec('image_classif_pipeline.json', pipeline_root=PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9PBzzr7sGin"
   },
   "source": [
    "## AutoML Tabular Classification\n",
    "\n",
    "Define and run an AutoML Tabular Classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYGHro4qFhp5"
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE_NAME = 'california_housing_train.csv'\n",
    "!gsutil cp sample_data/california_housing_train.csv {PIPELINE_ROOT}/data/\n",
    "\n",
    "gcs_csv_path = f'{PIPELINE_ROOT}/data/{TRAIN_FILE_NAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7nDFZWus7M8"
   },
   "source": [
    "Define the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sC0jQS6tGL_d"
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from aiplatform.pipelines import client\n",
    "\n",
    "gcc_aip.utils.DEFAULT_CONTAINER_IMAGE=CONTAINER_GCR_URI\n",
    "\n",
    "@kfp.dsl.pipeline(name='automl-tab-training-v2')\n",
    "def pipeline():\n",
    "  dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "      project=PROJECT_ID, \n",
    "      display_name='housing',\n",
    "      gcs_source=gcs_csv_path)\n",
    "\n",
    "  training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n",
    "      project=PROJECT_ID,\n",
    "      display_name='train-housing-automl_1',\n",
    "      optimization_prediction_type='regression',\n",
    "      optimization_objective='minimize-rmse',    \n",
    "      column_transformations=[\n",
    "          {\"numeric\": {\"column_name\": \"longitude\"}},\n",
    "          {\"numeric\": {\"column_name\": \"latitude\"}},\n",
    "          {\"numeric\": {\"column_name\": \"housing_median_age\"}},\n",
    "          {\"numeric\": {\"column_name\": \"total_rooms\"}},\n",
    "          {\"numeric\": {\"column_name\": \"total_bedrooms\"}},\n",
    "          {\"numeric\": {\"column_name\": \"population\"}},\n",
    "          {\"numeric\": {\"column_name\": \"households\"}},\n",
    "          {\"numeric\": {\"column_name\": \"median_income\"}},\n",
    "      ],\n",
    "      dataset = dataset_create_op.outputs['dataset'],\n",
    "      target_column = \"longitude\"\n",
    "  )\n",
    "\n",
    "  deploy_op = gcc_aip.ModelDeployOp(\n",
    "      model=training_op.outputs['model'],\n",
    "      project=PROJECT_ID,\n",
    "      machine_type='n1-standard-4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-Rd6905zxhy"
   },
   "source": [
    "Compile your pipeline, and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2N3Y-sFFGYN"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='tab_classif_pipeline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTkhvmfJsF0X"
   },
   "outputs": [],
   "source": [
    "api_client = client.Client(project_id=PROJECT_ID, region='us-central1',\n",
    "                          api_key=API_KEY)\n",
    "\n",
    "response = api_client.create_run_from_job_spec('tab_classif_pipeline.json', \n",
    "                                               pipeline_root=PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJKmkefDybhC"
   },
   "source": [
    "## AutoML Text Classification\n",
    "\n",
    "Define and run an AutoML Text Classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mLpNwQkye2G"
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components.aiplatform import TextDatasetCreateOp, AutoMLTextTrainingJobRunOp, ModelDeployOp\n",
    "from aiplatform.pipelines import client\n",
    "\n",
    "import uuid\n",
    "\n",
    "IMPORT_FILE = \"gs://cloud-ml-data/NL-classification/happiness.csv\"\n",
    "gcc_aip.utils.DEFAULT_CONTAINER_IMAGE=CONTAINER_GCR_URI\n",
    "\n",
    "@kfp.dsl.pipeline(name='automl-text-classification' + str(uuid.uuid4()))\n",
    "def pipeline(\n",
    "    project: str=PROJECT_ID,\n",
    "    import_file: str=IMPORT_FILE):\n",
    "    \n",
    "    dataset_create_task = TextDatasetCreateOp(\n",
    "            display_name=\"happydb\",\n",
    "            gcs_source=import_file,\n",
    "            import_schema_uri=aiplatform.schema.dataset.ioformat.text.multi_label_classification,\n",
    "            project=project\n",
    "        )\n",
    "    \n",
    "    training_run_task = AutoMLTextTrainingJobRunOp(\n",
    "        dataset=dataset_create_task.outputs['dataset'],\n",
    "        display_name=\"train-housing-automl_1\",\n",
    "        prediction_type=\"classification\",\n",
    "        multi_label=True,\n",
    "        training_fraction_split=0.6,\n",
    "        validation_fraction_split=0.2,\n",
    "        test_fraction_split=0.2,\n",
    "        model_display_name=\"happy-model\",\n",
    "        project=project\n",
    "    )\n",
    "    \n",
    "    model_deploy_op = ModelDeployOp(\n",
    "        model=training_run_task.outputs['model'],\n",
    "        project=project\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Bgiiqt9ygDP"
   },
   "source": [
    "Compile your pipeline, and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEicvGNSyhJY"
   },
   "outputs": [],
   "source": [
    "pipeline = compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='text_classsif_pipeline.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvlQCgeIyrvX"
   },
   "outputs": [],
   "source": [
    "api_client = client.Client(project_id=PROJECT_ID, region='us-central1',\n",
    "                          api_key=API_KEY)\n",
    "\n",
    "response = api_client.create_run_from_job_spec('text_classsif_pipeline.json', \n",
    "                                    pipeline_root=PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89fYarRLW7cN"
   },
   "source": [
    "-----------------------------\n",
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KFP SDK: Components for AI Platform MB SDK.ipynb",
   "provenance": [
    {
     "file_id": "1wlUBBAK1VdySRXQWACeKD1dFebvshPXX",
     "timestamp": 1619017781669
    },
    {
     "file_id": "1U_n4BYqvnvxv1z_1iqNpZT9NeEBwJOX9",
     "timestamp": 1617985790340
    },
    {
     "file_id": "1q-9LDgu6rHV9ftjhK8I9XvT-jWqvCIPP",
     "timestamp": 1616079732995
    },
    {
     "file_id": "1Orb3y6mkhjFsIPXG0QhIyNfHxy3uHeHe",
     "timestamp": 1616078261849
    }
   ],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
