{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/unofficial/AI_Platform_(Unified)_SDK_Custom_Container_Prediction_Keras.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial walks through building a custom container to serve a Keras model on AI Platform Predictions. You will use the FastAPI Python web server framework to create a prediction and health endpoint.\n",
    "You will also cover incorporating a pre-processor from training into your online serving.\n",
    "\n",
    "This tutorial shows how to deploy a trained Keras model to AI Platform and serve predictions on AI Platform Predictions using a [custom container](https://cloud.google.com/ai-platform-unified/docs/predictions/use-custom-container). This lets you customize how AI Platform responds to each prediction request.\n",
    "\n",
    "In this example, you will use a custom container with a preprocessing step that scales prediction input, and a postprocess step to convert prediction output from [softmax](https://developers.google.com/machine-learning/glossary/#s) probability outputs to label strings.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "This tutorial uses R.A. Fisher's Iris dataset, a small dataset that is popular for trying out machine learning techniques. Each instance has four numerical features, which are different measurements of a flower, and a target label that\n",
    "marks it as one of three types of iris: Iris setosa, Iris versicolour, or Iris virginica.\n",
    "\n",
    "This tutorial uses [the copy of the Iris dataset included in the\n",
    "scikit-learn library](https://scikit-learn.org/stable/datasets/index.html#iris-dataset).\n",
    "\n",
    "### Objective\n",
    "\n",
    "The goal is to train a model that uses a flower's measurements as input to predict what type of iris it is.\n",
    "\n",
    "The tutorial walks through several steps:\n",
    "\n",
    "- Training a simple Keras model locally (in this notebook)\n",
    "- Save the model and its serialized pre-processor and post-processor\n",
    "- Build a FastAPI server to handle predictions and health checks\n",
    "- Build a custom container with model artifacts\n",
    "- Upload and deploy custom container to AI Platform Prediction\n",
    "- Serve prediction requests from that deployment\n",
    "\n",
    "This tutorial focuses more on deploying this model with AI Platform than on\n",
    "the design of the model itself.\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* AI Platform (Unified)\n",
    "\n",
    "Learn about [AI Platform (Unified)\n",
    "pricing](https://cloud.google.com/ai-platform-unified/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or AI Platform Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* Docker\n",
    "* Git\n",
    "* Google Cloud SDK (gcloud)\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install additional packages\n",
    "\n",
    "Install additional package dependencies not installed in your notebook environment, such as Tensorflow, NumPy, Scikit-learn, FastAPI, Uvicorn, and joblib. Use the latest major GA version of each package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "747f59abb3a5",
    "outputId": "78c4a18b-a9e0-4046-cdb9-e31ab8ce8eed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "joblib~=1.0\n",
    "numpy~=1.20\n",
    "scikit-learn~=0.24\n",
    "tensorflow>=1.15\n",
    "google-cloud-storage>=1.26.0,<2.0.0dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyy5Lbnzg5fi"
   },
   "outputs": [],
   "source": [
    "# Required in Docker serving container\n",
    "%pip install -U --user -r requirements.txt\n",
    "\n",
    "# For local FastAPI development and running\n",
    "%pip install -U --user \"uvicorn[standard]>=0.12.0,<0.14.0\" fastapi~=0.63\n",
    "\n",
    "# AI Platform (Unified) SDK\n",
    "%pip install -U --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the AI Platform (Unified) API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` or `%` as shell commands, and it interpolates Python variables with `$` or `{}` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oM1iC_MfAts1",
    "outputId": "49f06f06-12c3-4090-fe46-f91fe62a1378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: rthallam-demo-project\n"
     ]
    }
   ],
   "source": [
    "# Get your Google Cloud project ID from gcloud\n",
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "\n",
    "try:\n",
    "    PROJECT_ID = shell_output[0]\n",
    "except IndexError:\n",
    "    PROJECT_ID = None\n",
    "\n",
    "print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"rthallam-demo-project\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using AI Platform Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"AI Platform\"\n",
    "into the filter box, and select\n",
    "   **AI Platform Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# If on AI Platform, then don't execute this code\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\") and not os.getenv(\n",
    "        \"GOOGLE_APPLICATION_CREDENTIALS\"\n",
    "    ):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Configure project and resource names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "MODEL_ARTIFACT_DIR = \"custom-container-prediction-model\"  # @param {type:\"string\"}\n",
    "REPOSITORY = \"custom-container-prediction\"  # @param {type:\"string\"}\n",
    "IMAGE = \"keras-fastapi-server\"  # @param {type:\"string\"}\n",
    "MODEL_DISPLAY_NAME = \"keras-custom-container\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca1a915d641d"
   },
   "source": [
    "`REGION` - Used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Cloud\n",
    "AI Platform services are\n",
    "available](https://cloud.google.com/ai-platform-unified/docs/general/locations#feature-availability). You may\n",
    "not use a Multi-Regional Storage bucket for training with AI Platform.\n",
    "\n",
    "`MODEL_ARTIFACT_DIR` - Folder directory path to your model artifacts within a Cloud Storage bucket, for example: \"my-models/fraud-detection/trial-4\"\n",
    "\n",
    "`REPOSITORY` - Name of the Artifact Repository to create or use.\n",
    "\n",
    "`IMAGE` - Name of the container image that will be pushed.\n",
    "\n",
    "`MODEL_DISPLAY_NAME` - Display name of AI Platform (Unified) Model resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62f861b68b50"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "To update your model artifacts without re-building the container, you must upload your model\n",
    "artifacts and any custom code to Cloud Storage.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9724b00aeead"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://cloud-ai-platform-2f444b6a-a742-444b-b91a-c7519f51bd77\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58cb4f5895f0"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d2208676cee"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c664a5abc11a"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2c1b1c29f5f6"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c2d091d9e73"
   },
   "source": [
    "## Write your pre-processor\n",
    "Scaling training data so each numerical feature column has a mean of 0 and a standard deviation of 1 [can improve your model](https://developers.google.com/machine-learning/crash-course/representation/cleaning-data).\n",
    "\n",
    "Create `preprocess.py`, which contains a class to do this scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6e74556ea0b4"
   },
   "outputs": [],
   "source": [
    "%mkdir app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58d843d21fa8",
    "outputId": "47b72480-ea4e-430c-a0ea-17df31aed9a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/preprocess.py\n",
    "import numpy as np\n",
    "\n",
    "class MySimpleScaler(object):\n",
    "  def __init__(self):\n",
    "    self._means = None\n",
    "    self._stds = None\n",
    "\n",
    "  def preprocess(self, data):\n",
    "    if self._means is None: # during training only\n",
    "      self._means = np.mean(data, axis=0)\n",
    "\n",
    "    if self._stds is None: # during training only\n",
    "      self._stds = np.std(data, axis=0)\n",
    "      if not self._stds.all():\n",
    "        raise ValueError('At least one column has standard deviation of 0.')\n",
    "\n",
    "    return (data - self._means) / self._stds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLnSJWJgDaJN"
   },
   "source": [
    "Notice that an instance of `MySimpleScaler` saves the means and standard deviations of each feature column on first use. Then it uses these summary statistics to scale data it encounters afterward.\n",
    "\n",
    "This lets you store characteristics of the training distribution and use them for identical preprocessing at prediction time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b816cd52f4b"
   },
   "source": [
    "## Train and store model with pre-processor\n",
    "Next, use `preprocess.MySimpleScaler` to preprocess the iris data, then train a model using scikit-learn.\n",
    "\n",
    "At the end, export your trained model as a joblib (`.joblib`) file and export your `MySimpleScaler` instance as a pickle (`.pkl`) file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43e47249f736",
    "outputId": "1f1615c1-70de-4203-e36c-44c83f423735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/cloud-aiplatform-demos/ucaip-notebooks/predictions/app\n",
      "Epoch 1/10\n",
      "150/150 [==============================] - 1s 1ms/step - loss: 0.9426 - accuracy: 0.6294\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4453 - accuracy: 0.8753\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3130 - accuracy: 0.8956\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2627 - accuracy: 0.9481\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2059 - accuracy: 0.9550\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1712 - accuracy: 0.9428\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.9572\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9647\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.9792\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9921\n"
     ]
    }
   ],
   "source": [
    "%cd app/\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "import tensorflow as tf\n",
    "\n",
    "from preprocess import MySimpleScaler\n",
    "\n",
    "iris = load_iris()\n",
    "scaler = MySimpleScaler()\n",
    "num_classes = len(iris.target_names)\n",
    "X = scaler.preprocess(iris.data)\n",
    "y = tf.keras.utils.to_categorical(iris.target, num_classes=num_classes)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(25, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(25, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
    "model.compile(\n",
    "  optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=10, batch_size=1)\n",
    "\n",
    "model.save('model.h5')\n",
    "with open ('preprocessor.pkl', 'wb') as f:\n",
    "  pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTPk8zy2r7N6"
   },
   "source": [
    "**Note:** When deploying a TensorFlow model to AI Platform without a custom container, you must export the trained model in the `SavedModel` format. When you deploy a custom container to AI Platform Predictions, you are able to export to the HDF5 format instead—or any other format that suits your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3849066a33bd"
   },
   "source": [
    "### Upload model artifacts and custom code to Cloud Storage\n",
    "\n",
    "Before you can deploy your model for serving, AI Platform needs access to the following files in Cloud Storage:\n",
    "\n",
    "* `model.h5` (model artifact)\n",
    "* `preprocessor.pkl` (model artifact)\n",
    "\n",
    "Run the following commands to upload your files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca67ee52d4d9",
    "outputId": "025ae8c9-7664-4d0a-e5c5-93c64f52f384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model.h5 [Content-Type=application/x-hdf5]...\n",
      "Copying file://preprocessor.pkl [Content-Type=application/octet-stream]...      \n",
      "/ [2 files][ 42.1 KiB/ 42.1 KiB]                                                \n",
      "Operation completed over 2 objects/42.1 KiB.                                     \n",
      "/home/jupyter/cloud-aiplatform-demos/ucaip-notebooks/predictions\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp model.h5 preprocessor.pkl {BUCKET_NAME}/{MODEL_ARTIFACT_DIR}/\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "480a1d88ecdb"
   },
   "source": [
    "## Build a HTTP Server with FastAPI\n",
    "\n",
    "Custom Container image [requires](https://cloud.google.com/ai-platform-unified/docs/predictions/custom-container-requirements#image) that the container must run an HTTP server. Specifically, the container must listen and respond to liveness checks, health checks, and prediction requests.\n",
    "\n",
    "In this tutorial, we will use FastAPI to implement the HTTP server. The HTTP server must listen for requests on 0.0.0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94af0ba5eadd",
    "outputId": "d17a2445-2b51-4067-93ba-9b098752eba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/main.py\n",
    "from fastapi import FastAPI, Request\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from google.cloud import storage\n",
    "from preprocess import MySimpleScaler\n",
    "from sklearn.datasets import load_iris\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "gcs_client = storage.Client()\n",
    "\n",
    "with open(\"preprocessor.pkl\", 'wb') as preprocessor_f, open(\"model.h5\", 'wb') as model_f:\n",
    "    gcs_client.download_blob_to_file(\n",
    "        f\"{os.environ['AIP_STORAGE_URI']}/preprocessor.pkl\", preprocessor_f\n",
    "    )\n",
    "    gcs_client.download_blob_to_file(\n",
    "        f\"{os.environ['AIP_STORAGE_URI']}/model.h5\", model_f\n",
    "    )\n",
    "\n",
    "with open(\"preprocessor.pkl\", \"rb\") as f:\n",
    "    preprocessor = pickle.load(f)\n",
    "\n",
    "_class_names = load_iris().target_names\n",
    "_model = joblib.load(\"model.h5\")\n",
    "_preprocessor = preprocessor\n",
    "\n",
    "\n",
    "@app.get(os.environ['AIP_HEALTH_ROUTE'], status_code=200)\n",
    "def health():\n",
    "    \"\"\" health check to ensure HTTP server is ready to handle \n",
    "        prediction requests\n",
    "    \"\"\"\n",
    "    return {}\n",
    "\n",
    "\n",
    "@app.post(os.environ['AIP_PREDICT_ROUTE'])\n",
    "async def predict(request: Request):\n",
    "    body = await request.json()\n",
    "\n",
    "    instances = body[\"instances\"]\n",
    "    inputs = np.asarray(instances)\n",
    "    preprocessed_inputs = _preprocessor.preprocess(inputs)\n",
    "    outputs = _model.predict(preprocessed_inputs)\n",
    "\n",
    "    parameters = body[\"parameters\"]\n",
    "    if parameters.get('probabilities'):\n",
    "      return outputs.tolist()\n",
    "    else:\n",
    "      return {\"predictions\": [_class_names[class_num] for class_num \n",
    "                              in np.argmax(outputs, axis=1)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ai-platform-2f444b6a-a742-444b-b91a-c7519f51bd77/custom-container-prediction-model\n"
     ]
    }
   ],
   "source": [
    "AIP_STORAGE_URI=f\"{BUCKET_NAME}/{MODEL_ARTIFACT_DIR}\"\n",
    "print(AIP_STORAGE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/cloud-aiplatform-demos/ucaip-notebooks/predictions\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/ucaip-cc/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/envs/ucaip-cc/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/jupyter/.local/lib/python3.8/site-packages/uvicorn/__main__.py\", line 4, in <module>\n",
      "    uvicorn.main()\n",
      "  File \"/home/jupyter/.local/lib/python3.8/site-packages/click/core.py\", line 829, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.8/site-packages/click/core.py\", line 782, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/jupyter/.local/lib/python3.8/site-packages/click/core.py\", line 1066, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/jupyter/.local/lib/python3.8/site-packages/click/core.py\", line 610, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.8/site-packages/uvicorn/main.py\", line 362, in main\n",
      "    run(**kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.8/site-packages/uvicorn/main.py\", line 386, in run\n",
      "    server.run()\n",
      "  File \"/home/jupyter/.local/lib/python3.8/site-packages/uvicorn/server.py\", line 49, in run\n",
      "    loop.run_until_complete(self.serve(sockets=sockets))\n",
      "  File \"uvloop/loop.pyx\", line 1494, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"/home/jupyter/.local/lib/python3.8/site-packages/uvicorn/server.py\", line 56, in serve\n",
      "    config.load()\n",
      "  File \"/home/jupyter/.local/lib/python3.8/site-packages/uvicorn/config.py\", line 308, in load\n",
      "    self.loaded_app = import_from_string(self.app)\n",
      "  File \"/home/jupyter/.local/lib/python3.8/site-packages/uvicorn/importer.py\", line 23, in import_from_string\n",
      "    raise exc from None\n",
      "  File \"/home/jupyter/.local/lib/python3.8/site-packages/uvicorn/importer.py\", line 20, in import_from_string\n",
      "    module = importlib.import_module(module_str)\n",
      "  File \"/opt/conda/envs/ucaip-cc/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"./app/main.py\", line 10, in <module>\n",
      "    from preprocess import MySimpleScaler\n",
      "ModuleNotFoundError: No module named 'preprocess'\n"
     ]
    }
   ],
   "source": [
    "!cd app & pwd & python -m uvicorn app.main:app --host 0.0.0.0 --port 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUH8rSUe3DKj"
   },
   "source": [
    "Notice that, in addition to using the preprocessor that you defined during training, this predictor performs a postprocessing step that converts the neural network's softmax output (an array denoting the probability of each label being the correct one) into the label with the highest probability.\n",
    "\n",
    "However, if the predictor receives a `probabilities` keyword argument with the value `True`, it returns the probability array instead. The last part of this tutorial shows how to provide these additional parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "469f55daf250"
   },
   "source": [
    "### Add pre-start script\n",
    "FastAPI will execute this script before starting up the server. The `PORT` environment variable is set to equal `AIP_HTTP_PORT` in order to run FastAPI on same the port expected by AI Platform (Unified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69f438aca35b",
    "outputId": "3b6b15c7-de4d-4801-c855-9cff5f9bd44b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/prestart.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/prestart.sh\n",
    "#!/bin/bash\n",
    "export PORT=$AIP_HTTP_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b62ddf1def3"
   },
   "source": [
    "### Store test instances to use later\n",
    "To learn more about formatting input instances in JSON, [read the documentation.](https://cloud.google.com/ai-platform-unified/docs/predictions/online-predictions-custom-models#request-body-details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6605e9e6186",
    "outputId": "79be3c95-96ed-4b58-e074-8e820428fa6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing instances.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile instances.json\n",
    "{\n",
    "    \"instances\": [\n",
    "        [6.7, 3.1, 4.7, 1.5],\n",
    "        [4.6, 3.1, 1.5, 0.2]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51e149fdec1b"
   },
   "source": [
    "## Build and push container to Artifact Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bdb9a7768a5"
   },
   "source": [
    "### Build your container\n",
    "Optionally copy in your credentials to run the container locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fbb77f4f56c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: missing destination file operand after 'app/credentials.json'\n",
      "Try 'cp --help' for more information.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Copy in credentials to run locally, this step can be skipped for deployment\n",
    "%cp $GOOGLE_APPLICATION_CREDENTIALS app/credentials.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "240578ec9efe"
   },
   "source": [
    "Write the Dockerfile, using `tiangolo/uvicorn-gunicorn-fastapi` as a base image. This will automatically run FastAPI for you using Gunicorn and Uvicorn. Visit [the FastAPI docs to read more about deploying FastAPI with Docker](https://fastapi.tiangolo.com/deployment/docker/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d3a6b9ed22b",
    "outputId": "b06cec6a-773c-498c-f113-27fda17aac3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
    "\n",
    "COPY ./app /app\n",
    "COPY requirements.txt requirements.txt\n",
    "\n",
    "RUN pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR\u001b[0m:    Error loading ASGI app. Could not import module \"main\".\n"
     ]
    }
   ],
   "source": [
    "!python3 -m uvicorn main:app --host 0.0.0.0 --port 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04c988201499"
   },
   "source": [
    "Build the image and tag the Artifact Registry path that you will push to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1e7d639b9cc",
    "outputId": "96ba0665-fdbc-440d-8220-6e58674b6146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon    215kB\n",
      "Step 1/4 : FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
      " ---> e2f19ac0b4e3\n",
      "Step 2/4 : COPY ./app /app\n",
      " ---> Using cache\n",
      " ---> 8bea115f1b2f\n",
      "Step 3/4 : COPY requirements.txt requirements.txt\n",
      " ---> Using cache\n",
      " ---> 3bc0f24abcb9\n",
      "Step 4/4 : RUN pip install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> e99dacd25e33\n",
      "Successfully built e99dacd25e33\n",
      "Successfully tagged us-central1-docker.pkg.dev/rthallam-demo-project/custom-container-prediction/keras-fastapi-server:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build \\\n",
    "    --tag={REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE} \\\n",
    "    ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "147a555f6c93"
   },
   "source": [
    "### Run and test the container locally (optional)\n",
    "\n",
    "Run the container locally in detached mode and provide the environment variables that the container requires. These env vars will be provided to the container by AI Platform Prediction once deployed. Test the `/health` and `/predict` routes, then stop the running image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  google-cloud-sdk-cloud-build-local\n",
      "0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded.\n",
      "Need to get 2716 kB of archives.\n",
      "After this operation, 9768 kB of additional disk space will be used.\n",
      "Get:1 http://packages.cloud.google.com/apt cloud-sdk-buster/main amd64 google-cloud-sdk-cloud-build-local amd64 339.0.0-0 [2716 kB]\n",
      "Fetched 2716 kB in 1s (5048 kB/s)                           \n",
      "Selecting previously unselected package google-cloud-sdk-cloud-build-local.\n",
      "(Reading database ... 94082 files and directories currently installed.)\n",
      "Preparing to unpack .../google-cloud-sdk-cloud-build-local_339.0.0-0_amd64.deb ...\n",
      "Unpacking google-cloud-sdk-cloud-build-local (339.0.0-0) ...\n",
      "Setting up google-cloud-sdk-cloud-build-local (339.0.0-0) ...\n",
      "Processing triggers for google-cloud-sdk (338.0.0-0) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install google-cloud-sdk-cloud-build-local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/05/07 04:30:11 Specify a source\n",
      "Usage: cloud-build-local --config=cloudbuild.yaml [--substitutions=_FOO=bar] [--dryrun=true/false] [--push=true/false] [--bind-mount-source=true/false] source\n"
     ]
    }
   ],
   "source": [
    "!cloud-build-local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/05/07 04:30:52 There should be only one positional argument. Pass all the flags before the source.\n",
      "Usage: cloud-build-local --config=cloudbuild.yaml [--substitutions=_FOO=bar] [--dryrun=true/false] [--push=true/false] [--bind-mount-source=true/false] source\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Run Container Locally\n",
    "!cloud-build-local run -p 80:80 {REGION}-docker.pkg.dev/{PROJECT_ID}/{ARTIFACT_REPO_NAME}/{CONTAINER_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "62ed2d334d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local-iris\n",
      "13d602da9fcc4911b23ebc656c7d1031016cdc127309589d4d23c0fc5903d9f8\n",
      "CONTAINER ID   IMAGE                                                                                               COMMAND                  CREATED        STATUS                  PORTS                          NAMES\n",
      "13d602da9fcc   us-central1-docker.pkg.dev/rthallam-demo-project/custom-container-prediction/keras-fastapi-server   \"/bin/sh -c 'exec gu…\"   1 second ago   Up Less than a second   80/tcp, 0.0.0.0:80->8080/tcp   local-iris\n",
      "5e1e47066058   gcr.io/inverting-proxy/agent                                                                        \"/bin/sh -c '/opt/bi…\"   9 days ago     Up 9 days                                              proxy-agent\n",
      "curl: (56) Recv failure: Connection reset by peer\n",
      "curl: (52) Empty reply from server\n"
     ]
    }
   ],
   "source": [
    "!docker rm local-iris\n",
    "!docker run -d -p 80:8080 \\\n",
    "    --name=local-iris \\\n",
    "    -e AIP_HTTP_PORT=8080 \\\n",
    "    -e AIP_HEALTH_ROUTE=/health \\\n",
    "    -e AIP_PREDICT_ROUTE=/predict \\\n",
    "    -e AIP_STORAGE_URI={BUCKET_NAME}/{MODEL_ARTIFACT_DIR} \\\n",
    "    -e GOOGLE_APPLICATION_CREDENTIALS=credentials.json \\\n",
    "    {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\n",
    "!docker container ls\n",
    "!curl localhost/health\n",
    "!curl -X POST \\\n",
    "  -d @instances.json \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  localhost/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ce629eea32fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl: (52) Empty reply from server\n"
     ]
    }
   ],
   "source": [
    "!curl localhost/health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                          COMMAND                  CREATED      STATUS      PORTS     NAMES\n",
      "5e1e47066058   gcr.io/inverting-proxy/agent   \"/bin/sh -c '/opt/bi…\"   9 days ago   Up 9 days             proxy-agent\n"
     ]
    }
   ],
   "source": [
    "!docker container ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "56986f93438e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl: (7) Failed to connect to localhost port 80: Connection refused\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST \\\n",
    "  -d @instances.json \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  localhost/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a29fcbbe0188"
   },
   "outputs": [],
   "source": [
    "!docker stop local-iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "212b2935ea12"
   },
   "source": [
    "### Push the container to artifact registry\n",
    "\n",
    "Configure Docker to access Artifact Registry. Then push your container image to your Artifact Registry repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "09ffe2434e3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create request issued for: [custom-container-prediction]\n",
      "Waiting for operation [projects/rthallam-demo-project/locations/us-central1/ope\n",
      "rations/56590d90-1fd4-4848-8710-9d8865f7cee1] to complete...done.              \n",
      "Created repository [custom-container-prediction].\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta artifacts repositories create {REPOSITORY} \\\n",
    "    --repository-format=docker \\\n",
    "    --location=$REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "293437024749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "After update, the following will be written to your Docker config file\n",
      " located at [/home/jupyter/.docker/config.json]:\n",
      " {\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Do you want to continue (Y/n)?  ^C\n",
      "\n",
      "\n",
      "Command killed by keyboard interrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "1dd7448f4703"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/rthallam-demo-project/custom-container-prediction/keras-fastapi-server]\n",
      "\n",
      "\u001b[1B8398e771: Preparing \n",
      "\u001b[1B37dcc776: Preparing \n",
      "\u001b[1B8c47e087: Preparing \n",
      "\u001b[1B2331eddf: Preparing \n",
      "\u001b[1Bdd42a306: Preparing \n",
      "\u001b[1B5e087746: Preparing \n",
      "\u001b[1Ba89f95f7: Preparing \n",
      "\u001b[1B960321f5: Preparing \n",
      "\u001b[1Bc8cc20a5: Preparing \n",
      "\u001b[1B85a516c9: Preparing \n",
      "\u001b[1Bab020550: Preparing \n",
      "\u001b[1B5ea49213: Preparing \n",
      "\u001b[1B28316107: Preparing \n",
      "\u001b[1B0ec29f78: Preparing \n",
      "\u001b[1Bcecc2826: Preparing \n",
      "\u001b[1B81fca4b7: Preparing \n",
      "\u001b[1B92e98337: Preparing \n",
      "\u001b[1B306e673e: Preparing \n",
      "\u001b[1Ba3b3ed45: Preparing \n",
      "\u001b[1Ba51ade6a: Preparing \n",
      "\u001b[12B5a516c9: Waiting g denied: Permission \"artifactregistry.repositories.downloadArtifacts\" denied on resource \"projects/rthallam-demo-project/locations/us-central1/repositories/custom-container-prediction\" (or it may not exist)\n"
     ]
    }
   ],
   "source": [
    "!docker push {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 13 file(s) totalling 214.7 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://rthallam-demo-project_cloudbuild/source/1620362098.570874-99b13165e2fe4af0b89e495f6f6a6a39.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/rthallam-demo-project/locations/global/builds/23cee9bd-6ec2-457c-b5ee-599925f3ea1b].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/23cee9bd-6ec2-457c-b5ee-599925f3ea1b?project=560224572293].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"23cee9bd-6ec2-457c-b5ee-599925f3ea1b\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://rthallam-demo-project_cloudbuild/source/1620362098.570874-99b13165e2fe4af0b89e495f6f6a6a39.tgz#1620362098865662\n",
      "Copying gs://rthallam-demo-project_cloudbuild/source/1620362098.570874-99b13165e2fe4af0b89e495f6f6a6a39.tgz#1620362098865662...\n",
      "/ [1 files][ 54.8 KiB/ 54.8 KiB]                                                \n",
      "Operation completed over 1 objects/54.8 KiB.                                     \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  232.4kB\n",
      "Step 1/4 : FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
      "python3.7: Pulling from tiangolo/uvicorn-gunicorn-fastapi\n",
      "90fe46dd8199: Pulling fs layer\n",
      "35a4f1977689: Pulling fs layer\n",
      "bbc37f14aded: Pulling fs layer\n",
      "74e27dc593d4: Pulling fs layer\n",
      "4352dcff7819: Pulling fs layer\n",
      "deb569b08de6: Pulling fs layer\n",
      "1aa33568be24: Pulling fs layer\n",
      "a8034acd5893: Pulling fs layer\n",
      "7dfa9b763153: Pulling fs layer\n",
      "a035e7b052a3: Pulling fs layer\n",
      "d4f2cd280496: Pulling fs layer\n",
      "e227f25dc86f: Pulling fs layer\n",
      "97a4044094b0: Pulling fs layer\n",
      "5d40afec4e92: Pulling fs layer\n",
      "f6b5e3636b46: Pulling fs layer\n",
      "85a8adaae1ef: Pulling fs layer\n",
      "3bd649e0d635: Pulling fs layer\n",
      "7855c9b64055: Pulling fs layer\n",
      "4352dcff7819: Waiting\n",
      "deb569b08de6: Waiting\n",
      "1aa33568be24: Waiting\n",
      "a8034acd5893: Waiting\n",
      "7dfa9b763153: Waiting\n",
      "a035e7b052a3: Waiting\n",
      "d4f2cd280496: Waiting\n",
      "e227f25dc86f: Waiting\n",
      "97a4044094b0: Waiting\n",
      "5d40afec4e92: Waiting\n",
      "f6b5e3636b46: Waiting\n",
      "85a8adaae1ef: Waiting\n",
      "3bd649e0d635: Waiting\n",
      "7855c9b64055: Waiting\n",
      "74e27dc593d4: Waiting\n",
      "35a4f1977689: Verifying Checksum\n",
      "35a4f1977689: Download complete\n",
      "bbc37f14aded: Verifying Checksum\n",
      "bbc37f14aded: Download complete\n",
      "90fe46dd8199: Verifying Checksum\n",
      "90fe46dd8199: Download complete\n",
      "74e27dc593d4: Verifying Checksum\n",
      "74e27dc593d4: Download complete\n",
      "deb569b08de6: Verifying Checksum\n",
      "deb569b08de6: Download complete\n",
      "a8034acd5893: Verifying Checksum\n",
      "a8034acd5893: Download complete\n",
      "7dfa9b763153: Verifying Checksum\n",
      "7dfa9b763153: Download complete\n",
      "1aa33568be24: Verifying Checksum\n",
      "1aa33568be24: Download complete\n",
      "d4f2cd280496: Verifying Checksum\n",
      "d4f2cd280496: Download complete\n",
      "a035e7b052a3: Verifying Checksum\n",
      "a035e7b052a3: Download complete\n",
      "e227f25dc86f: Verifying Checksum\n",
      "e227f25dc86f: Download complete\n",
      "97a4044094b0: Verifying Checksum\n",
      "97a4044094b0: Download complete\n",
      "5d40afec4e92: Verifying Checksum\n",
      "5d40afec4e92: Download complete\n",
      "f6b5e3636b46: Verifying Checksum\n",
      "f6b5e3636b46: Download complete\n",
      "85a8adaae1ef: Verifying Checksum\n",
      "85a8adaae1ef: Download complete\n",
      "3bd649e0d635: Verifying Checksum\n",
      "3bd649e0d635: Download complete\n",
      "7855c9b64055: Verifying Checksum\n",
      "7855c9b64055: Download complete\n",
      "4352dcff7819: Verifying Checksum\n",
      "4352dcff7819: Download complete\n",
      "90fe46dd8199: Pull complete\n",
      "35a4f1977689: Pull complete\n",
      "bbc37f14aded: Pull complete\n",
      "74e27dc593d4: Pull complete\n",
      "4352dcff7819: Pull complete\n",
      "deb569b08de6: Pull complete\n",
      "1aa33568be24: Pull complete\n",
      "a8034acd5893: Pull complete\n",
      "7dfa9b763153: Pull complete\n",
      "a035e7b052a3: Pull complete\n",
      "d4f2cd280496: Pull complete\n",
      "e227f25dc86f: Pull complete\n",
      "97a4044094b0: Pull complete\n",
      "5d40afec4e92: Pull complete\n",
      "f6b5e3636b46: Pull complete\n",
      "85a8adaae1ef: Pull complete\n",
      "3bd649e0d635: Pull complete\n",
      "7855c9b64055: Pull complete\n",
      "Digest: sha256:a0e0188a485fd8c232d8774ae4680d3b834f95dd2deccdb0211ce71cfd778b97\n",
      "Status: Downloaded newer image for tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
      " ---> e2f19ac0b4e3\n",
      "Step 2/4 : COPY ./app /app\n",
      " ---> c044b29dff01\n",
      "Step 3/4 : COPY requirements.txt requirements.txt\n",
      " ---> dd8aa1504599\n",
      "Step 4/4 : RUN pip install -r requirements.txt\n",
      " ---> Running in 76eead5ed17e\n",
      "Collecting joblib~=1.0\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Collecting numpy~=1.20\n",
      "  Downloading numpy-1.20.2-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
      "Collecting scikit-learn~=0.24\n",
      "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "Collecting tensorflow>=1.15\n",
      "  Downloading tensorflow-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3 MB)\n",
      "Collecting google-cloud-storage<2.0.0dev,>=1.26.0\n",
      "  Downloading google_cloud_storage-1.38.0-py2.py3-none-any.whl (103 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting wheel~=0.35\n",
      "  Downloading wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.16.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting h5py~=2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting google-auth<2.0dev,>=1.11.0\n",
      "  Downloading google_auth-1.30.0-py2.py3-none-any.whl (146 kB)\n",
      "Collecting google-cloud-core<2.0dev,>=1.4.1\n",
      "  Downloading google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB)\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Collecting google-resumable-media<2.0dev,>=1.2.0\n",
      "  Downloading google_resumable_media-1.2.0-py2.py3-none-any.whl (75 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=1.15->-r requirements.txt (line 4)) (46.1.3)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting google-api-core<2.0.0dev,>=1.21.0\n",
      "  Downloading google_api_core-1.26.3-py2.py3-none-any.whl (93 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
      "Collecting chardet<5,>=3.0.2\n",
      "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"\n",
      "  Downloading google_crc32c-1.1.2-cp37-cp37m-manylinux2014_x86_64.whl (38 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-4.0.1-py3-none-any.whl (16 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting packaging>=14.3\n",
      "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
      "Collecting pytz\n",
      "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "Collecting cffi>=1.0.0\n",
      "  Downloading cffi-1.14.5-cp37-cp37m-manylinux1_x86_64.whl (402 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyparsing>=2.0.2\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "Building wheels for collected packages: wrapt, termcolor\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=76412 sha256=c2b848fe972c178e420f684af2168a6ab1dec6bc2b18d3bf1f27fe4b3729932f\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=f87299d939a48d1e389ec6dda5583b4fc6aa716305da5f9825ae34f22636f571\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built wrapt termcolor\n",
      "\u001b[91mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.20.2 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: joblib, numpy, scipy, threadpoolctl, scikit-learn, opt-einsum, six, google-pasta, wrapt, wheel, absl-py, protobuf, gast, termcolor, tensorflow-estimator, typing-extensions, tensorboard-data-server, zipp, importlib-metadata, markdown, urllib3, idna, certifi, chardet, requests, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, tensorboard-plugin-wit, werkzeug, tensorboard, keras-preprocessing, flatbuffers, h5py, astunparse, tensorflow, pyparsing, packaging, pytz, googleapis-common-protos, google-api-core, google-cloud-core, pycparser, cffi, google-crc32c, google-resumable-media, google-cloud-storage\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.34.2\n",
      "    Uninstalling wheel-0.34.2:\n",
      "      Successfully uninstalled wheel-0.34.2\n",
      "Successfully installed absl-py-0.12.0 astunparse-1.6.3 cachetools-4.2.2 certifi-2020.12.5 cffi-1.14.5 chardet-4.0.0 flatbuffers-1.12 gast-0.3.3 google-api-core-1.26.3 google-auth-1.30.0 google-auth-oauthlib-0.4.4 google-cloud-core-1.6.0 google-cloud-storage-1.38.0 google-crc32c-1.1.2 google-pasta-0.2.0 google-resumable-media-1.2.0 googleapis-common-protos-1.53.0 grpcio-1.32.0 h5py-2.10.0 idna-2.10 importlib-metadata-4.0.1 joblib-1.0.1 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.20.2 oauthlib-3.1.0 opt-einsum-3.3.0 packaging-20.9 protobuf-3.16.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 pytz-2021.1 requests-2.25.1 requests-oauthlib-1.3.0 rsa-4.7.2 scikit-learn-0.24.2 scipy-1.6.3 six-1.15.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 termcolor-1.1.0 threadpoolctl-2.1.0 typing-extensions-3.7.4.3 urllib3-1.26.4 werkzeug-1.0.1 wheel-0.36.2 wrapt-1.12.1 zipp-3.4.1\n",
      "\u001b[91mWARNING: You are using pip version 20.0.2; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 76eead5ed17e\n",
      " ---> 3b8eba79fb30\n",
      "Successfully built 3b8eba79fb30\n",
      "Successfully tagged us-central1-docker.pkg.dev/rthallam-demo-project/custom-container-prediction/keras-fastapi-server:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/rthallam-demo-project/custom-container-prediction/keras-fastapi-server\n",
      "The push refers to repository [us-central1-docker.pkg.dev/rthallam-demo-project/custom-container-prediction/keras-fastapi-server]\n",
      "201d1ce1f2bb: Preparing\n",
      "b97b6b66015c: Preparing\n",
      "4f71f647bb1f: Preparing\n",
      "7f552331eddf: Preparing\n",
      "020fdd42a306: Preparing\n",
      "195d5e087746: Preparing\n",
      "9d89a89f95f7: Preparing\n",
      "b74e960321f5: Preparing\n",
      "a64fc8cc20a5: Preparing\n",
      "eac685a516c9: Preparing\n",
      "9018ab020550: Preparing\n",
      "2c065ea49213: Preparing\n",
      "956d28316107: Preparing\n",
      "86120ec29f78: Preparing\n",
      "5d34cecc2826: Preparing\n",
      "baf481fca4b7: Preparing\n",
      "3d3e92e98337: Preparing\n",
      "8967306e673e: Preparing\n",
      "9794a3b3ed45: Preparing\n",
      "5f77a51ade6a: Preparing\n",
      "e40d297cf5f8: Preparing\n",
      "956d28316107: Waiting\n",
      "195d5e087746: Waiting\n",
      "86120ec29f78: Waiting\n",
      "5d34cecc2826: Waiting\n",
      "baf481fca4b7: Waiting\n",
      "9d89a89f95f7: Waiting\n",
      "b74e960321f5: Waiting\n",
      "a64fc8cc20a5: Waiting\n",
      "eac685a516c9: Waiting\n",
      "9018ab020550: Waiting\n",
      "2c065ea49213: Waiting\n",
      "8967306e673e: Waiting\n",
      "9794a3b3ed45: Waiting\n",
      "5f77a51ade6a: Waiting\n",
      "e40d297cf5f8: Waiting\n",
      "4f71f647bb1f: Pushed\n",
      "7f552331eddf: Pushed\n",
      "b97b6b66015c: Pushed\n",
      "020fdd42a306: Pushed\n",
      "195d5e087746: Pushed\n",
      "b74e960321f5: Pushed\n",
      "9d89a89f95f7: Pushed\n",
      "a64fc8cc20a5: Pushed\n",
      "9018ab020550: Pushed\n",
      "eac685a516c9: Pushed\n",
      "2c065ea49213: Pushed\n",
      "956d28316107: Pushed\n",
      "86120ec29f78: Pushed\n",
      "5d34cecc2826: Pushed\n",
      "baf481fca4b7: Pushed\n",
      "9794a3b3ed45: Pushed\n",
      "5f77a51ade6a: Pushed\n",
      "8967306e673e: Pushed\n",
      "e40d297cf5f8: Pushed\n",
      "3d3e92e98337: Pushed\n",
      "201d1ce1f2bb: Pushed\n",
      "latest: digest: sha256:b4af8ad66b3fa133d5d05fc01ae43fbaacb44bcb3834bb143f6481598bbb099a size: 4718\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                               IMAGES                                                                                                       STATUS\n",
      "23cee9bd-6ec2-457c-b5ee-599925f3ea1b  2021-05-07T04:34:59+00:00  5M57S     gs://rthallam-demo-project_cloudbuild/source/1620362098.570874-99b13165e2fe4af0b89e495f6f6a6a39.tgz  us-central1-docker.pkg.dev/rthallam-demo-project/custom-container-prediction/keras-fastapi-server (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit \\\n",
    "    --tag={REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE} \\\n",
    "    ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b438bfa2129f"
   },
   "source": [
    "## Deploy to AI Platform (Unified)\n",
    "\n",
    "Use the [Python SDK for Cloud AI Platform](https://googleapis.dev/python/aiplatform/latest/index.html) to upload and deploy your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ae19df6a33e"
   },
   "source": [
    "### Upload the custom container model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "8d682d8388ec"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "574fb82d3eed"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "2738154345d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/560224572293/locations/us-central1/models/352811291120762880/operations/264971856933552128\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/560224572293/locations/us-central1/models/352811291120762880\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/560224572293/locations/us-central1/models/352811291120762880')\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=f\"{BUCKET_NAME}/{MODEL_ARTIFACT_DIR}\",\n",
    "    serving_container_image_uri=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd1b85afc7df"
   },
   "source": [
    "### Deploy the model on AI Platform (Unified)\n",
    "After this step completes, the model is deployed and ready for online prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "62cf66498a28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/560224572293/locations/us-central1/endpoints/8235623567018950656/operations/3935405553240506368\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/560224572293/locations/us-central1/endpoints/8235623567018950656\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/560224572293/locations/us-central1/endpoints/8235623567018950656')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/560224572293/locations/us-central1/endpoints/8235623567018950656\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/560224572293/locations/us-central1/endpoints/8235623567018950656/operations/7783168484875173888\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-1ee6182b5014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mendpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmachine_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"n1-standard-4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, explanation_metadata, explanation_parameters, metadata, encryption_spec_key_name, sync)\u001b[0m\n\u001b[1;32m   1592\u001b[0m         )\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1594\u001b[0;31m         return self._deploy(\n\u001b[0m\u001b[1;32m   1595\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m             \u001b[0mdeployed_model_display_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeployed_model_display_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, explanation_metadata, explanation_parameters, metadata, encryption_spec_key_name, sync)\u001b[0m\n\u001b[1;32m   1720\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_start_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Deploying model to\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1722\u001b[0;31m         Endpoint._deploy_call(\n\u001b[0m\u001b[1;32m   1723\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy_call\u001b[0;34m(cls, api_client, endpoint_resource_name, model_resource_name, endpoint_resource_traffic_split, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, explanation_metadata, explanation_parameters, metadata)\u001b[0m\n\u001b[1;32m    854\u001b[0m         )\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         \u001b[0moperation_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     def undeploy(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \"\"\"\n\u001b[1;32m    128\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mDEFAULT_RETRY\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"retry\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36m_blocking_poll\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mDEFAULT_RETRY\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"retry\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mretry_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_or_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             raise concurrent.futures.TimeoutError(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             )\n\u001b[0;32m--> 281\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    282\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;34m\"Retrying due to {}, sleeping {:.1f}s ...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_exc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sleep generator stopped yielding sleep values.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6883e7b07143"
   },
   "source": [
    "## Send predictions\n",
    "\n",
    "### Using Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d69ed411c2d3"
   },
   "outputs": [],
   "source": [
    "endpoint.predict(instances=[[6.7, 3.1, 4.7, 1.5], [4.6, 3.1, 1.5, 0.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arqz41gK7rWx"
   },
   "source": [
    "#### Sending prediction instances with additional parameters\n",
    "When you send a prediction request to a custom container, you can provide additional fields on your request body. The container's predict method receives these as fields of the `parameters` dictionary.\n",
    "\n",
    "The following code sends the same request as before, but this time it adds a probabilities field as an additional parameter to the request body:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FtZQp9t7r5V"
   },
   "outputs": [],
   "source": [
    "endpoint.predict(instances=[[6.7, 3.1, 4.7, 1.5], [4.6, 3.1, 1.5, 0.2]], \n",
    "                 parameters={'probabilities': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "370d22f53427"
   },
   "source": [
    "### Using REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba55bc560d58"
   },
   "outputs": [],
   "source": [
    "ENDPOINT_ID = endpoint.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95c562b4e98b"
   },
   "outputs": [],
   "source": [
    "! curl \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d @instances.json \\\n",
    "https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa71174a7dd0"
   },
   "source": [
    "### Using gcloud CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23b8e807b02c"
   },
   "outputs": [],
   "source": [
    "!gcloud beta ai endpoints predict $ENDPOINT_ID \\\n",
    "  --region=$REGION \\\n",
    "  --json-request=instances.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Undeploy model and delete endpoint\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete the model resource\n",
    "model.delete()\n",
    "\n",
    "# Delete the container image from Artifact Registry\n",
    "!gcloud artifacts docker images delete \\\n",
    "    --quiet \\\n",
    "    --delete-tags \\\n",
    "    {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI_Platform_(Unified)_SDK_Custom_Container_Prediction_Keras.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:ucaip-cc]",
   "language": "python",
   "name": "conda-env-ucaip-cc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
