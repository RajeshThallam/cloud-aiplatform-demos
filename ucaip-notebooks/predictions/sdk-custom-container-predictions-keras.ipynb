{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/unofficial/AI_Platform_(Unified)_SDK_Custom_Container_Prediction_Keras.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial shows how to deploy a trained Keras model to Vertex AI and serve predictions on Vertex Predictions using a [custom container](https://cloud.google.com/vertex-ai/docs/predictions/use-custom-container). This lets you customize how Vertex AI responds to each prediction request. To demonstrate the customization, this tutorial covers incorporating a pre-processor from the training application code into online serving. You will use the [FastAPI](https://fastapi.tiangolo.com/) Python web server framework to create a prediction and health endpoint. \n",
    "\n",
    "In this example, you will use a custom container with a preprocessing step that scales prediction input, and a postprocess step to convert prediction output from [softmax](https://developers.google.com/machine-learning/glossary/#s) probability outputs to label strings.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "This tutorial uses R.A. Fisher's Iris dataset, a small dataset that is popular for trying out machine learning techniques. Each instance has four numerical features, which are different measurements of a flower, and a target label that\n",
    "marks it as one of three types of iris: Iris setosa, Iris versicolour, or Iris virginica.\n",
    "\n",
    "This tutorial uses [the copy of the Iris dataset included in the\n",
    "scikit-learn library](https://scikit-learn.org/stable/datasets/index.html#iris-dataset).\n",
    "\n",
    "### Objective\n",
    "\n",
    "The goal is to train a model that uses a flower's measurements as input to predict what type of iris it is.\n",
    "\n",
    "The tutorial walks through several steps:\n",
    "\n",
    "- Training a simple Keras model locally (in this notebook)\n",
    "- Save the model and its serialized pre-processor and post-processor\n",
    "- Build a FastAPI server to handle predictions and health checks\n",
    "- Build a custom container (Docker) with model artifacts\n",
    "- Upload and deploy custom container to Vertex Predictions (Endpoints)\n",
    "- Serve prediction requests from that deployment\n",
    "\n",
    "This tutorial focuses more on deploying this model on Vertex AI than on the design of the model itself.\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* Docker\n",
    "* Git\n",
    "* Google Cloud SDK (gcloud)\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions for meeting these requirements. The following steps provide a condensed set of instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv) and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip install jupyter` on the command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install additional packages\n",
    "\n",
    "Install additional package dependencies not installed in your notebook environment, such as Tensorflow, NumPy, Scikit-learn, FastAPI, Uvicorn, and joblib. Use the latest major GA version of each package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "747f59abb3a5",
    "outputId": "78c4a18b-a9e0-4046-cdb9-e31ab8ce8eed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "joblib~=1.0\n",
    "numpy~=1.19.2\n",
    "scikit-learn~=0.24\n",
    "tensorflow==2.4.1\n",
    "google-cloud-storage>=1.26.0,<2.0.0dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyy5Lbnzg5fi"
   },
   "outputs": [],
   "source": [
    "# Required in Docker serving container\n",
    "%pip install -U --user -r requirements.txt\n",
    "\n",
    "# For local FastAPI development and running\n",
    "%pip install -U --user \"uvicorn[standard]>=0.12.0,<0.14.0\" fastapi~=0.63\n",
    "\n",
    "# AI Platform (Unified) SDK\n",
    "%pip install -U --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` or `%` as shell commands, and it interpolates Python variables with `$` or `{}` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oM1iC_MfAts1",
    "outputId": "49f06f06-12c3-4090-fe46-f91fe62a1378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: rthallam-demo-project\n"
     ]
    }
   ],
   "source": [
    "# Get your Google Cloud project ID from gcloud\n",
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "\n",
    "try:\n",
    "    PROJECT_ID = shell_output[0]\n",
    "except IndexError:\n",
    "    PROJECT_ID = None\n",
    "\n",
    "print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"<YOUR-PROJECT-ID>\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Google Cloud Notebooks**, your environment is already authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"AI Platform\"\n",
    "into the filter box, and select\n",
    "   **AI Platform Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# If on Vertex AI, then don't execute this code\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\") and not os.getenv(\n",
    "        \"GOOGLE_APPLICATION_CREDENTIALS\"\n",
    "    ):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Configure project and resource names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "MODEL_ARTIFACT_DIR = \"custom-container-prediction-model\"  # @param {type:\"string\"}\n",
    "REPOSITORY = \"custom-container-prediction\"  # @param {type:\"string\"}\n",
    "IMAGE = \"keras-fastapi-server\"  # @param {type:\"string\"}\n",
    "MODEL_DISPLAY_NAME = \"keras-custom-container\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca1a915d641d"
   },
   "source": [
    "`REGION` - Used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Cloud\n",
    "AI Platform services are\n",
    "available](https://cloud.google.com/ai-platform-unified/docs/general/locations#feature-availability). You may\n",
    "not use a Multi-Regional Storage bucket for training with AI Platform.\n",
    "\n",
    "`MODEL_ARTIFACT_DIR` - Folder directory path to your model artifacts within a Cloud Storage bucket, for example: \"my-models/fraud-detection/trial-4\"\n",
    "\n",
    "`REPOSITORY` - Name of the Artifact Repository to create or use.\n",
    "\n",
    "`IMAGE` - Name of the container image that will be pushed.\n",
    "\n",
    "`MODEL_DISPLAY_NAME` - Display name of AI Platform (Unified) Model resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62f861b68b50"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "To update your model artifacts without re-building the container, you must upload your model\n",
    "artifacts and any custom code to Cloud Storage.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://<YOUR-BUCKET-NAME\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9724b00aeead"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://cloud-ai-platform-2f444b6a-a742-444b-b91a-c7519f51bd77\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58cb4f5895f0"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d2208676cee"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c664a5abc11a"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2c1b1c29f5f6"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom extension to write script files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "from IPython.core import magic_arguments\n",
    "import os\n",
    "import re\n",
    "\n",
    "@magic_arguments.magic_arguments()\n",
    "@magic_arguments.argument(\n",
    "    '-a', '--append', action='store_true', default=False,\n",
    "    help='Append contents of the cell to an existing file. '\n",
    "         'The file will be created if it does not exist.'\n",
    ")\n",
    "@magic_arguments.argument(\n",
    "    'filename', type=str,\n",
    "    help='file to write'\n",
    ")\n",
    "@register_line_cell_magic\n",
    "def writefilecustom(line, cell):\n",
    "    \"\"\"Write the contents of the cell to a file.\n",
    "        \n",
    "    The file will be overwritten unless the -a (--append) flag is specified.\n",
    "    \"\"\"\n",
    "    args = magic_arguments.parse_argstring(writefilecustom, line)\n",
    "    if re.match(r'^(\\'.*\\')|(\".*\")$', args.filename):\n",
    "        filename = os.path.expanduser(args.filename[1:-1])\n",
    "    else:\n",
    "        filename = os.path.expanduser(args.filename)\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        if args.append:\n",
    "            print(\"Appending to %s\" % filename)\n",
    "        else:\n",
    "            print(\"Overwriting %s\" % filename)\n",
    "    else:\n",
    "        print(\"Writing %s\" % filename)\n",
    "\n",
    "    mode = 'a' if args.append else 'w'\n",
    "    with open(filename, mode, encoding='utf-8') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c2d091d9e73"
   },
   "source": [
    "## Write your pre-processor\n",
    "Scaling training data so each numerical feature column has a mean of 0 and a standard deviation of 1 [can improve your model](https://developers.google.com/machine-learning/crash-course/representation/cleaning-data).\n",
    "\n",
    "Create `preprocess.py`, which contains a class to do this scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6e74556ea0b4"
   },
   "outputs": [],
   "source": [
    "%mkdir -p app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58d843d21fa8",
    "outputId": "47b72480-ea4e-430c-a0ea-17df31aed9a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/preprocess.py\n",
    "import numpy as np\n",
    "\n",
    "class MySimpleScaler(object):\n",
    "    def __init__(self):\n",
    "        self._means = None\n",
    "        self._stds = None\n",
    "        \n",
    "    def preprocess(self, data):\n",
    "        if self._means is None: # during training only\n",
    "            self._means = np.mean(data, axis=0)\n",
    "\n",
    "        if self._stds is None: # during training only\n",
    "            self._stds = np.std(data, axis=0)\n",
    "            if not self._stds.all():\n",
    "                raise ValueError('At least one column has standard deviation of 0.')\n",
    "\n",
    "        return (data - self._means) / self._stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLnSJWJgDaJN"
   },
   "source": [
    "Notice that an instance of `MySimpleScaler` saves the means and standard deviations of each feature column on first use. Then it uses these summary statistics to scale data it encounters afterward.\n",
    "\n",
    "This lets you store characteristics of the training distribution and use them for identical preprocessing at prediction time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b816cd52f4b"
   },
   "source": [
    "## Train and store model with pre-processor\n",
    "Next, use `preprocess.MySimpleScaler` to preprocess the iris data, then train a model using scikit-learn.\n",
    "\n",
    "At the end, export your trained model as a joblib (`.joblib`) file and export your `MySimpleScaler` instance as a pickle (`.pkl`) file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43e47249f736",
    "outputId": "1f1615c1-70de-4203-e36c-44c83f423735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/cloud-aiplatform-demos/ucaip-notebooks/predictions/app\n",
      "Epoch 1/10\n",
      "150/150 [==============================] - 1s 1ms/step - loss: 0.8170 - accuracy: 0.7202\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4950 - accuracy: 0.7903\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3140 - accuracy: 0.8232\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2556 - accuracy: 0.8928\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2029 - accuracy: 0.9515\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9594\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9674\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9527\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9729\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9631\n"
     ]
    }
   ],
   "source": [
    "%cd app/\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "import tensorflow as tf\n",
    "\n",
    "from preprocess import MySimpleScaler\n",
    "\n",
    "iris = load_iris()\n",
    "scaler = MySimpleScaler()\n",
    "num_classes = len(iris.target_names)\n",
    "X = scaler.preprocess(iris.data)\n",
    "y = tf.keras.utils.to_categorical(iris.target, num_classes=num_classes)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(25, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(25, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
    "model.compile(\n",
    "  optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=10, batch_size=1)\n",
    "\n",
    "model.save('model.h5')\n",
    "\n",
    "with open ('preprocessor.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTPk8zy2r7N6"
   },
   "source": [
    "**Note:** When deploying a TensorFlow model to AI Platform without a custom container, you must export the trained model in the `SavedModel` format. When you deploy a custom container to AI Platform Predictions, you are able to export to the HDF5 format instead—or any other format that suits your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3849066a33bd"
   },
   "source": [
    "### Upload model artifacts and custom code to Cloud Storage\n",
    "\n",
    "Before you can deploy your model for serving, AI Platform needs access to the following files in Cloud Storage:\n",
    "\n",
    "* `model.h5` (model artifact)\n",
    "* `preprocessor.pkl` (model artifact)\n",
    "\n",
    "Run the following commands to upload your files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca67ee52d4d9",
    "outputId": "025ae8c9-7664-4d0a-e5c5-93c64f52f384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model.h5 [Content-Type=application/x-hdf5]...\n",
      "Copying file://preprocessor.pkl [Content-Type=application/octet-stream]...      \n",
      "/ [2 files][ 42.1 KiB/ 42.1 KiB]                                                \n",
      "Operation completed over 2 objects/42.1 KiB.                                     \n",
      "/home/jupyter/cloud-aiplatform-demos/ucaip-notebooks/predictions\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp model.h5 preprocessor.pkl {BUCKET_NAME}/{MODEL_ARTIFACT_DIR}/\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "480a1d88ecdb"
   },
   "source": [
    "## Build a HTTP Server with FastAPI\n",
    "\n",
    "Custom Container image [requires](https://cloud.google.com/ai-platform-unified/docs/predictions/custom-container-requirements#image) that the container must run an HTTP server. Specifically, the container must listen and respond to liveness checks, health checks, and prediction requests.\n",
    "\n",
    "In this tutorial, we will use FastAPI to implement the HTTP server. The HTTP server must listen for requests on 0.0.0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94af0ba5eadd",
    "outputId": "d17a2445-2b51-4067-93ba-9b098752eba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/main.py\n",
    "from fastapi import FastAPI, Request\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from google.cloud import storage\n",
    "from preprocess import MySimpleScaler\n",
    "from sklearn.datasets import load_iris\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "gcs_client = storage.Client()\n",
    "\n",
    "with open(\"preprocessor.pkl\", 'wb') as preprocessor_f, open(\"model.h5\", 'wb') as model_f:\n",
    "    gcs_client.download_blob_to_file(\n",
    "        f\"{os.environ['AIP_STORAGE_URI']}/preprocessor.pkl\", preprocessor_f\n",
    "    )\n",
    "    gcs_client.download_blob_to_file(\n",
    "        f\"{os.environ['AIP_STORAGE_URI']}/model.h5\", model_f\n",
    "    )\n",
    "\n",
    "with open(\"preprocessor.pkl\", \"rb\") as f:\n",
    "    preprocessor = pickle.load(f)\n",
    "\n",
    "_class_names = load_iris().target_names\n",
    "_model = tf.keras.models.load_model(\"model.h5\")\n",
    "_preprocessor = preprocessor\n",
    "\n",
    "\n",
    "@app.get(os.environ['AIP_HEALTH_ROUTE'], status_code=200)\n",
    "def health():\n",
    "    \"\"\" health check to ensure HTTP server is ready to handle \n",
    "        prediction requests\n",
    "    \"\"\"\n",
    "    return {\"Status\": \"Running\"}\n",
    "\n",
    "\n",
    "@app.post(os.environ['AIP_PREDICT_ROUTE'])\n",
    "async def predict(request: Request):\n",
    "    body = await request.json()\n",
    "    print(body)\n",
    "\n",
    "    instances = body[\"instances\"]\n",
    "    inputs = np.asarray(instances)\n",
    "    preprocessed_inputs = _preprocessor.preprocess(inputs)\n",
    "    outputs = _model.predict(preprocessed_inputs)\n",
    "    print(outputs)\n",
    "\n",
    "    parameters = body.get(\"parameters\", {})\n",
    "    if parameters.get('probabilities'):\n",
    "      return {\"predictions\": outputs.tolist()}\n",
    "    else:\n",
    "      return {\"predictions\": [_class_names[class_num] for class_num \n",
    "                              in np.argmax(outputs, axis=1)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUH8rSUe3DKj"
   },
   "source": [
    "Notice that, in addition to using the preprocessor that you defined during training, this predictor performs a postprocessing step that converts the neural network's softmax output (an array denoting the probability of each label being the correct one) into the label with the highest probability.\n",
    "\n",
    "However, if the predictor receives a `probabilities` keyword argument with the value `True`, it returns the probability array instead. The last part of this tutorial shows how to provide these additional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-ai-platform-2f444b6a-a742-444b-b91a-c7519f51bd77/custom-container-prediction-model\n",
      "gs://cloud-ai-platform-2f444b6a-a742-444b-b91a-c7519f51bd77/custom-container-prediction-model/model.h5\n",
      "gs://cloud-ai-platform-2f444b6a-a742-444b-b91a-c7519f51bd77/custom-container-prediction-model/preprocessor.pkl\n"
     ]
    }
   ],
   "source": [
    "AIP_STORAGE_URI=f\"{BUCKET_NAME}/{MODEL_ARTIFACT_DIR}\"\n",
    "print(AIP_STORAGE_URI)\n",
    "!gsutil ls $AIP_STORAGE_URI/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b62ddf1def3"
   },
   "source": [
    "### Store test instances to use later\n",
    "To learn more about formatting input instances in JSON, [read the documentation.](https://cloud.google.com/ai-platform-unified/docs/predictions/online-predictions-custom-models#request-body-details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6605e9e6186",
    "outputId": "79be3c95-96ed-4b58-e074-8e820428fa6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting instances.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile instances.json\n",
    "\n",
    "{\n",
    "    \"instances\": [\n",
    "        [6.7, 3.1, 4.7, 1.5],\n",
    "        [4.6, 3.1, 1.5, 0.2]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test prediction server running on FastAPI locally [optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/run_predictor.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefilecustom app/run_predictor.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "cd app; \\\n",
    "export PROJECT_ID=$PROJECT_ID ; \\\n",
    "export AIP_HTTP_PORT=8080 ; \\\n",
    "export AIP_HEALTH_ROUTE=/health ; \\\n",
    "export AIP_PREDICT_ROUTE=/predict ; \\\n",
    "export AIP_STORAGE_URI={BUCKET_NAME}/{MODEL_ARTIFACT_DIR} ; \\\n",
    "python -m uvicorn main:app --host 0.0.0.0 --port 8501 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\n"
     ]
    }
   ],
   "source": [
    "!chmod +x app/run_predictor.sh\n",
    "!nohup ./app/run_predictor.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "\u001b[1mdate\u001b[0m: Thu, 01 Jul 2021 22:40:06 GMT\n",
      "\u001b[1mserver\u001b[0m: uvicorn\n",
      "\u001b[1mcontent-length\u001b[0m: 20\n",
      "\u001b[1mcontent-type\u001b[0m: application/json\n",
      "\n",
      "{\"Status\":\"Running\"}"
     ]
    }
   ],
   "source": [
    "!curl -i http://0.0.0.0:8501/health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0012253387831151485,0.7999316453933716,0.19884294271469116],[0.9997209906578064,0.0002631914976518601,1.5905799955362454e-05]]"
     ]
    }
   ],
   "source": [
    "!curl -X POST \\\n",
    "-d @instances.json \\\n",
    "-H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "http://0.0.0.0:8501/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "469f55daf250"
   },
   "source": [
    "### Add pre-start script\n",
    "FastAPI will execute this script before starting up the server. The `PORT` environment variable is set to equal `AIP_HTTP_PORT` in order to run FastAPI on same the port expected by AI Platform (Unified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69f438aca35b",
    "outputId": "3b6b15c7-de4d-4801-c855-9cff5f9bd44b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/prestart.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefilecustom app/prestart.sh\n",
    "#!/bin/bash\n",
    "export PORT=$AIP_HTTP_PORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51e149fdec1b"
   },
   "source": [
    "## Build and push container to Artifact Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bdb9a7768a5"
   },
   "source": [
    "Optionally copy in your credentials to run the container locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbb77f4f56c7"
   },
   "outputs": [],
   "source": [
    "# NOTE: Copy in credentials to run locally, this step can be skipped for deployment\n",
    "%cp $GOOGLE_APPLICATION_CREDENTIALS app/credentials.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "240578ec9efe"
   },
   "source": [
    "Write the Dockerfile, using `tiangolo/uvicorn-gunicorn-fastapi` as a base image. This will automatically run FastAPI for you using Gunicorn and Uvicorn. Visit [the FastAPI docs to read more about deploying FastAPI with Docker](https://fastapi.tiangolo.com/deployment/docker/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d3a6b9ed22b",
    "outputId": "b06cec6a-773c-498c-f113-27fda17aac3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
    "\n",
    "COPY ./app /app\n",
    "COPY requirements.txt requirements.txt\n",
    "\n",
    "RUN pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04c988201499"
   },
   "source": [
    "Build the image and tag the Artifact Registry path that you will push to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1e7d639b9cc",
    "outputId": "96ba0665-fdbc-440d-8220-6e58674b6146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 29 file(s) totalling 312.9 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://rthallam-demo-project_cloudbuild/source/1625180276.503493-09f9a69845eb455a88e2700a7ededfca.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/rthallam-demo-project/locations/global/builds/ea230126-4067-4b6b-8d91-9cf20aa11271].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/ea230126-4067-4b6b-8d91-9cf20aa11271?project=560224572293].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"ea230126-4067-4b6b-8d91-9cf20aa11271\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://rthallam-demo-project_cloudbuild/source/1625180276.503493-09f9a69845eb455a88e2700a7ededfca.tgz#1625180276828895\n",
      "Copying gs://rthallam-demo-project_cloudbuild/source/1625180276.503493-09f9a69845eb455a88e2700a7ededfca.tgz#1625180276828895...\n",
      "/ [1 files][ 76.4 KiB/ 76.4 KiB]                                                \n",
      "Operation completed over 1 objects/76.4 KiB.                                     \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  347.6kB\n",
      "Step 1/4 : FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
      "python3.7: Pulling from tiangolo/uvicorn-gunicorn-fastapi\n",
      "6c33745f49b4: Pulling fs layer\n",
      "ef072fc32a84: Pulling fs layer\n",
      "c0afb8e68e0b: Pulling fs layer\n",
      "d599c07d28e6: Pulling fs layer\n",
      "f2ecc74db11a: Pulling fs layer\n",
      "26856d31ce86: Pulling fs layer\n",
      "d4db05d8da44: Pulling fs layer\n",
      "a63ae5575b2c: Pulling fs layer\n",
      "a8c72204fafe: Pulling fs layer\n",
      "588d4fc87a50: Pulling fs layer\n",
      "0d7a4a6073cb: Pulling fs layer\n",
      "3a7e0859d882: Pulling fs layer\n",
      "644da687b92a: Pulling fs layer\n",
      "f6e61dbee877: Pulling fs layer\n",
      "3e19947ae254: Pulling fs layer\n",
      "4a4150ad76a8: Pulling fs layer\n",
      "331fe1ea8d9a: Pulling fs layer\n",
      "edff73282333: Pulling fs layer\n",
      "588d4fc87a50: Waiting\n",
      "0d7a4a6073cb: Waiting\n",
      "3a7e0859d882: Waiting\n",
      "644da687b92a: Waiting\n",
      "f6e61dbee877: Waiting\n",
      "3e19947ae254: Waiting\n",
      "4a4150ad76a8: Waiting\n",
      "331fe1ea8d9a: Waiting\n",
      "edff73282333: Waiting\n",
      "d599c07d28e6: Waiting\n",
      "f2ecc74db11a: Waiting\n",
      "26856d31ce86: Waiting\n",
      "d4db05d8da44: Waiting\n",
      "a63ae5575b2c: Waiting\n",
      "a8c72204fafe: Waiting\n",
      "c0afb8e68e0b: Verifying Checksum\n",
      "c0afb8e68e0b: Download complete\n",
      "ef072fc32a84: Verifying Checksum\n",
      "ef072fc32a84: Download complete\n",
      "6c33745f49b4: Verifying Checksum\n",
      "6c33745f49b4: Download complete\n",
      "26856d31ce86: Verifying Checksum\n",
      "26856d31ce86: Download complete\n",
      "d599c07d28e6: Verifying Checksum\n",
      "d599c07d28e6: Download complete\n",
      "a63ae5575b2c: Verifying Checksum\n",
      "a63ae5575b2c: Download complete\n",
      "d4db05d8da44: Verifying Checksum\n",
      "d4db05d8da44: Download complete\n",
      "a8c72204fafe: Verifying Checksum\n",
      "a8c72204fafe: Download complete\n",
      "588d4fc87a50: Verifying Checksum\n",
      "588d4fc87a50: Download complete\n",
      "0d7a4a6073cb: Verifying Checksum\n",
      "0d7a4a6073cb: Download complete\n",
      "3a7e0859d882: Download complete\n",
      "644da687b92a: Download complete\n",
      "f6e61dbee877: Verifying Checksum\n",
      "f6e61dbee877: Download complete\n",
      "3e19947ae254: Verifying Checksum\n",
      "3e19947ae254: Download complete\n",
      "4a4150ad76a8: Verifying Checksum\n",
      "4a4150ad76a8: Download complete\n",
      "f2ecc74db11a: Verifying Checksum\n",
      "f2ecc74db11a: Download complete\n",
      "edff73282333: Verifying Checksum\n",
      "edff73282333: Download complete\n",
      "331fe1ea8d9a: Verifying Checksum\n",
      "331fe1ea8d9a: Download complete\n",
      "6c33745f49b4: Pull complete\n",
      "ef072fc32a84: Pull complete\n",
      "c0afb8e68e0b: Pull complete\n",
      "d599c07d28e6: Pull complete\n",
      "f2ecc74db11a: Pull complete\n",
      "26856d31ce86: Pull complete\n",
      "d4db05d8da44: Pull complete\n",
      "a63ae5575b2c: Pull complete\n",
      "a8c72204fafe: Pull complete\n",
      "588d4fc87a50: Pull complete\n",
      "0d7a4a6073cb: Pull complete\n",
      "3a7e0859d882: Pull complete\n",
      "644da687b92a: Pull complete\n",
      "f6e61dbee877: Pull complete\n",
      "3e19947ae254: Pull complete\n",
      "4a4150ad76a8: Pull complete\n",
      "331fe1ea8d9a: Pull complete\n",
      "edff73282333: Pull complete\n",
      "Digest: sha256:8efb7b7c2419090159955d01c9978b2d59ef0980605f237673d7ae1ed5d76b4a\n",
      "Status: Downloaded newer image for tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
      " ---> 86ade7dea2c7\n",
      "Step 2/4 : COPY ./app /app\n",
      " ---> dd4a2e022ca2\n",
      "Step 3/4 : COPY requirements.txt requirements.txt\n",
      " ---> 9bdc832391f9\n",
      "Step 4/4 : RUN pip install -r requirements.txt\n",
      " ---> Running in 02b1ff32a793\n",
      "Collecting tensorflow==2.4.1\n",
      "  Downloading tensorflow-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3 MB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (0.36.2)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting google-cloud-storage<2.0.0dev,>=1.26.0\n",
      "  Downloading google_cloud_storage-1.40.0-py2.py3-none-any.whl (104 kB)\n",
      "Collecting joblib~=1.0\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting scikit-learn~=0.24\n",
      "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting google-auth<2.0dev,>=1.11.0\n",
      "  Downloading google_auth-1.32.1-py2.py3-none-any.whl (147 kB)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage<2.0.0dev,>=1.26.0->-r requirements.txt (line 5)) (51.0.0)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting google-cloud-core<2.0dev,>=1.4.1\n",
      "  Downloading google_cloud_core-1.7.1-py2.py3-none-any.whl (28 kB)\n",
      "Collecting google-api-core<2.0.0dev,>=1.21.0\n",
      "  Downloading google_api_core-1.30.0-py2.py3-none-any.whl (93 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting google-resumable-media<2.0dev,>=1.3.0\n",
      "  Downloading google_resumable_media-1.3.1-py2.py3-none-any.whl (75 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.1.2-cp37-cp37m-manylinux2014_x86_64.whl (38 kB)\n",
      "Collecting cffi>=1.0.0\n",
      "  Downloading cffi-1.14.5-cp37-cp37m-manylinux1_x86_64.whl (402 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
      "Collecting h5py~=2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting packaging>=14.3\n",
      "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.17.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting pyparsing>=2.0.2\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
      "Collecting chardet<5,>=3.0.2\n",
      "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
      "Collecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting importlib-metadata\n",
      "  Downloading importlib_metadata-4.6.0-py3-none-any.whl (17 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "Collecting pytz\n",
      "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=5178de53452d703ace8366cedbd88b3aef9746952cf61c491bd5f814ab6bf02c\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=76469 sha256=c10803b4438397d97a300aa13335508a4cf7d5a581c2c606e88b962d1f45fec1\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: urllib3, six, pyasn1, idna, chardet, certifi, zipp, rsa, requests, pyparsing, pycparser, pyasn1-modules, protobuf, oauthlib, cachetools, requests-oauthlib, pytz, packaging, importlib-metadata, googleapis-common-protos, google-auth, cffi, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, numpy, markdown, grpcio, google-crc32c, google-auth-oauthlib, google-api-core, absl-py, wrapt, threadpoolctl, termcolor, tensorflow-estimator, tensorboard, scipy, opt-einsum, keras-preprocessing, joblib, h5py, google-resumable-media, google-pasta, google-cloud-core, gast, flatbuffers, astunparse, tensorflow, scikit-learn, google-cloud-storage\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.2 certifi-2021.5.30 cffi-1.14.5 chardet-4.0.0 flatbuffers-1.12 gast-0.3.3 google-api-core-1.30.0 google-auth-1.32.1 google-auth-oauthlib-0.4.4 google-cloud-core-1.7.1 google-cloud-storage-1.40.0 google-crc32c-1.1.2 google-pasta-0.2.0 google-resumable-media-1.3.1 googleapis-common-protos-1.53.0 grpcio-1.32.0 h5py-2.10.0 idna-2.10 importlib-metadata-4.6.0 joblib-1.0.1 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 packaging-20.9 protobuf-3.17.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyparsing-2.4.7 pytz-2021.1 requests-2.25.1 requests-oauthlib-1.3.0 rsa-4.7.2 scikit-learn-0.24.2 scipy-1.7.0 six-1.15.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 termcolor-1.1.0 threadpoolctl-2.1.0 urllib3-1.26.6 werkzeug-2.0.1 wrapt-1.12.1 zipp-3.4.1\n",
      "\u001b[91mWARNING: You are using pip version 20.3.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 02b1ff32a793\n",
      " ---> 1aeaa0f61e7f\n",
      "Successfully built 1aeaa0f61e7f\n",
      "Successfully tagged us-central1-docker.pkg.dev/rthallam-demo-project/custom-container-prediction/keras-fastapi-server:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/rthallam-demo-project/custom-container-prediction/keras-fastapi-server\n",
      "The push refers to repository [us-central1-docker.pkg.dev/rthallam-demo-project/custom-container-prediction/keras-fastapi-server]\n",
      "76aa58391636: Preparing\n",
      "67c1ae444788: Preparing\n",
      "c2a52f0a84de: Preparing\n",
      "b0cbd55a00a8: Preparing\n",
      "5c1526d87912: Preparing\n",
      "bb80579fc546: Preparing\n",
      "075c652b44f7: Preparing\n",
      "b26121b4dfdd: Preparing\n",
      "92c41c21be74: Preparing\n",
      "ab98b1286ea9: Preparing\n",
      "c72c79cd478b: Preparing\n",
      "f7737f3c85d2: Preparing\n",
      "3da8662a6eed: Preparing\n",
      "6a6ea1335e48: Preparing\n",
      "4324e0912cc9: Preparing\n",
      "59840d625c92: Preparing\n",
      "da87e334550a: Preparing\n",
      "c5f4367d4a59: Preparing\n",
      "ceecb62b2fcc: Preparing\n",
      "193bc1d68b80: Preparing\n",
      "f0e10b20de19: Preparing\n",
      "bb80579fc546: Waiting\n",
      "4324e0912cc9: Waiting\n",
      "59840d625c92: Waiting\n",
      "da87e334550a: Waiting\n",
      "c5f4367d4a59: Waiting\n",
      "ceecb62b2fcc: Waiting\n",
      "193bc1d68b80: Waiting\n",
      "f0e10b20de19: Waiting\n",
      "075c652b44f7: Waiting\n",
      "b26121b4dfdd: Waiting\n",
      "92c41c21be74: Waiting\n",
      "ab98b1286ea9: Waiting\n",
      "c72c79cd478b: Waiting\n",
      "f7737f3c85d2: Waiting\n",
      "3da8662a6eed: Waiting\n",
      "6a6ea1335e48: Waiting\n",
      "b0cbd55a00a8: Layer already exists\n",
      "5c1526d87912: Layer already exists\n",
      "bb80579fc546: Layer already exists\n",
      "075c652b44f7: Layer already exists\n",
      "92c41c21be74: Layer already exists\n",
      "b26121b4dfdd: Layer already exists\n",
      "ab98b1286ea9: Layer already exists\n",
      "c72c79cd478b: Layer already exists\n",
      "f7737f3c85d2: Layer already exists\n",
      "3da8662a6eed: Layer already exists\n",
      "6a6ea1335e48: Layer already exists\n",
      "4324e0912cc9: Layer already exists\n",
      "59840d625c92: Layer already exists\n",
      "da87e334550a: Layer already exists\n",
      "c2a52f0a84de: Pushed\n",
      "67c1ae444788: Pushed\n",
      "193bc1d68b80: Layer already exists\n",
      "f0e10b20de19: Layer already exists\n",
      "ceecb62b2fcc: Layer already exists\n",
      "c5f4367d4a59: Layer already exists\n",
      "76aa58391636: Pushed\n",
      "latest: digest: sha256:d0205d523b75445a067eebafb567086e3bbc9d7c32fe6c1257f88c422bd3d8a5 size: 4719\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                               IMAGES                                                                                                       STATUS\n",
      "ea230126-4067-4b6b-8d91-9cf20aa11271  2021-07-01T22:57:56+00:00  3M53S     gs://rthallam-demo-project_cloudbuild/source/1625180276.503493-09f9a69845eb455a88e2700a7ededfca.tgz  us-central1-docker.pkg.dev/rthallam-demo-project/custom-container-prediction/keras-fastapi-server (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit \\\n",
    "    --timeout=3600 \\\n",
    "    --tag={REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE} \\\n",
    "    ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "147a555f6c93"
   },
   "source": [
    "### Run and test the container locally (optional)\n",
    "\n",
    "Run the container locally in detached mode and provide the environment variables that the container requires. These env vars will be provided to the container by AI Platform Prediction once deployed. Test the `/health` and `/predict` routes, then stop the running image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "62ed2d334d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response from daemon: No such container: local-iris\n",
      "Error: No such container: local-iris\n",
      "fe8d25491e12f44578858ee7e11f6a350111a15094b4186bec9530aef5fd746d\n",
      "CONTAINER ID   IMAGE                                                                                               COMMAND                  CREATED                  STATUS                  PORTS                          NAMES\n",
      "fe8d25491e12   us-central1-docker.pkg.dev/rthallam-demo-project/custom-container-prediction/keras-fastapi-server   \"/start.sh\"              Less than a second ago   Up Less than a second   80/tcp, 0.0.0.0:80->8501/tcp   local-iris\n",
      "a5a368c8c275   gcr.io/inverting-proxy/agent                                                                        \"/bin/sh -c '/opt/bi…\"   28 hours ago             Up 28 hours                                            proxy-agent\n",
      "HTTP/1.1 200 OK\n",
      "\u001b[1mdate\u001b[0m: Thu, 01 Jul 2021 22:49:33 GMT\n",
      "\u001b[1mserver\u001b[0m: uvicorn\n",
      "\u001b[1mcontent-length\u001b[0m: 20\n",
      "\u001b[1mcontent-type\u001b[0m: application/json\n",
      "\n",
      "{\"Status\":\"Running\"}[[0.0012253387831151485,0.7999316453933716,0.19884294271469116],[0.9997209906578064,0.0002631914976518601,1.5905799955362454e-05]]"
     ]
    }
   ],
   "source": [
    "!docker stop local-iris\n",
    "!docker rm local-iris\n",
    "!docker run -t -d --rm -p 80:8501 \\\n",
    "    --name=local-iris \\\n",
    "    -e AIP_HTTP_PORT=8501 \\\n",
    "    -e AIP_HEALTH_ROUTE=/health \\\n",
    "    -e AIP_PREDICT_ROUTE=/predict \\\n",
    "    -e AIP_STORAGE_URI={BUCKET_NAME}/{MODEL_ARTIFACT_DIR} \\\n",
    "    -e GOOGLE_APPLICATION_CREDENTIALS=credentials.json \\\n",
    "    {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\n",
    "!docker container ls\n",
    "!curl -i http://0.0.0.0:8501/health\n",
    "!curl -X POST \\\n",
    "  -d @instances.json \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  http://0.0.0.0:8501/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "a29fcbbe0188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response from daemon: No such container: local-iris\n"
     ]
    }
   ],
   "source": [
    "!docker stop local-iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b438bfa2129f"
   },
   "source": [
    "## Deploy to Vertex AI\n",
    "\n",
    "Use the [Python SDK for Cloud AI Platform](https://googleapis.dev/python/aiplatform/latest/index.html) to upload and deploy your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ae19df6a33e"
   },
   "source": [
    "### Upload the custom container model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "8d682d8388ec"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "574fb82d3eed"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "2738154345d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/560224572293/locations/us-central1/models/1889409577263300608/operations/2594902692010655744\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/560224572293/locations/us-central1/models/1889409577263300608\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/560224572293/locations/us-central1/models/1889409577263300608')\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=f\"{BUCKET_NAME}/{MODEL_ARTIFACT_DIR}\",\n",
    "    serving_container_image_uri=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd1b85afc7df"
   },
   "source": [
    "### Deploy the model on AI Platform (Unified)\n",
    "After this step completes, the model is deployed and ready for online prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "62cf66498a28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/560224572293/locations/us-central1/endpoints/4318723344229728256/operations/1012450372943347712\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/560224572293/locations/us-central1/endpoints/4318723344229728256\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/560224572293/locations/us-central1/endpoints/4318723344229728256')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/560224572293/locations/us-central1/endpoints/4318723344229728256\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/560224572293/locations/us-central1/endpoints/4318723344229728256/operations/6936372732795813888\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/560224572293/locations/us-central1/endpoints/4318723344229728256\n"
     ]
    }
   ],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6883e7b07143"
   },
   "source": [
    "## Send predictions\n",
    "\n",
    "### Using Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "d69ed411c2d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=['versicolor', 'setosa'], deployed_model_id='5761493706104373248', explanations=None)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.predict(instances=[[6.7, 3.1, 4.7, 1.5], [4.6, 3.1, 1.5, 0.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arqz41gK7rWx"
   },
   "source": [
    "#### Sending prediction instances with additional parameters\n",
    "When you send a prediction request to a custom container, you can provide additional fields on your request body. The container's predict method receives these as fields of the `parameters` dictionary.\n",
    "\n",
    "The following code sends the same request as before, but this time it adds a probabilities field as an additional parameter to the request body:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "6FtZQp9t7r5V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[[0.001225338783115149, 0.7999316453933716, 0.1988429427146912], [0.9997209906578064, 0.0002631914976518601, 1.590579995536245e-05]], deployed_model_id='5761493706104373248', explanations=None)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.predict(instances=[[6.7, 3.1, 4.7, 1.5], [4.6, 3.1, 1.5, 0.2]], \n",
    "                 parameters={\"probabilities\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "370d22f53427"
   },
   "source": [
    "### Using REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "ba55bc560d58"
   },
   "outputs": [],
   "source": [
    "ENDPOINT_ID = endpoint.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "95c562b4e98b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"predictions\": [\n",
      "    \"versicolor\",\n",
      "    \"setosa\"\n",
      "  ],\n",
      "  \"deployedModelId\": \"5761493706104373248\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! curl \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d @instances.json \\\n",
    "https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa71174a7dd0"
   },
   "source": [
    "### Using gcloud CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "23b8e807b02c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-prediction-aiplatform.googleapis.com/]\n",
      "['versicolor', 'setosa']\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai endpoints predict $ENDPOINT_ID \\\n",
    "  --region=$REGION \\\n",
    "  --json-request=instances.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Undeploy model and delete endpoint\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete the model resource\n",
    "model.delete()\n",
    "\n",
    "# Delete the container image from Artifact Registry\n",
    "!gcloud artifacts docker images delete \\\n",
    "    --quiet \\\n",
    "    --delete-tags \\\n",
    "    {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI_Platform_(Unified)_SDK_Custom_Container_Prediction_Keras.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:ucaip-cc]",
   "language": "python",
   "name": "conda-env-ucaip-cc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
